# 概述
这个项目是数码内部让校招生比赛的一个项目，给一个月的时间使用蚂蚁技术栈写出一个 minialiay
我的职责充当技术官，负责对项目设计，把控组内人员的进度，其中有三个模块，用户模块、业务、账务模块。
本项目分前台后台。
我主要负责三个模块技术调研，选型，实现。
# 分库分表路由设计
## 数据分析
而淘宝活跃用户有8亿，对于双11那天来说没人大概一单。
其他时间日活大概30w 人就会下一单。对于我们这个系统来说，而淘宝前三年用户增长量也只有1000w，所以大概一天只会有300单成功交易。
双十一 qps 大概万级不到，不过这个数据是在03年到06年互联网相对没那么兴起的情况，所以我还参照了后来起家的拼多多数据拼多多第一年大概3000w，而后面每年都多一亿，基于这个数据，我觉得这个系统未来三年估算大概3亿用户，这样的话每天大概就是1000w 笔订单双十一而淘宝 qps 大约50w 这个数据非常恐怖支付宝双十一40w。不过这些终究是数据。
淘宝：50w
支付宝：40w
拼多多：50w
他们的用户活跃都是8亿左右，平时每天大概3000w 笔订单，大概就是1：。
vivo 中
## 实战案例
对于行业中就有一个非常好的案例，就是 vivo 的`vivo 全球商城 亿级订单中心、优惠券中心架构设计与实践`。
**_我手上的方案，大概是他们2018年的，那时候，他们的订单只有1000万级别。_**
**_那个时候的vivo商城数据量看上去不多，但是刚好是完美的学习型数据。**_
随着历史订单不断累积，2017年MySQL中订单表数据量已达千万级。之后的订单数据，远远大于亿级
## 优化分析
亿级数据按照业内的参考标准，单表的数据在500-1000W，B+树的高度在2-3层，一般2-3次IO操作，就可以读取到数据记录，那么大概要分20个表
从高并发的场景出发，选择了本地消息表方案：
在本地事务中将要执行的异步操作记录在消息表中，如果执行失败，可以通过定时任务来补偿。
对数据量大的问题，进行了以下优化：
- 数据归档，冷热数据分离方案，冷热分离方案扩展Hbase，以及为什么这里不使用 Hbase
### 什么时候应该使用 HBase?

HBase 并不适合解决所有问题。

第一，确认有足够的数据。如果有上亿或数十亿行数据，那么 HBase 是一个好的选择。如果只有几千或几百万行数据，那么使用传统的 RDBMS 或许是个更好的选择，因为全部数据可能会被调度到单独一个（或者两个）节点上，集群的其他节点可能是空闲的。

第二，确认可以不用 RDBMS 提供的所有额外功能（例如，类型列，二级索引，事务，高级查询语言等）。针对 RDBMS 构建的应用程序无法通过简单地更改 JDBC Driver 就“移植”到 HBase。从 RDBMS 迁移到 HBase 需要完全重新设计而不是移植。

第三，确认有足够的硬件设施。少于 5 个 DataNode 时，HDFS 甚至什么都做不了（由于诸如 HDFS 块复制默认值为 3 之类的情况），再加上 NameNode。
HBase 可以在笔记本电脑上独立运行 - 但是这只应该被当做开发配置
- 分表 
## 消息的有序性问题
采用MQ消费的方式同步数据库的订单相关数据到ES中，遇到的写入数据不是订单最新数据问题。



上图左边是原方案：

在消费订单数据同步的MQ时，如果线程A在先执行，查出数据，

这时候订单数据被更新了，线程B开始执行同步操作，查出订单数据后先于线程A一步写入ES中，

线程A执行写入时就会将线程B写入的数据覆盖，导致ES中的订单数据不是最新的。

上图右边是解决方案：

解决方案是在查询订单数据时加行锁，整个业务执行在事务中，执行完成后再执行下一个线程。
## 三板斧：缓存、池化、异步
## 缓存
先考虑的是分布式缓存 Redis，使用Redis作为MySQL的前置缓存，可以挡住大部分的查询请求，并降低响应时延。

其次，对于热点数据，可以使用二级缓存，甚至三级缓存
比如： 商品系统、 优惠券系统、活动系统，这里存在局部热点、周期性热点数据的系统，使用一级缓存、二级缓存、甚至三级缓存。

但是，订单系统不属于这个场景。

订单熊有一个特点，每个用户的订单数据都不一样，

所以，在订单系统中，缓存的缓存命中率不高。不存在太热的数据，所以一级缓存、三级缓存就不用了。

但是，redis 二级缓存，能缓存最近的订单，

最近的订单也是用户最近最可能使用的数据，矮个子里边拔将军，

所以，redis分布式还是能够为DB分担一下压力。这个还是要用的。

分析前台和后台需求侧合理定制需求，前台采用基因法，后台采用多一个全部数据表，并且考虑到后台不需要很强的实时性，所以可以使用异步去做.
路由策略设计，2的次方。
# 注册

# 分布式事务


## 缓存重构
# 解决一期历史遗留问题做性能上的优化。
### 场景描述：商品业务。
商品详情接口，该接口需要执行以下操作：

1. 获取商品的基本信息。
2. 获取商品的用户评论。
3. 获取推荐商品列表。
饥饿现象 tp50 tp90差很多 tp50指的是一半请求 rt 时间 tp90指的是90%请求的 rt 时间
在性能测试中，我们经常会选择 TP90、TP95 或者 TP99 等水位线作为性能指标。首先，我们先解释一下 TP90、TP95 和 TP99 的含义：

TP90，top percent 90，即 90% 的数据都满足某一条件；  
TP95，top percent 95，即 95% 的数据都满足某一条件；  
TP99，top percent 99，即 99% 的数据都满足某一条件；  
在这里，我们之所以说其“满足某一条件”，是因为在计算的时候，我们既可以向前计算也可以向后计算，例如：  
1, 2, 3, …, 98, 99, 100  
如上所示，这是一个从 1 至 100 的数列，如果我们想计算其 TP99 的值，其方法为用数列中数值的总个数乘以 99%，即100 * 99% = 99，显然在这个数列中有两个数值满足这个 99 的概念，分别为：

- 2，即数列中 99% 的数值都大于等于2
- 99，即数列中 99% 的数值都小于等于99
服务器配置 2c 4c （c 指的是核数）

20线程是最好的，tomcat 线程池   平均rt 下来，线程太多线程调度不过来
异步线程延迟双删 100毫秒
不是业务代码的代码会尽量去做封装解耦

解析# SpEL表达式导致性能问题的坑

自我介绍直接介绍自己的工作职责责任，比如代码上的优化，帮助组内做过冷热分离并提出一些注意事项，
所有 hashtbale 方法都加了锁



堆和非堆的内存分配