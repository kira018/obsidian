# 前言

随着业务的持续发展，业务数据库数据持续增长，单表数据过千万时就需要考虑分库分表，或者选择存储更大扩展性更好的NOSQL数据库，传统关系型数据库存储与计算一体，拆分过程需要做复杂的rehash和数据迁移，分库分表能一定程度降低存储压力，提高单实例读写性能，但存储成本也成倍增长，因此出现冷热分离的通用解决方案。

# 一.为什么做冷热分离

## 1.冷热数据划分

- 冷数据，指访问频次较低的数据
- 热数据，指访问频次较高的数据

数据访问频率对应着数据所产生的价值，冷热划分虽然是以访问频率来进行划分，但引起访问频率差异的因素是多维度的，微博场景下热点用户访问频率高，IM场景近期记录访问频率高，基于时间远近是常见划分因素，比如用户购物记录，聊天记录，随着时间推移历史数据访问频率越来越低，逐渐变为冷数据。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624760687712-2a251fc5-be84-4f6f-bc92-6cc225446270.png)

  

**基于时间划分**

冷热划分时间点与业务强相关，部分场景可能对数据进行更细粒度的划分，比如热数据，温数据、冷数据，但原理类似

|      |                  |               |        |
| ---- | ---------------- | ------------- | ------ |
| 数据特征 | 访问频率             | 数据诉求          | 数据写入时长 |
| 热数据  | 访问频率较高           | 高性能访问         | 小于X天   |
| 冷数据  | 访问频率较低，访问QPS几乎为0 | 大存储空间，有一定性能诉求 | 大于X天   |

## 2.冷热分离的本质

随着时间推移业务数据越来越多，新写入的数据访问频率远高于历史数据，我们可以认为新写入是热数据，而历史写入是冷数据，冷热数据的访问频率差异较大，却耗费同样的存储与计算成本，做冷热分离实际是节约资源成本。

# 二.冷热分离技术概述

冷热分离的核心是冷数据成本问题，1是如何让冷数据存储和计算成本更低，比如用更廉价的磁盘、CPU核数更少的机器，2是如何让冷数据变得更小，比如将数据压缩存储，冷热分离的解决方案很多，业务架构和存储中间件都可以实现，多数是基于这两个点出发。

## 1.冷热介质分离

冷数据使用不同介质存储，冷节点机器规格更低，满足大容量，热节点机器规格高，保证高性能。热数据变为冷数据后定期向冷节点迁移。比如[阿里云ES](https://help.aliyun.com/document_detail/173474.html?spm=a2c4g.11186623.6.817.69f84637DDvBae)及[HBASE](https://help.aliyun.com/document_detail/135291.html?spm=a2c4g.11186623.6.600.6c2d7a9c1XhgZA)使用的是这种方案。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624934578190-dbf838ce-0fd0-4d82-bc28-04b294052a1d.png)

  

## 2.数据压缩技术

数据存储压缩技术比较常见，使用压缩算法对元数据及索引压缩，压缩和解压过程增加CPU开销，用更多的计算时长换取更小的空间，数据压缩比与压缩算法及数据特征相关，以下列举部分存储压缩技术。

|                                                                                                         |               |                                     |
| ------------------------------------------------------------------------------------------------------- | ------------- | ----------------------------------- |
| 压缩选型                                                                                                    | 压缩比(相比innodb) | 适用场景                                |
| [innodb压缩](https://dev.mysql.com/doc/refman/5.6/en/innodb-introduction.html)                            | 25％~ 50％      | 大字符类型数据表，事务并发度要求不高                 |
| [archive](https://dev.mysql.com/doc/refman/5.6/en/archive-storage-engine.html)                          | 1/10~1/15     | 只支持INSERT和SELECT，不支持UPDATE，比如统计数据收集 |
| [tokudb](https://www.percona.com/software/mysql-database/percona-tokudb)                                | 大约25%         | 读多写少场景，追求高压缩比                       |
| [X-Engine](https://help.aliyun.com/document_detail/148660.html?spm=a2c4g.11186623.6.613.7a0818e3lomT3M) | 10%~50%       | 阿里云自研数据库存储引擎，适用大规模数据以及高并发场景        |

# 三.冷热分离实践

## 1.项目背景
跨境付款为支付宝国际事业部的核心业务，整合了出国留学、国际捐款、跨境红包、打通香港、留学缴费等业务，其中的订单表数据库使用阿里云MYSQL版本RDS ，业务持续扩张，存储层容量将在4个月内到达系统瓶颈，且单实例存储层已无法扩容(所购规格最大支持6T磁盘)磁盘使用率70%。
	- 业务快速增长,主库存储18年到现在所有的订单数据（跨境18年成立）
- 随着跨境业务（目前打通积分支付导致订单量开始变多）快速增长

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1623935494810-6f89c05f-b824-424c-a679-f78ac0290ca3.png)

## 2.项目目标

- 突破存储瓶颈：解决单实例无法扩容问题，降低存储成本
- 业务稳定性诉求：改造过程上层业务基本无感知，数据不丢失
- 架构扩展性诉求：改造方案是可持续的，业务持续增长后依然适用

## 3.拆分方案

### 1.方案调研

#### 1.现状梳理

- 库表规则

![](https://cdn.nlark.com/yuque/0/2021/jpeg/1054019/1624853254070-68e6a931-6f94-4783-971b-1e9c880a4c88.jpeg)

  

- 数据表梳理

主表是每个订单都会有一条记录，扩展表是特定业务场景才会有数据，扩展表数据较少，因此考虑先对主表数据进行迁移。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624935557110-bf8f3742-d1ed-4e68-88c1-497997762d64.png)

#### 2.方案选型

- 数据治理

订单库目前共8实例，32个DB，可以从横向拆分、数据治理以及冷热分离几个方面进行考虑，考虑到改造时间和成本因素最终决定先做冷热分离，后续再进行无效数据治理。

|      |                                         |                         |                                |
| ---- | --------------------------------------- | ----------------------- | ------------------------------ |
| 方案比较 | 实现成本                                    | 数据收益                    | 风险性                            |
| 横向扩容 | 开发成本低，资源成本高，需要再购买8实例                    | 可降低50%占用                | 改造风险较低，但未来还是会面临磁盘空间问题          |
| 数据治理 | 开发成本高，周期长，收敛无效订单使用场景，对无效数据进行单独冗余存储      | 按下单到播单1：10计算，主表约减少90%数据 | 风险较高，需业务上下游梳理这类订单使用场景，同时新增查询方式 |
| 冷热分离 | 开发成本高，不依赖上下游服务改造，需新增存储库存储老数据，并兼顾业务更新和查询 | 随着时间推移，数据收益会越高          | 风险较高，但解决彻底，热库只存储一段时间的数据        |

- 冷热存储方案

1.冷存储介质

基于之前讲到的冷热分离技术和现有的阿里云产品，我们使用RDS+[X-Engine](https://help.aliyun.com/document_detail/148404.html?spm=5176.12818093.help.dexternal.5adc16d0e3VBx0)来存储冷数据（钉钉、淘宝交易也是使用该方案，参考[X-Engine如何支撑钉钉跃居AppStore第一](https://help.aliyun.com/document_detail/161317.html?spm=a2c4g.11186623.6.638.32682b0aLErroL)），X-Engine压缩比能到达10%到50%，同时保证一定的读写性能。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624935670654-956d67b9-d886-46c1-bc9f-668cb53b744b.png)

2.冷数据区间

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624935861815-9389f593-4eee-4693-9d54-b9dfc033b90b.png)

方案二风险较高，对比方案一和三，考虑到数据同步成本最终选用方案三，对冷热数据进行部分冗余，由于用户只会查询近期订单，最终选取120天(实现时可云控配置)作为临界点，90天之外数据存储冷库，120天内数据存储热库，冷热冗余30天数据。

### 2.冷热分离

#### 1.分离规则

- 用数据创建时间gmtCreate字段(业务表均已支持)作为分隔依据
- 热区存储120天内数据，冷区存储90天外数据
- 冷热冗余30天共存数据是防止异常情况下数据丢失情况，共存区更新时冷热库均需要更新
- 状态非终态订单存储热库(未完成订单查询更新频率高，终态订单更新场景少，比如退款)

  

数据分区示意图

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1623935433673-88df4f85-5912-4f94-bfcc-54c0bf27df81.png)

#### 2.读写流程

（1）访问流程

![](https://cdn.nlark.com/yuque/0/2021/jpeg/1054019/1624936210503-1c7798ab-1464-4888-b5dd-5eb5b598790c.jpeg)

1. 使用RPC更新共存区时涉及到分布式事务，一致性如何保证？ 从查询流程可以看出120天内只查询热库，因此冷热共存区更新时热库成功冷库失败业务暂无影响，冷库需保证最终一致性，冷库更新异常(失败或超时)时热库事务不回滚，发送延时消息进行数据校验补齐，冷区和热区更新不涉及跨库更新。

  

2. 冷库更新可否考虑使用消息进行更新？

使用消息方式进行异步更新时可能出现丢失、逆序等问题，且接口调用成功时事务并未执行，上游业务查询时返回脏数据可能引发异常问题(比如退款后查询订单实收金额)。

（2）访问规则

- DML操作

下表梳理了业务中主要的DML操作类型和对应目标库，主要为单条读写、批量读写、分页列表查询以及未完成订单。

|      |                            |              |                            |
| ---- | -------------------------- | ------------ | -------------------------- |
| DML  | 维度                         | 场景           | 目标库                        |
| 增加   | 单条订单                       | 下单           | 订单维度新增数据均村塾热库              |
| 批量订单 | 扬招单批量同步                    |              |                            |
| 查询   | 订单维度                       | 订单详情         | 120天之内的数据访问热库，120天之外数据访问冷库 |
| 用户维度 | 司机订单列表，未完成订单               |              |                            |
| 修改   | 单条订单                       | 状态机流转，比如用户支付 |                            |
| 批量订单 | 批量更新发票状态                   |              |                            |
| 删除   | 业务流程无物理删除逻辑，均为逻辑删，暂不考虑这类场景 |              |                            |

![](https://intranetproxy.alipay.com/skylark/lark/0/2021/jpeg/17356477/1627828688351-fa744a18-ec48-4e2d-ad50-3acbc1fab03d.jpeg)

用伪代码表示分离后订单维度查询和更新逻辑，从查询逻辑可以看出，120天内需保证热库数据准确，120天外需要保证冷库数据准确，共存区只做冗余，保证最终一致性即可。

```
currTime:系统当前日期
orderTime:订单日期(订单号前6位)
coldDb:冷库
hotDb:热库
//查询场景,120天(左闭右开)查询热库，否则更新冷库
if (currTime - orderTime) ∈  [0，120）
return hotDb
else 
return coldDb
//更新场景，120天内(左闭右开)更新热库，90天外(左闭右开)更新冷库，共存区需同时更新
if (currTime - orderTime) ∈  [0，120）
return hotDb
if (currTime - orderTime) ∈  [90，∞ ）
return coldDb
```

  

（3）特殊场景

1.用户维度查询是传入月份，单月数据能否不进行跨库查询，只查询热库或者冷库？

热库存储120天数据，能覆盖3个月但无法覆盖完整的4个月(比如7到10月共123天)，为了保证热库数据能完整覆盖4个月，需要将删除向前推移一段时间(目前为一周)，以下用10月进行举例，其它月份类似。

- 当前日期是10月1日，当前月份为10月，则10、9、8、7查询热库，跨越(1+30+31+31) 93天
- 当前日期是10月15日，当前月份为10月，则10、9、8、7查询热库，跨越(15+30+31+31) 107天
- 当前日期是10月31日，当前月份为10月，则10、9、8、7查询热库，跨越(31+30+31+31) 123天

2.将删除向前推迟一周后，当列表查询的4个月时间覆盖超过120天时，由于热库只更新120天内的数据，超出部分状态不是最新时怎么办？

- 退款状态目前未在列表接口透出，不影响列表查询数据正确性，详情查询时可通过订单号路由到冷库查询最新数据(此类状况触发场景比较极端，因此只做兼容逻辑，后续调整为3个月内查询热库也可解决)。

  

3.订单号规则创建时间(获取订单号时生成)早于数据库gmtCreate字段(插入时生成)，如果Y为当前日期，订单号位于X天，gmtCreate位于X+1天会有何影响？

订单号时间主要是订单维度查询或者更新，gmtCreate使用场景是数据迁移(每天迁移Y-90天的数据)和删除(每天删除Y-127天的数据)，以下分析临界情况。

- 订单号位于Y-89天，gmtCreate位于Y-90天（无影响，均查询热库，但是数据会被提前一天迁移到冷库）
- 订单号位于Y-90天，gmtCreate位于Y-91天（无影响，原因同上）
- 订单号位于Y-119天，gmtCreate位于Y-120天(无影响，热库数据删除第Y-127天数据，并未立即删除，订单号路由到热库仍能够查询更新)
- 订单号位于Y-120天，gmtCreate位于Y-121天(无影响，数据均位于冷库，查询更新均走冷库)  

4.唯一索引是订单+状态的数据如何保证只在热库或冷库？

此类订单数据的完整性是由多条数据共同组成的，因此删除时需要关联主单是否已到达终态，到终态时可进行删除。

5.数据库gmtCreate字段有修改的场景该怎么办？

部分业务逻辑gmtCreate使用的是入参传入进行插入覆盖，导致gmtCreate与订单号规则时间差距较大，此类状况需要对修改范围做限制，不允许无限制修改(数据迁移是以天为维度，插入gmtCreate为90天前的数据迁移时会遗漏)，如果业务需要修改gmtCreate字段，建议单独新增业务字段，不和数据创建时间共用。

#### 3.前端改造

由于数据通过日期进行了划分，如果用户查询所有订单列表时需要分别查询冷库和热库，分页查询也会变得更加复杂，考虑到历史订单访问频率较低，因此考虑在查询时增加日期(一般使用月份)，前端交互部分可考虑下拉选择月份或者列表滑动自动切换月份。

- 滴滴乘客列表和微信支付账单

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624864648312-52cdfe86-6d33-4171-8fa2-6eb38c74e8da.png)![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1624864443827-b09595ef-769d-449f-9f1c-a2720e09384f.png)

  

- 司机端行程列表

1. 客户端列表查询需要传入月份，通过月份路由路由热库或冷库，默认查询当前月份。
2. 老版本只支持查询4个月内数据，并提示查询全部订单需升级至最新版本。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1623935434131-057d75c8-43be-48b2-adb1-883f70dc7d65.png)

### 3.数据迁移

#### 1.数据迁移

- 全量数据迁移，需要将历史数据均同步至冷库，可采用DTS或定时任务进行数据同步，业务采用定时任务进行同步(冷库数据表hash规则有调整)，迁移完成进行校验比对并灰度冷库读写流程， 均无异常后删除历史数据。
- 增量热数据迁移,每天凌晨迁移当前日期前90天数据，使用定时任务自动迁移，为减少迁移过程影响在线库性能，每个实例创建一个子任务进行串行迁移。

#### 2.数据校验

- 对于当前日期90天前的数据进行迁移，数据迁移完后开启校验任务对冷热库进行字段校验，冷热库数据不一致时以热库为准进行数据修复。

#### 3.数据删除

- 热数据删除，删除当前日期127天前数据，删除前校验冷库是否已存在该数据。

#### 4.磁盘优化

数据删除后磁盘空间不会真正释放，因此需要做[optimize优化](https://dev.mysql.com/doc/refman/8.0/en/optimize-table.html)，虽然Mysql5.7版本online DDL支持optimize，但考虑到回收过程对数据库性能仍会有影响，因此业务使用从库(无业务使用)定期进行磁盘回收，在业务低峰期进行主从切换实现主库磁盘回收。[](https://help.aliyun.com/document_detail/181285.html?spm=5176.21213303.J_6028563670.7.16d33edaZGprgD&scm=20140722.S_help%40%40%E6%96%87%E6%A1%A3%40%40181285.S_0.ID_181285-RL_OPTIMIZE-OR_s%2Bhelpproduct-V_1-P0_0)

## 4.拆分结果

数据库磁盘由95%降低到65%，磁盘储瓶颈问题得以缓解(目前还未将数据完全迁移，历史数据仍在删除中)，目前热库实例配置32核128G，磁盘6T，冷库实例配置4核16G，磁盘8T，并且冷数据访问查询RT小于20ms，更新小于RT150ms。

![](https://cdn.nlark.com/yuque/0/2021/png/1054019/1623935435013-cec39873-2c5d-47ff-a09b-1a852e247e5e.png)

## 5.总结与规划

- 总结

整体改造需在2个月内完成并且不允许延期，改造方案需兼顾业务诉求以及资源成本，并保障业务平稳过渡，整体过程比较紧张，部分细节未在文中表述。值得反思的是由于改造点比较晚，导致改造时间并不充足，技术架构至少能维持3到6个月的业务增长，业务才能有充足的时间进行技术改造，这也是提前做好技术规划的意义。

  

- 规划

目前存储层的燃眉之急已经解决，但业务增长2到3倍之后，热库仍然会面临存储瓶颈，因此近期会继续对无效订单(下单后未接单)数据进行治理，降低主库存储压力，让存储架构能支撑未来3到5年的业务发展。

# 写在最后

## 招聘

高德打车出租车团队求贤若渴，我们期待您的加入！

岗位职责：负责高德共享打车出租车业务(交易、调度派单、计价引擎，用户资产等)等相关架构设计与研发工作，全新业务模式，复杂业务场景，高并发大流量场景挑战：链接运力和出行场景，构建共享出行新生态。

感兴趣请戳后面链接，惊喜不要错过**：[https://hire.alibaba-inc.com/i18n/positionDetail.html?positionId=672997](https://hire.alibaba-inc.com/i18n/positionDetail.html?positionId=672997)

## 扩展阅读

**稳定性建设**

[共享稳定性建设-监控治理及资损防控篇](https://www.atatech.org/articles/193550)

[共享打车下单服务治理&RT优化](https://www.atatech.org/articles/195148)

[共享出行Top流量卡片处理演进优化](https://www.atatech.org/articles/195753)

[高德打车双旦稳定性保障挑战和总结](https://www.atatech.org/articles/197493)

[高德打车巡网对接模式稳定性建设实践](https://www.atatech.org/articles/197667)

[高德打车双旦稳定性保障实践——数据库变更规范](https://www.atatech.org/articles/198140)

[高德打车](https://www.atatech.org/articles/198574)[双旦](https://www.atatech.org/articles/198140)[稳定性保障实践——可回滚能力总结](https://www.atatech.org/articles/198574)

[高德打车双旦稳定性保障KickOff](https://www.atatech.org/articles/198784)

[高德打车支付系统大促稳定性保障总结](https://www.atatech.org/articles/199160)

[高德打车支付系统数据一致性实践（DRC&BCP应用）](https://www.atatech.org/articles/199346)

[高德打车延迟任务稳定性治理及故障注入实践](https://www.atatech.org/articles/199518)

[高德打车双旦稳定性保障实践——安全生产](https://www.atatech.org/articles/199524#30)

[高德打车新人总结：服务端新人入职看这篇就够了](https://www.atatech.org/articles/199777#0)

  

**架构演进**

[高德共享出行订单数据服务化演进实践](https://www.atatech.org/articles/193238)

[弹外服务分库分表扩容及数据迁移实践](https://www.atatech.org/articles/194260)

[高德打车供需网格系统建设实践](https://www.atatech.org/articles/201068?spm=ata.13269325.0.0.51d149faal2N9v)

[高德打车代码依赖与数据依赖分离实践](https://www.atatech.org/articles/201441?spm=ata.13269325.0.0.258d49fakMpJd0)

[高德打车网络及接入层现状、演进与规范](https://topic.atatech.org/articles/200561)

[高德打车下单服务从单体到平台级服务的演进](https://www.atatech.org/articles/201942#37)

[高德打车订单状态机治理谈单体应用拆分](https://topic.atatech.org/articles/202596)

[高德打车-出租车巡网融合演进](https://topic.atatech.org/articles/202945)

[高德打车渠道服务治理-渠道基础服务建设](https://topic.atatech.org/articles/203262)

[高德打车服务化架构升级-支付数据治理](https://topic.atatech.org/articles/203494)

[高德打车-复杂结算业务架构设计](https://topic.atatech.org/articles/204257)

[高德打车通用可编排订单状态机引擎设计](https://topic.atatech.org/articles/203762)

[高德打车服务化架构升级-订单详情读服务治理](https://topic.atatech.org/articles/203963)

[高德打车发票系统的服务化与演进过程](https://topic.atatech.org/articles/204484)

  

**业务实践**

[高德打车-出租车智能计价器实践](https://www.atatech.org/articles/196651)

[聚合打车平台行业难题：高德打车司机拒载问题解决之道](https://topic.atatech.org/articles/205876)

[高德打车服务保障：规则引擎助力司机精细化管控](https://topic.atatech.org/articles/206668)

[高德打车首创-聚合模式下的多维度司机推荐创新](https://topic.atatech.org/articles/208336)

[你打车，我护航-高德打车弹外安全体系建设](https://topic.atatech.org/articles/208370)


把分库规则设置为userid+gmt_create 把。
那传订单 id 怎么办？那直接三个都拿到。



# 库表路由规则
- 库序号：Hash(userId)  % n
- 表序号：Hash(userId)/ m % m 表之所以要这这样目的是为了数据分布均匀