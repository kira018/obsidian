

# Java提供的常见集合

集合分单列和双列

- Collection:
  - List (有序,可重复)
    - Vector(向量):线程安全,数组
    - ArrayList:底层是动态数组,线程不安全
    - LinkedList:底层是双向链表,线程不安全
  - set(无序,不重复,通过hash+equals去重)
    - HashSet :hashset实现了HashMap所有方法,不可重复,散列表
    - TreeSet :二叉树,不重复,自动排序
  - Queue:队列集合
- Map:
  - HashMap:线程不安全,数组+链表+红黑树
  - HashTable:数组+链表,线程安全
  - CucureterHashMap:线程安全

# List

## ArrayList底层是如何实现的？

**底层**

ArrayList底层是动态数组实现的

**add方法流程**

- 第一次add他会有个cacalataCapacity来计算当前是否需要扩容,第一次的话默认扩容长度为10,然后添加到size+1的下标位置返回true添加成功
- 第二次的话他会判断size实际占用长度+1是否大于elementdata.length的长度,如果大于他会调用方法grow来扩容,扩容长度是旧数组长度的1.5倍,扩容完之后他会把旧数据使用arrys.copyOf到新数组然后返回true

## ArrayList list=new ArrayList(10)中的list扩容几次

不扩容

## ArrayList扩容，使用的时候要注意什么

1. 线程安全
2. 增删多不要用
3. 扩容的时候比较消耗性能,如果能预先知道数据大小可以提前预设好集合大小

## 如何实现数组和List之间的转换

- 数组转List:使用Arrays.aslist可以转换,这里是直接地址引用,所以原数组的值如果修改,list的值也会跟着修改
- list转数组:使用List.toArray可以转为数组,这里底层是直接copyOf所以修改数据不会跟着修改

## ArrayList 和 LinkedList 的区别是什么？

**从数据结构方面来说**

- ArrayList底层是动态数组

- LinkedList底层是双向链表

**从效率上来说**

- ArrayList:
  - 根据下标查询的时间复杂度是O(1)
  - 而如果是查未知索引则需要遍历时间复杂度是O(n)
  - 删除和添加都要挪动数组所以是O(n),除非是尾部添加就是O(1)
- LinkedList
  - 由于底层是双向链表查询链表头尾时间复杂度都是O(1)
  - 然后其他位置效率相对数组来说删除和添加只需要改变前驱和后继节点的指向所以比数组效率高
  - 注意，这里有个陷阱，LinkedList 更利于增删不是体现在时间复杂度上，因为二者增删的时间复杂度都是 O(n)，都需要遍历列表；而是**体现在增删的效率上**，因为 LinkedList 的增删只需要改变引用，而 ArrayList 的增删可能需要移动元素。

**空间上来说**

- ArrayList:更省空间，ArrayList 是基于数组的，是一块连续的内存空间，所以它的内存占用是比较紧凑的；但如果涉及到扩容，就会重新分配内存，空间是原来的 1.5 倍，存在一定的空间浪费。
- LinkedList:LinkedList 是基于链表的，每个节点都有一个指向下一个节点和上一个节点的引用，于是每个节点占用的内存空间稍微大一点。

**线程安全来说**

- 俩个都不是线程安全的,线程安全可以使用vector

**使用场景上**

ArrayList 适用于：

- 随机访问频繁：需要频繁通过索引访问元素的场景。
- 读取操作远多于写入操作：如存储不经常改变的列表。
- 末尾添加元素：需要频繁在列表末尾添加元素的场景。

LinkedList 适用于：

- 频繁插入和删除：在列表中间频繁插入和删除元素的场景。
- 不需要快速随机访问：顺序访问多于随机访问的场景。
- 队列和栈：由于其双向链表的特性，LinkedList 可以高效地实现队列（FIFO）和栈（LIFO）。

## 为什么ArrayList不是线程安全的，具体来说是哪里不安全？ 


在高并发添加数据下，ArrayList会暴露三个问题;

* 部分值为null（我们并没有add null进去）
* 索引越界异常
* size与我们add的数量不符

下面我们来分析三种情况都是如何产生的：


* **部分值为nul**l：当线程1走到了扩容那里发现当前size是9，而数组容量是10，所以不用扩容，这时候cpu让出执行权，线程2也进来了，发现size是9，而数组容量是10，所以不用扩容，这时候线程1继续执行，将数组下标索引为9的位置set值了，还没有来得及执行size++，这时候线程2也来执行了，又把数组下标索引为9的位置set了一遍，这时候两个先后进行size++，导致下标索引10的地方就为null了。
* **索引越界异常**：线程1走到扩容那里发现当前size是9，数组容量是10不用扩容，cpu让出执行权，线程2也发现不用扩容，这时候数组的容量就是10，而线程1 set完之后size++，这时候线程2再进来size就是10，数组的大小只有10，而你要设置下标索引为10的就会越界（数组的下标索引从0开始）；
* **size与我们add的数量不符**：这个基本上每次都会发生，这个理解起来也很简单，因为size++本身就不是原子操作，可以分为三步：获取size的值，将size的值加1，将新的size值覆盖掉原来的，线程1和线程2拿到一样的size值加完了同时覆盖，就会导致一次没有加上，所以肯定不会与我们add的数量保持一致的；

## 线程安全的List怎么实现？ *

可以使用 `Collections.synchronizedList()` 方法，它将返回一个线程安全的 List。

```java
SynchronizedList list = Collections.synchronizedList(new ArrayList());
```

内部是通过 synchronized 关键字加锁来实现的。

也可以直接使用 CopyOnWriteArrayList，它是线程安全的，遵循写时复制的原则，每当对列表进行修改（例如添加、删除或更改元素）时，都会创建列表的一个新副本，这个新副本会替换旧的列表，而对旧列表的所有读取操作仍然可以继续。

```java
CopyOnWriteArrayList list = new CopyOnWriteArrayList();
```

通俗的讲，CopyOnWrite 就是当我们往一个容器添加元素的时候，不直接往容器中添加，而是先复制出一个新的容器，然后在新的容器里添加元素，添加完之后，再将原容器的引用指向新的容器。多个线程在读的时候，不需要加锁，因为当前容器不会添加任何元素。这样就实现了线程安全。

### copyOnWriteArrayList 了解多少

CopyOnWriteArrayList 就是线程安全版本的 ArrayList。

它的名字叫`CopyOnWrite`——写时复制，已经明示了它的原理。

CopyOnWriteArrayList 采用了一种读写分离的并发策略。CopyOnWriteArrayList 容器允许并发读，读操作是无锁的，性能较高。至于写操作，比如向容器中添加一个元素，首先会先加上锁，然后将当前容器复制一份，然后在新副本上执行写操作，结束之后再将原容器的引用指向新容器。

所以实际上写时复制是用来读写分离高并发读的。

而加锁才是真正用来保证线程安全的



![CopyOnWriteArrayList原理](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-7.png)

# Map

## HashMap

### HashMap底层数据结构

HashMap分JDK1.7和1.8版本

- 1.7的话底层是数组+链表,头插法
- 1.8数组+链表+红黑树,尾插法

> 为什么需要更改数据结构

这是因为数组+链表的话会有个问题就是,假设很多数据存储在链表里,查询起来时间复杂度为O(n)效率太低了

> 为什么改为尾插法?	

主要是为了避免头插法在多线程的情况下出现死循环的问题

### 寻址算法

将hash值右移16位然后与第一次的hash异或得到二次hash，其实就是让 hashcode 的高 16 位和低 16 位进行异或操作。

> 为什么要二次hash呢?

只有一次hash的话会出现hash分布不均匀的情况,想想也知道,如果一个hash桶下面有多个数据,而一个hash桶下面有只有一点数据,那么存储上来说分布不均匀,查询上来说性能不稳定,所以**二次hash目的就是为了让他们数据分布均匀**

```java
static final int hash(Object key) {
    int h;
    // key的hashCode和key的hashCode右移16位做异或运算
    return (key == null) ? 0 : (h = key.hashCode()) ^ (h >>> 16);
}
```

这么设计是为了降低哈希碰撞的概率。

### 为什么 hash 函数能降哈希碰撞？ *

这个其实是因为hashmap为了提升性能，把取余改为hash & (n-1)  ，而这个在底层举个例子比如说n为16那么就是对hash的低4位按位与操作得到下标，这个性能更高，但是带来另一个问题，这里计算下标只用上了低位做下标计算，高位浪费了，所以hashcode算法采用二次hash将高16位右移16位然后和原hash进行^运算，实际上就是高位和低位的异或运算，这样做让原本的只有低位进行下标计算变成高位和低位共同进行下标计算，所以能使得hash分布均匀，并且这里设计16位是考虑到int是32位，然后高低位要进行计算，所以就一半一半，只移动16位最大化的让高位和低位参与运算

> &:按位与

### Put流程

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211347310.png)

他会根据寻址算法计算你的key得到一个hashcode,这个寻址算法是二次hashcode,第一次是使用key.hashcode来获取hash值,然后第二次是对这个hash值异或运算然后将他二进制位右移16为,二次hash目的是为了将数据存储分布均匀,然后根据这个hashcode也就是源码的table[i]数组这个位置他会去判断是否为null,如果为null说明是第一次存储吗,就直接添加就可以了,然后假如说不为null,他会继续去判断,判断当前table[i]的key是否跟我要添加数据的key相同,相同的话就覆盖,不相同的话他会继续判断,他会判断是不是红黑树,如果是红黑树就按红黑树来添加,如果是链表他会去遍历链表.然后判断是否有key等于当前要添加数据的key,如果相等覆盖嘛,如果没有同样的key则尾插法,这里提一嘴,不用头插是因为1.7版本的HashMap就是头插但是在多线程的情况下会有死循环的问题,而尾插法很好的解决了这个问题,然后回到刚刚的步骤好像是降到了尾插到链表,查完之后会判断链表长度是否大于8大于的话转化为红黑树,然后按红黑树添加数据,添加的时候他会判断是否大于扩容阈值,大于的话说明需要扩容了,这里扩容resize方法是按旧数据的2倍进行扩容,他这个扩容机制我可以讲一下,第一次扩容的话是长为16扩容阈值为12的数组,这个扩容阈值是16*0.75扩容因子算出来的,然后如果是第二次扩容他会遍历数组,然后判断e.next是否为null,如果为null则直接根据hash来存储到新数组,如果不为null则看是红黑树还是链表,是红黑树就按红黑树添加,是链表就遍历然后根据与运算特定算法添加,最后结束所有流程

### 扩容机制

扩容时，HashMap 会创建一个新的数组，其容量是原数组容量的两倍。然后将键值对放到新计算出的索引位置上。一部分索引不变，另一部分索引为“原索引+旧容量”。这样可以减少hash冲突的情况，这个功劳既属于新的哈希算法，也离不开 n 为 2 的整数次幂这个前提，这是它俩通力合作后的结果 `hash & (newCapacity - 1)`

#### 那你说说扩容的时候每个节点都要进行位运算吗，如果我这个 HashMap 里面有几十万条数据，都要进行位运算吗？

在 JDK 8 的新 hash 算法下，数组扩容后的索引位置，要么就是原来的索引位置，要么就是“原索引+原来的容量”，遵循一定的规律。

具体来说，就是判断原哈希值的高位中新增的那一位是否为 1，如果是，该元素会被移动到原位置加上旧容量的位置；如果不是，则保持在原位置。

所以，尽管有几十万条数据，每个数据项的位置决定仅需要一次简单的位运算。位运算的计算速度非常快，因此，尽管扩容操作涉及遍历整个哈希表并对每个节点进行操作，但这部分操作的计算成本是相对较低的。

### HashMap手动输入初始容量会发生什么? *

会讲数组长度向上取整,这个取整是2的n次幂,比如17我会取整到18

> 数组长度为什么是2的n次幂?

1. 这是因为源码底层为了更高效的计算下标把取模替换成了数组长度- 1然后对hash值进行与运算得到数组下标,在数组长度位2的n次方的时候比如16 -1 & hash计算出来是正常的下标,但是如果是15 -1 & hash值会变为0
2. 在一个就是扩容的时候涉及到重新计算hash位置,而他计算方式数组如果是2的次方效率会更好
3. HashMap 的容量是 2 的倍数，或者说是 2 的整数次幂，是为了快速定位元素的下标：HashMap 在定位元素位置时，先通过 `hash(key) = (h = key.hashCode()) ^ (h >>> 16)` 计算出哈希值，再通过 `hash & (n-1)` 来定位元素位置的，n 为数组的大小，也就是 HashMap 的容量。因为（数组长度-1）正好相当于一个“低位掩码”——这个掩码的低位最好全是 1，这样 & 操作才有意义，否则结果就肯定是 0。

   > &：有0得0

### HashMap是线程安全的吗？ *

hashmap不是线程安全的，hashmap在多线程会存在下面的问题：

* JDK1.7中的 HashMap 使用头插法插入元素，在多线程的环境下，扩容的时候有可能导致环形链表的出现，形成死循环。因此，JDK1.8使用尾插法插入元素，在扩容时会保持链表元素原本的顺序，不会出现环形链表的问题。
* ==多线程同时执行 put 操作==，如果计算出来的索引位置是相同的，那会造成前一个 key 被后一个 key 覆盖，从而导致元素的丢失。此问题在JDK 1.7和 JDK 1.8 中都存在。

### 了解的哈希冲突解决方法有哪些？ 

* **拉链法**：使用链表或其他数据结构来存储冲突的键值对，将它们链接在同一个哈希桶中。(Redis/HashMap)
* 开放寻址法：在哈希表中找到另一个可用的位置来存储冲突的键值对，而不是存储在链表中。常见的开放寻址方法包括线性探测、二次探测和双重散列。
* **再哈希法**（Rehashing）：当发生冲突时，使用另一个哈希函数再次计算键的哈希值，直到找到一个空槽来存储键值对。
* **哈希桶扩容**：当哈希冲突过多时，可以动态地扩大哈希桶的数量，重新分配键值对，以减少冲突的概率。

### 为什么 HashMap 链表转红黑树的阈值为 8 呢？ *

红黑树节点的大小大概是普通节点大小的两倍，所以转红黑树，牺牲了空间换时间，更多的是一种兜底的策略，保证极端情况下的查找效率。

阈值为什么要选 8 呢？和统计学有关。理想情况下，使用随机哈希码，链表里的节点符合泊松分布，出现节点个数的概率是递减的，节点个数为 8 的情况，发生概率仅为`0.00000006`。

事实上，**链表长度超过 8 就转为红黑树的设计，更多的是为了防止用户自己实现了不好的哈希算法时导致链表过长，从而导致查询效率低，** 而此时转为红黑树更多的是一种保底策略，用来保证极端情况下查询的效率。

至于红黑树转回链表的阈值为什么是 6，而不是 8？是因为如果这个阈值也设置成 8，假如发生碰撞，节点增减刚好在 8 附近，会发生链表和红黑树的不断转换，导致资源浪费。

### 那么为什么选择了 0.75 作为 HashMap 的默认加载因子呢？ *

简单来说，这是对`空间`成本和`时间`成本平衡的考虑。

在 HashMap 中有这样一段注释：

![关于默认负载因子的注释](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-24.png)关于默认负载因子的注释

我们都知道，HashMap 的散列构造方式是 Hash 取余，负载因子决定元素个数达到多少时候扩容。

假如我们设的比较大，元素比较多，空位比较少的时候才扩容，那么发生哈希冲突的概率就增加了，查找的时间成本就增加了。

我们设的比较小的话，元素比较少，空位比较多的时候就扩容了，发生哈希碰撞的概率就降低了，查找时间成本降低，但是就需要更多的空间去存储元素，空间成本就增加了。

### HashMap死循环问题 

hashmap死循环问题实在扩容的时候出现的问题,在出现并发的情况下,多线程同时扩容,比如线程X Y

![image-20241022203838590](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410222038762.png)

这里线程X先执行,AB移过去,新链表就BA,然后时间片回到Y线程而对于线程Y来说,他并不知道你扩容结束了

![image-20241022203913764](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410222039863.png)

所以在线程Y进行扩容的时候,他以为旧链表的cur指针指向A实际上因为线程X这里cur是C,所以这时候线程Y将A指向新链表的头也就是B而对于线程X来说B->A这就导致了AB互相指向出现了环状

![image-20241022203943130](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410222039228.png)

### JDK 8 对 HashMap 主要做了哪些优化呢？为什么？ *

相比较 JDK 7，JDK 8 的 HashMap 主要做了四点优化：

①、底层数据结构由数组 + 链表改成了数组 + 链表或红黑树的结构。

原因：如果多个键映射到了同一个哈希值，链表会变得很长，在最坏的情况下，当所有的键都映射到同一个桶中时，性能会退化到 O(n)，而红黑树的时间复杂度是 O(logn)。

②、链表的插入方式由头插法改为了尾插法。

原因：头插法虽然简单快捷，但扩容后容易改变原来链表的顺序。

③、扩容的时机由插入时判断改为插入后判断。

原因：可以避免在每次插入时都进行不必要的扩容检查，因为有可能插入后仍然不需要扩容。

④、优化了哈希算法。

JDK 7 进行了多次移位和异或操作来计算元素的哈希值。

![二哥的 Java 进阶之路：JDK 7 的 hash 方法](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-20240512093223.png)

JDK 8 优化了这个算法，只进行了一次异或操作，但仍然能有效地减少冲突。

![二哥的 Java 进阶之路：JDK 8 的 hash 方法](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-20240512093327.png)

并且能够保证扩容后，元素的新位置要么是原位置，要么是原位置加上旧容量大小。

### HashMap一般用什么做Key？为啥String适合做Key呢？ 

用 string 做 key，因为 String对象是不可变的，一旦创建就不能被修改，这确保了Key的稳定性。如果Key是可变的，可能会导致hashCode和equals方法的不一致，进而影响HashMap的正确性。

### 怎么判断 key 相等呢？ *

`HashMap`判断两个`key`是否相等，依赖于`key`的`equals()`方法和`hashCode()`方法，以及 `==` 运算符。

```java
if (e.hash == hash &&
((k = e.key) == key || (key != null && key.equals(k))))
```

①、**hashCode()** ：首先，使用`key`的`hashCode()`方法计算`key`的哈希码。由于不同的`key`可能有相同的哈希码，`hashCode()`只是第一步筛选。

②、**equals()** ：当两个`key`的哈希码相同时，`HashMap`还会调用`key`的`equals()`方法进行精确比较。只有当`equals()`方法返回`true`时，两个`key`才被认为是完全相同的。

③、==：当然了，如果两个`key`的引用指向同一个对象，那么它们的`hashCode()`和`equals()`方法都会返回`true`，所以在 equals 判断之前会优先使用`==`运算符判断一次。

### hashmap key可以为null吗？ 


可以为 null。

* hashMap中使用hash()方法来计算key的哈希值，当key为空时，直接另key的哈希值为0，不走key.hashCode()方法；

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211347659.png)

* hashMap虽然支持key和value为null，但是null作为key只能有一个，null作为value可以有多个；
* 因为hashMap中，如果key值一样，那么会覆盖相同key值的value为最新，所以key为null只能有一个。

### 有什么办法能解决 HashMap 线程不安全的问题呢？

HashMap 不是线程安全的，因此在早期的 JDK 版本中，是用 Hashtable 来保证线程安全的。

①、Hashtable 是直接在方法上加 [synchronized 关键字](https://javabetter.cn/thread/synchronized-1.html)，比较粗暴。

![二哥的 Java 进阶之路：Hashtable](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-20240323125211.png)

因此在 Java 的后期版本中，更推荐使用 [ConcurrentHashMap](https://javabetter.cn/thread/ConcurrentHashMap.html)和`Collections.synchronizedMap(Map)`包装器。

②、`Collections.synchronizedMap` 返回的是 [Collections](https://javabetter.cn/common-tool/collections.html) 工具类的内部类。

![二哥的 Java 进阶之路：Collections.synchronizedMap](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-20240323125418.png)

内部是通过 synchronized 对象锁来保证线程安全的。

③、[ConcurrentHashMap](https://javabetter.cn/thread/ConcurrentHashMap.html) 在 JDK 7 中使用分段锁，在 JKD 8 中使用 [CAS（Compare-And-Swap）](https://javabetter.cn/thread/cas.html)+ [synchronized 关键字](https://javabetter.cn/thread/synchronized-1.html)，性能得到了进一步提升。

### HashMap 内部节点是有序的吗？*

HashMap 是无序的，根据 hash 值随机插入。如果想使用有序的 Map，可以使用 LinkedHashMap 或者 TreeMap。

### 红黑树/平衡树/二叉树区别 

1.首先,二叉树如果是一个有序序列,比如123456,那么此时树结构会变得像一个链表一样,那么我们知道链表查询复杂度o(n)还是比较差的

2.那平衡二叉树呢,他是会进行自旋的,每插入一个节点他都会判断树高之间的差值,如果差值超过1,也就是说如果最矮的树和最高的树之间差2,这个时候就会开始自旋,使其高度差为小于等于1

3.红黑树也会自旋,但是他俩区别就在于,红黑树自旋条件是最高树如果是最矮树的俩倍才开始自旋,所以基于这个特点,就可以得出如果使用avl(平衡二叉树)的话每次添加删除都需要去移动树,这个操作太频繁,所以相对来说虽然查询比红黑树好,但是添加删除相对没那么好,红黑树是一种折中的方案，查找、插入、删除的时间复杂度都是 `O(log n)`。

## ConcurrentHashMap 

### ConcurrentHashMap原理

ConcurrentHashMap 是一种线程安全的高效Map集合，jdk1.7和1.8也做了很多调整。

- JDK1.7的底层采用是**分段的数组**+**链表** 实现,锁分段也就是segment的某个下标,粒度较大
- JDK1.8 采用的数据结构跟HashMap1.8的结构一样，数组+链表/红黑二叉树。锁粒度是头节点

在jdk1.7中 ConcurrentHashMap 里包含一个 Segment 数组默认长度为16,不可扩容。一个 Segment 包含一个 HashEntry 数组，每个 HashEntry 是一个链表结构 的元素,并且采用的是对数组分多段来锁来减少锁粒度,但是在并发特别高的情况下效率不够好

先是它的数据结构与jdk1.8的HashMap数据结构完全一致。其次是放弃了Segment臃肿的设计，取而代之的是采用Node + CAS/Synchronized来保 证并发安全进行实现，具体是Synchronized还是cas主要看锁的竞争激烈不激烈,竞争激烈用Synchronized,竞争不激烈用CAS,因为竞争激烈的情况下CAS会多次自旋等待效率不够好

### 已经用了synchronized，为什么还要用CAS呢？ 


主要是根据锁竞争程度来判断的。

比如：在put时，如果计算出来的hash槽没有存放元素，那么就可以直接使用CAS来进行设置值，这是因为在设置元素的时候，因为hash值经过了各种扰动后，造成hash碰撞的几率较低，那么我们可以预测使用较少的自旋来完成具体的hash落槽操作。

当发生了hash碰撞的时候说明容量不够用了或者已经有大量线程访问了，因此这时候使用synchronized来处理hash碰撞比CAS效率要高，因为发生了hash碰撞大概率来说是线程竞争比较强烈。

> 竞争激烈使用synchronized,竞争不激烈使用cas+volatile

### ConcurrentHashMap用了悲观锁还是乐观锁? 

悲观锁和乐观锁都有用到。

添加元素时首先会判断容器是否为空：

* 如果为空则使用 volatile 加 **CAS （乐观锁）** 来初始化。
* 如果容器不为空，则根据存储的元素计算该位置是否为空。
* 如果根据存储的元素计算结果为空，则利用 **CAS（乐观锁）** 设置该节点；
* 如果根据存储的元素计算结果不为空，则使用 **synchronized（悲观锁）** ，然后，遍历桶中的数据，并替换或新增节点到桶中，最后再判断是否需要转为红黑树，这样就能保证并发访问时的线程安全了。

## HashSet与HashMap的区别？

HashSet是无序不可重复,实现了HashMap的所有方法

## HashTable与ConcurrentHashMap的区别 

**底层数据结构**：

* jdk7之前的ConcurrentHashMap底层采用的是**分段的数组+链表**实现，jdk8之后采用的是**数组+链表/红黑树；**
* HashTable采用的是**数组+链表**，数组是主体，链表是解决hash冲突存在的。

**实现线程安全的方式：**

* jdk8以前，ConcurrentHashMap采用分段锁，对整个数组进行了分段分割，每一把锁只锁容器里的一部分数据，多线程访问不同数据段里的数据，就不会存在锁竞争，提高了并发访问；jdk8以后，直接采用数组+链表/红黑树，**并发控制使用CAS和synchronized操作**，更加提高了速度。
* HashTable：**所有的方法都加了锁来保证线程安全，但是效率非常的低下**，当一个线程访问同步方法，另一个线程也访问的时候，就会陷入阻塞或者轮询的状态。

## TreeMap 和 HashMap 的区别*

在没有发生哈希冲突的情况下，HashMap 的查找效率是 `O(1)`。适用于查找操作比较频繁的场景。

TreeMap 的查找效率是 `O(logn)`。并且保证了元素的顺序，因此适用于需要大量范围查找或者有序遍历的场景。

# 讲讲 HashSet 的底层实现？

## HashSet底层实现

HashSet 其实是由 HashMap 实现的，只不过值由一个固定的 Object 对象填充，而键用于操作。

```java
public class HashSet<E>
    extends AbstractSet<E>
    implements Set<E>, Cloneable, java.io.Serializable
{
    static final long serialVersionUID = -5024744406713321676L;
    private transient HashMap<E,Object> map;
    // Dummy value to associate with an Object in the backing Map
    private static final Object PRESENT = new Object();
    // ……
}
```

实际开发中，HashSet 并不常用，比如，如果我们需要按照顺序存储一组元素，那么 ArrayList 和 LinkedList 可能更适合；如果我们需要存储键值对并根据键进行查找，那么 HashMap 可能更适合。

HashSet 主要用于去重，比如，我们需要统计一篇文章中有多少个不重复的单词，就可以使用 HashSet 来实现。

**HashSet 会自动去重，因为它是用 HashMap 实现的，HashMap 的键是唯一的（哈希值），相同键的值会覆盖掉原来的值，于是第二次 set.add("沉默") 的时候就覆盖了第一次的 set.add("沉默")。**

```java
public boolean add(E e) {
    return map.put(e, PRESENT)==null;
}
```

![三分恶面渣逆袭：HashSet套娃](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/collection-36.png)

## HashSet 和 ArrayList 的区别

- ArrayList 是基于动态数组实现的，HashSet 是基于 HashMap 实现的。
- ArrayList 允许重复元素和 null 值，可以有多个相同的元素；HashSet 保证每个元素唯一，不允许重复元素，基于元素的 hashCode 和 equals 方法来确定元素的唯一性。
- ArrayList 保持元素的插入顺序，可以通过索引访问元素；HashSet 不保证元素的顺序，元素的存储顺序依赖于哈希算法，并且可能随着元素的添加或删除而改变。

## HashSet 怎么判断元素重复，重复了是否 put

HashSet 的 add 方法是通过调用 HashMap 的 put 方法实现的：

```java
public boolean add(E e) {
    return map.put(e, PRESENT)==null;
}
```

所以 HashSet 判断元素重复的逻辑底层依然是 HashMap 的底层逻辑