引用参考:

[图解｜索引覆盖、索引下推以及如何避免索引失效](https://zhuanlan.zhihu.com/p/481750465)

[小林coding](https://xiaolincoding.com/mysql/)

hollins八股文

# MySQL

## SQL基础

### 执行一条 select 语句，期间发生了什么？ 

![查询语句执行流程](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410180934974.png)



可以看到， MySQL 的架构共分为两层：**Server 层和存储引擎层**，

- **Server 层负责建立连接、分析和执行 SQL**。MySQL 大多数的核心功能模块都在这实现，主要包括连接器，查询缓存、解析器、预处理器、优化器、执行器等。另外，所有的内置函数（如日期、时间、数学和加密函数等）和所有跨存储引擎的功能（如存储过程、触发器、视图等。）都在 Server 层实现。
- **存储引擎层负责数据的存储和提取**。支持 InnoDB、MyISAM、Memory 等多个存储引擎，不同的存储引擎共用一个 Server 层。现在最常用的存储引擎是 InnoDB，从 MySQL 5.5 版本开始， InnoDB 成为了 MySQL 的默认存储引擎。我们常说的索引数据结构，就是由存储引擎层实现的，不同的存储引擎支持的索引类型也不相同，比如 InnoDB 支持索引类型是 B+树 ，且是默认使用，也就是说在数据表中创建的主键索引和二级索引默认使用的是 B+ 树索引。

好了，现在我们对 Server 层和存储引擎层有了一个简单认识，接下来，就详细说一条 SQL 查询语句的执行流程，依次看看每一个功能模块的作用。 

#### 第一步：连接器

如果你在 Linux 操作系统里要使用 MySQL，那你第一步肯定是要先连接 MySQL 服务，然后才能执行 SQL 语句，普遍我们都是使用下面这条命令进行连接：

```shell
# -h 指定 MySQL 服务得 IP 地址，如果是连接本地的 MySQL服务，可以不用这个参数；
# -u 指定用户名，管理员角色名为 root；
# -p 指定密码，如果命令行中不填写密码（为了密码安全，建议不要在命令行写密码），就需要在交互对话里面输入密码
mysql -h$ip -u$user -p
```

连接的过程需要先经过 TCP 三次握手，因为 MySQL 是基于 TCP 协议进行传输的，如果 MySQL 服务并没有启动，则会收到如下的报错：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/mysql连接错误.png)

如果 MySQL 服务正常运行，完成 TCP 连接的建立后，连接器就要开始验证你的用户名和密码，如果用户名或密码不对，就收到一个"Access denied for user"的错误，然后客户端程序结束执行。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/密码错误.png)

如果用户密码都没有问题，连接器就会获取该用户的权限，然后保存起来，后续该用户在此连接里的任何操作，都会基于连接开始时读到的权限进行权限逻辑的判断。

所以，如果一个用户已经建立了连接，即使管理员中途修改了该用户的权限，也不会影响已经存在连接的权限。修改完成后，只有再新建的连接才会使用新的权限设置。

> 如何查看 MySQL 服务被多少个客户端连接了？

如果你想知道当前 MySQL 服务被多少个客户端连接了，你可以执行 `show processlist` 命令进行查看。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/查看连接.png)

比如上图的显示结果，共有两个用户名为 root 的用户连接了 MySQL 服务，其中 id 为 6 的用户的 Command 列的状态为 `Sleep` ，这意味着该用户连接完 MySQL 服务就没有再执行过任何命令，也就是说这是一个空闲的连接，并且空闲的时长是 736 秒（ Time 列）。

> 空闲连接会一直占用着吗？

当然不是了，MySQL 定义了空闲连接的最大空闲时长，由 `wait_timeout` 参数控制的，默认值是 8 小时（28880秒），如果空闲连接超过了这个时间，连接器就会自动将它断开。

```sql
mysql> show variables like 'wait_timeout';
+---------------+-------+
| Variable_name | Value |
+---------------+-------+
| wait_timeout  | 28800 |
+---------------+-------+
1 row in set (0.00 sec)
```

当然，我们自己也可以手动断开空闲的连接，使用的是 kill connection + id 的命令。

```sql
mysql> kill connection +6;
Query OK, 0 rows affected (0.00 sec)
```

一个处于空闲状态的连接被服务端主动断开后，这个客户端并不会马上知道，等到客户端在发起下一个请求的时候，才会收到这样的报错“ERROR 2013 (HY000): Lost connection to MySQL server during query”。

> MySQL 的连接数有限制吗？

MySQL 服务支持的最大连接数由 max_connections 参数控制，比如我的 MySQL 服务默认是 151 个,超过这个值，系统就会拒绝接下来的连接请求，并报错提示“Too many connections”。

```sql
mysql> show variables like 'max_connections';
+-----------------+-------+
| Variable_name   | Value |
+-----------------+-------+
| max_connections | 151   |
+-----------------+-------+
1 row in set (0.00 sec)
```

MySQL 的连接也跟 HTTP 一样，有短连接和长连接的概念，它们的区别如下：

```c
// 短连接
连接 mysql 服务（TCP 三次握手）
执行sql
断开 mysql 服务（TCP 四次挥手）

// 长连接
连接 mysql 服务（TCP 三次握手）
执行sql
执行sql
执行sql
....
断开 mysql 服务（TCP 四次挥手）
```

可以看到，使用长连接的好处就是可以减少建立连接和断开连接的过程，所以一般是推荐使用长连接。

但是，使用长连接后可能会占用内存增多，因为 MySQL 在执行查询过程中临时使用内存管理连接对象，这些连接对象资源只有在连接断开时才会释放。如果长连接累计很多，将导致 MySQL 服务占用内存太大，有可能会被系统强制杀掉，这样会发生 MySQL 服务异常重启的现象。

> 怎么解决长连接占用内存的问题？

有两种解决方式。

第一种，**定期断开长连接**。既然断开连接后就会释放连接占用的内存资源，那么我们可以定期断开长连接。

第二种，**客户端主动重置连接**。MySQL 5.7 版本实现了 `mysql_reset_connection()` 函数的接口，注意这是接口函数不是命令，那么当客户端执行了一个很大的操作后，在代码里调用 mysql_reset_connection 函数来重置连接，达到释放内存的效果。这个过程不需要重连和重新做权限验证，但是会将连接恢复到刚刚创建完时的状态。

至此，连接器的工作做完了，简单总结一下：

- 与客户端进行 TCP 三次握手建立连接；
- 校验客户端的用户名和密码，如果用户名或密码不对，则会报错；
- 如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限；

#### 第二步：查询缓存

连接器得工作完成后，客户端就可以向 MySQL 服务发送 SQL 语句了，MySQL 服务收到 SQL 语句后，就会解析出 SQL 语句的第一个字段，看看是什么类型的语句。

如果 SQL 是查询语句（select 语句），MySQL 就会先去查询缓存（ Query Cache ）里查找缓存数据，看看之前有没有执行过这一条命令，这个查询缓存是以 key-value 形式保存在内存中的，key 为 SQL 查询语句，value 为 SQL 语句查询的结果。

如果查询的语句命中查询缓存，那么就会直接返回 value 给客户端。如果查询的语句没有命中查询缓存中，那么就要往下继续执行，等执行完后，查询的结果就会被存入查询缓存中。

这么看，查询缓存还挺有用，但是其实**查询缓存挺鸡肋**的。

对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。如果刚缓存了一个查询结果很大的数据，还没被使用的时候，刚好这个表有更新操作，查询缓冲就被清空了，相当于缓存了个寂寞。

所以，MySQL 8.0 版本直接将查询缓存删掉了，也就是说 MySQL 8.0 开始，执行一条 SQL 查询语句，不会再走到查询缓存这个阶段了。

对于 MySQL 8.0 之前的版本，如果想关闭查询缓存，我们可以通过将参数 query_cache_type 设置成 DEMAND。

TIP

这里说的查询缓存是 server 层的，也就是 MySQL 8.0 版本移除的是 server 层的查询缓存，并不是 Innodb 存储引擎中的 buffer pool。

#### 第三步：解析 SQL

在正式执行 SQL 查询语句之前， MySQL 会先对 SQL 语句做解析，这个工作交由「解析器」来完成。

##### 解析器

解析器会做如下两件事情。

第一件事情，**词法分析**。MySQL 会根据你输入的字符串识别出关键字出来，例如，SQL语句 select username from userinfo，在分析之后，会得到4个Token，其中有2个Keyword，分别为select和from：

| 关键字 | 非关键字 | 关键字 | 非关键字 |
| ------ | -------- | ------ | -------- |
| select | username | from   | userinfo |

第二件事情，**语法分析**。根据词法分析的结果，语法解析器会根据语法规则，判断你输入的这个 SQL 语句是否满足 MySQL 语法，如果没问题就会构建出 SQL 语法树，这样方便后面模块获取 SQL 类型、表名、字段名、 where 条件等等。

![img](https://cdn.xiaolincoding.com//picgo/db-mysql-sql-parser-2.png)

如果我们输入的 SQL 语句语法不对，就会在解析器这个阶段报错。比如，我下面这条查询语句，把 from 写成了 form，这时 MySQL 解析器就会给报错。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/语法错误.png)

但是注意，表不存在或者字段不存在，并不是在解析器里做的，《MySQL 45 讲》说是在解析器做的，但是经过我和朋友看 MySQL 源码（5.7和8.0）得出结论是解析器只负责检查语法和构建语法树，但是不会去查表或者字段存不存在。

那到底谁来做检测表和字段是否存在的工作呢？别急，接下来就是了。

#### 第四步：执行 SQL

经过解析器后，接着就要进入执行 SQL 查询语句的流程了，每条`SELECT` 查询语句流程主要可以分为下面这三个阶段：

- prepare 阶段，也就是预处理阶段；
- optimize 阶段，也就是优化阶段；
- execute 阶段，也就是执行阶段；

##### 预处理器

我们先来说说预处理阶段做了什么事情。

- 检查 SQL 查询语句中的表或者字段是否存在；
- 将 `select *` 中的 `*` 符号，扩展为表上的所有列；

我下面这条查询语句，test 这张表是不存在的，这时 MySQL 就会在执行 SQL 查询语句的 prepare 阶段中报错。

```sql
mysql> select * from test;
ERROR 1146 (42S02): Table 'mysql.test' doesn't exist
```

这里贴个 MySQL 8.0 源码来证明表或字段是否存在的判断，不是在解析器里做的，而是在 prepare 阶段。（*PS：下图是公众号「一树一溪」老哥帮我分析的，这位老哥专门写 MySQL 源码文章，感兴趣的朋友，可以微信搜索关注*）

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/表不存在.jpeg)

上面的中间部分是 MySQL 报错表不存在时的函数调用栈，可以看到表不存在的错误是在get_table_share() 函数里报错的，而这个函数是在 prepare 阶段调用的。

不过，对于 MySQL 5.7 判断表或字段是否存在的工作，是在词法分析&语法分析之后，prepare 阶段之前做的。结论都一样，不是在解析器里做的。代码我就不放了，正因为 MySQL 5.7 代码结构不好，所以 MySQL 8.0 代码结构变化很大，后来判断表或字段是否存在的工作就被放入到 prepare 阶段做了。

##### 优化器

经过预处理阶段后，还需要为 SQL 查询语句先制定一个执行计划，这个工作交由「优化器」来完成的。

**优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。

当然，我们本次的查询语句（select * from product where id = 1）很简单，就是选择使用主键索引。

要想知道优化器选择了哪个索引，我们可以在查询语句最前面加个 `explain` 命令，这样就会输出这条 SQL 语句的执行计划，然后执行计划中的 key 就表示执行过程中使用了哪个索引，比如下图的 key 为 `PRIMARY` 就是使用了主键索引。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/执行计划.png)

如果查询语句的执行计划里的 key 为 null 说明没有使用索引，那就会全表扫描（type = ALL），这种查询扫描的方式是效率最低档次的，如下图：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/全表扫描.png)

这张 product 表只有一个索引就是主键，现在我在表中将 name 设置为普通索引（二级索引）。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/产品表.png)

这时 product 表就有主键索引（id）和普通索引（name）。假设执行了这条查询语句：

```sql
select id from product where id > 1  and name like 'i%';
```

这条查询语句的结果既可以使用主键索引，也可以使用普通索引，但是执行的效率会不同。这时，就需要优化器来决定使用哪个索引了。

很显然这条查询语句是**覆盖索引**，直接在二级索引就能查找到结果（因为二级索引的 B+ 树的叶子节点的数据存储的是主键值），就没必要在主键索引查找了，因为查询主键索引的 B+ 树的成本会比查询二级索引的 B+ 的成本大，优化器基于查询成本的考虑，会选择查询代价小的普通索引。

在下图中执行计划，我们可以看到，执行过程中使用了普通索引（name），Exta 为 Using index，这就是表明使用了覆盖索引优化。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/选择索引.png)

##### 执行器

经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了，这个工作是由「执行器」完成的。在执行的过程中，执行器就会和存储引擎交互了，交互是以记录为单位的。

接下来，用三种方式执行过程，跟大家说一下执行器和存储引擎的交互过程（PS ：为了写好这一部分，特地去看 MySQL 源码，也是第一次看哈哈）。

- 主键索引查询
- 全表扫描
- 索引下推

###### 主键索引查询

以本文开头查询语句为例，看看执行器是怎么工作的。

```sql
select * from product where id = 1;
```

这条查询语句的查询条件用到了主键索引，而且是等值查询，同时主键 id 是唯一，不会有 id 相同的记录，所以优化器决定选用访问类型为 const 进行查询，也就是使用主键索引查询一条记录，那么执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为 InnoDB 引擎索引查询的接口，把条件 `id = 1` 交给存储引擎，**让存储引擎定位符合条件的第一条记录**。
- 存储引擎通过主键索引的 B+ 树结构定位到 id = 1的第一条记录，如果记录是不存在的，就会向执行器上报记录找不到的错误，然后查询结束。如果记录是存在的，就会将记录返回给执行器；
- 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。
- 执行器查询的过程是一个 while 循环，所以还会再查一次，但是这次因为不是第一次查询了，所以会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 const，这个函数指针被指向为一个永远返回 - 1 的函数，所以当调用该函数的时候，执行器就退出循环，也就是结束查询了。

至此，这个语句就执行完成了。

###### 全表扫描

举个全表扫描的例子：

```text
select * from product where name = 'iphone';
```

这条查询语句的查询条件没有用到索引，所以优化器决定选用访问类型为 ALL 进行查询，也就是全表扫描的方式查询，那么这时执行器与存储引擎的执行流程是这样的：

- 执行器第一次查询，会调用 read_first_record 函数指针指向的函数，因为优化器选择的访问类型为 all，这个函数指针被指向为 InnoDB 引擎全扫描的接口，**让存储引擎读取表中的第一条记录**；
- 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的（是的没错，Server 层每从存储引擎读到一条记录就会发送给客户端，之所以客户端显示的时候是直接显示所有记录的，是因为客户端是等查询语句查询完成后，才会显示出所有的记录）。
- 执行器查询的过程是一个 while 循环，所以还会再查一次，会调用 read_record 函数指针指向的函数，因为优化器选择的访问类型为 all，read_record 函数指针指向的还是 InnoDB 引擎全扫描的接口，所以接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端；
- 一直重复上述过程，直到存储引擎把表中的所有记录读完，然后向执行器（Server层） 返回了读取完毕的信息；
- 执行器收到存储引擎报告的查询完毕的信息，退出循环，停止查询。

至此，这个语句就执行完成了。

###### 索引下推

在这部分非常适合讲索引下推（MySQL 5.6 推出的查询优化策略），这样大家能清楚的知道，「下推」这个动作，下推到了哪里。

索引下推能够减少**二级索引**在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。

举一个具体的例子，方便大家理解，这里一张用户表如下，我对 age 和 reward 字段建立了联合索引（age，reward）：

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/路飞表.png)

现在有下面这条查询语句：

```sql
select * from t_user  where age > 20 and reward = 100000;
```

联合索引当遇到范围查询 (>、<) 就会停止匹配，也就是 **age 字段能用到联合索引，但是 reward 字段则无法利用到索引**。具体原因这里可以看这篇：[索引常见面试题(opens new window)](https://xiaolincoding.com/mysql/index/index_interview.html#按字段个数分类)

那么，不使用索引下推（MySQL 5.6 之前的版本）时，执行器与存储引擎的执行流程是这样的：

- Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
- 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后**进行回表操作**，将完整的记录返回给 Server 层；
- Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录；
- 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；
- 如此往复，直到存储引擎把表中的所有记录读完。

可以看到，没有索引下推的时候，每查询到一条二级索引记录，都要进行回表操作，然后将记录返回给 Server，接着 Server 再判断该记录的 reward 是否等于 100000。

而使用索引下推后，判断记录的 reward 是否等于 100000 的工作交给了存储引擎层，过程如下 ：

- Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
- 存储引擎定位到二级索引后，**先不执行回表**操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果**条件不成立**，则直接**跳过该二级索引**。如果**成立**，则**执行回表**操作，将完成记录返回给 Server 层。
- Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
- 如此往复，直到存储引擎把表中的所有记录读完。

可以看到，使用了索引下推后，虽然 reward 列无法使用到联合索引，但是因为它包含在联合索引（age，reward）里，所以直接在存储引擎过滤出满足 reward = 100000 的记录后，才去执行回表操作获取整个记录。相比于没有使用索引下推，节省了很多回表操作。

当你发现执行计划里的 Extr 部分显示了 “Using index condition”，说明使用了索引下推。

![img](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/索引下推执行计划.png)

------

总结

执行一条 SQL 查询语句，期间发生了什么？

- 连接器：建立连接，管理连接、校验用户身份；
- 查询缓存：查询语句如果命中查询缓存则直接返回，否则继续往下执行。MySQL 8.0 已删除该模块；
- 解析 SQL，通过解析器对 SQL 查询语句进行词法分析、语法分析，然后构建语法树，方便后续模块读取表名、字段、语句类型；
- 执行 SQL：执行 SQL 共有三个阶段：
  - 预处理阶段：检查表或字段是否存在；将 `select *` 中的 `*` 符号扩展为表上的所有列。
  - 优化阶段：基于查询成本的考虑， 选择查询成本最小的执行计划；
  - 执行阶段：根据执行计划执行 SQL 查询语句，从存储引擎读取记录，返回给客户端；

怎么样？现在再看这张图，是不是很清晰了。

![查询语句执行流程](https://cdn.xiaolincoding.com/gh/xiaolincoder/mysql/sql执行过程/mysql查询流程.png)

#### 总结

1. 连接器，建立连接

   1. 与客户端进行 TCP 三次握手建立连接；
   2. 校验客户端的用户名和密码，如果用户名或密码不对，则会报错；
   3. 如果用户名和密码都对了，会读取该用户的权限，然后后面的权限逻辑判断都基于此时读取到的权限；

2. 查询缓存

   1. MySQL 会先去查询缓存里的数据
   2. 但是其实**查询缓存挺鸡肋**的，对于更新比较频繁的表，查询缓存的命中率很低的，因为只要一个表有更新操作，那么这个表的查询缓存就会被清空。

3. 解析SQL，词法分析、语法分析

4. 执行SQL

   1. 预处理器

      1. 检查 SQL 查询语句中的表或者字段是否存在；
      2. 将 `select *` 中的 `*` 符号，扩展为表上的所有列；

   2. 优化器，对SQL进行优化

      1. **优化器主要负责将 SQL 查询语句的执行方案确定下来**，比如在表里面有多个索引的时候，优化器会基于查询成本的考虑，来决定选择使用哪个索引。

   3. 执行器，经历完优化器后，就确定了执行方案，接下来 MySQL 就真正开始执行语句了

      1. 主键索引查询

         1. 执行器第一次查询，把条件 `id = 1` 交给存储引擎，**让存储引擎定位符合条件的第一条记录**。
         2. 如果记录是不存在的，就会向执行器上报记录找不到的错误。如果记录是存在的，就会将记录返回给执行器。
         3. 执行器从存储引擎读到记录后，接着判断记录是否符合查询条件，如果符合则发送给客户端，如果不符合则跳过该记录。

      2. 全表扫描

         1. 执行器第一次查询，**让存储引擎读取表中的第一条记录**
         2. 执行器会判断读到的这条记录的 name 是不是 iphone，如果不是则跳过；如果是则将记录发给客户的
         3. 执行器查询的过程是一个 while 循环接着向存储引擎层要求继续读刚才那条记录的下一条记录，存储引擎把下一条记录取出后就将其返回给执行器（Server层），执行器继续判断条件，不符合查询条件即跳过该记录，否则发送到客户端。==(区别)==

      3. 索引下推，索引下推能够减少**二级索引**在查询时的回表操作，提高查询的效率，因为它将 Server 层部分负责的事情，交给存储引擎层去处理了。

         1. ```sql
            select * from t_user  where age > 20 and reward = 100000;
            ```

         2. 联合索引当遇到范围查询 (>、<) 就会停止匹配，也就是 **age 字段能用到联合索引，但是 reward 字段则无法利用到索引**。

         3. 不使用索引下推的情况

            1. Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
            2. 存储引擎根据二级索引的 B+ 树快速定位到这条记录后，获取主键值，然后**进行回表操作**，将完整的记录返回给 Server 层；
            3. Server 层在判断该记录的 reward 是否等于 100000，如果成立则将其发送给客户端；否则跳过该记录；
            4. 接着，继续向存储引擎索要下一条记录，存储引擎在二级索引定位到记录后，获取主键值，然后回表操作，将完整的记录返回给 Server 层；
            5. 如此往复，直到存储引擎把表中的所有记录读完

         4. 使用索引下推

            1. Server 层首先调用存储引擎的接口定位到满足查询条件的第一条二级索引记录，也就是定位到 age > 20 的第一条记录；
            2. 存储引擎定位到二级索引后，**先不执行回表**操作，而是先判断一下该索引中包含的列（reward列）的条件（reward 是否等于 100000）是否成立。如果**条件不成立**，则直接**跳过该二级索引**。如果**成立**，则**执行回表**操作，将完成记录返回给 Server 层。==(**区别**)==
            3. Server 层在判断其他的查询条件（本次查询没有其他条件）是否成立，如果成立则将其发送给客户端；否则跳过该记录，然后向存储引擎索要下一条记录。
            4. 如此往复，直到存储引擎把表中的所有记录读完。

### 表中十个字段，你主键用自增ID还是UUID，为什么？ 

**一般来说自增id好**

自增的主键的值是顺序的

**自增 id 优点**

- 下一条记录就会写入新的页中，一旦数据按照这种顺序的方式加载，主键页就会近乎于顺序的记录填满，提升了页面的最大填充率，不会有页的浪费
- 新插入的行一定会在原有的最大数据行下一行，mysql定位和寻址很快，不会为计算新行的位置而做出额外的消耗
- 减少了页分裂和碎片的产生

**UUID缺点**

- 页分裂问题

  - 对于id不规律的主键,MySQL只要满了16KB为了保证id的顺序性会频繁的进行页分裂

  关于页分裂问题可以看这篇 -> [数据页分裂问题](###数据页分裂问题)

- 寻址定位问题

  - **如果主键是自增的**，那它产生的id每次都比前一次要大，所以每次都会将数据加在B+树**尾部**，B+树的叶子节点本质上是**双向链表**，查找它的首部和尾部，**时间复杂度O(1)**。而如果此时最末尾的数据页满了，那创建个新的页就好。
  - **如果主键不是自增的**，比方说上次分配了id=7，这次分配了id=3，为了让新加入数据后**B+树的叶子节点还能保持有序**，它就需要往叶子结点的中间找，查找过程的**时间复杂度是O(lgn)**\

- ==占用内存导致树高度变大==

  - 而UUID 太占用内存。每个 UUID 由 36 个字符组成，字符串越长，占用的内存越大，由于页的大小是固定的16KB，这样一个页上能存放的关键字数量就会越少，这样最终就会导致索引树的高度越大，在索引搜索的时候，发生的磁盘 IO 次数越多，性能越差

### 什么字段适合当做主键？ 

- 字段具有唯一性，且不能为空的特性
- 字段最好的是有递增的趋势的，如果字段的值是随机无序的，可能会引发页分裂的问题，造型性能影响。
- 不建议用业务数据作为主键，比如会员卡号、订单号、学生号之类的，因为我们无法预测未来会不会因为业务需要，而出现业务字段重复或者重用的情况。
  - **如果用户id是自增的，那别人只要每个月都注册一个新用户，然后抓包得到这个用户的user_id，然后跟上个月的值减一下，就知道这个月新进多少用户了。**

- 通常情况下会用自增字段来做主键，对于单机系统来说是没问题的。但是，如果有多台服务器，各自都可以录入数据，那就不一定适用了。因为如果每台机器各自产生的数据需要合并，就可能会出现主键重复的问题，这时候就需要考虑分布式 id 的方案了。

### count(*) 和 count(1) 有什么区别？哪个性能最好？

1. 在mySAM引擎下由于记录了数据的行数,count(*)就相当于拿到那个记录数直接返回都不用去查询,效率非常快,当然前提是没有where条件
2. 而不管是innodb还是myisam count(*)和count(1)没有任何区别
3. 但是count(字段)会查询出统计非null的数据
4. 都走索引

### 数据库三大范式是什么？ 

**第一范式（1NF）：要求数据库表的每一列都是==不可分割的原子数据项==。**

举例说明：

一个列有张三 19880180520这个数据,应该分成俩列比如,姓名 手机号

可见，调整后的每一列都是不可再分的，因此满足第一范式（1NF）；

**==第二范式（2NF）==：在1NF的基础上，第二范式需要确保数据库表中的每一列都和主键相关，而不能只与主键的某一部分相关（主要针对联合主键而言）。**

类似于划分多对多的关系

举例说明：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831264527-80e8f129-577b-427c-aa72-f946ef0ad791.png)

在上图所示的情况中，同一个订单中可能包含不同的产品，因此主键必须是“订单号”和“产品号”联合组成，

但可以发现，产品数量、产品折扣、产品价格与“订单号”和“产品号”都相关，但是订单金额和订单时间仅与“订单号”相关，与“产品号”无关，

这样就不满足第二范式的要求，调整如下，需分成两个表：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831264581-946f0d2d-67b7-4b0b-a423-9aedfba7c79f.png)

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831265636-c5e40c1b-9c8d-4bb4-a01b-b4f2f1fead32.png)

**==第三范式（3NF）==：在2NF基础上，第三范式需要确保数据表中的每一列数据都和主键直接相关，而不能间接相关。**

必须要和主键直接依赖

举例说明：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831266056-95039496-fc66-4b10-b49f-95070ae03ec4.png)

上表中，所有属性都完全依赖于学号，所以满足第二范式，但是“班主任性别”和“班主任年龄”直接依赖的是“班主任姓名”，

而不是主键“学号”，所以需做如下调整：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831266031-136b7d2e-d5f5-4d6f-9d92-f781e36eb888.png)

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831266035-5c10bc4f-bda8-43f5-8500-8548a4f69826.png)

这样以来，就满足了第三范式的要求

因为在遵守范式的数据库设计中，表中不能有任何冗余字段，这就使得查询的时候就会经常有多表关联查询，这无疑是比较耗时的。

于是就有了反范式化。所谓反范式化，是一种针对遵从设计范式的数据库的性能优化策略。

也就是说，反范式化不等于非范式化，反范式化一定发生在满足范式设计的基础之上。前者相当于先遵守所有规则，再进行局部调整。

比如我们可以在表中增加一些冗余字段，方便我们进行数据查询，而不再需要经常做多表join，但同时，这也会带来一个问题，那就是这些冗余字段之间的一致性如何保证，这个问题本来在遵守范式的设计中是不会有的，一旦做了反范式，那就需要开发者自行解决了。

反范式其实本质上是软件开发中一个比较典型的方案，那就是"用空间换时间"，通过做一些数据冗余，来提升查询速度。

在互联网业务中，比较典型的就是数据量大，并发高，并且通常查询的频率要远高于写入的频率，所以适当的做一些反范式，通过做一些字段的冗余，可以提升查询性能，降低响应时长，从而提升并发度。

### CHAR 和 VARCHAR有什么区别？

**基本区别**

- **char是一种定长的数据类型，它的长度固定且在存储时会自动在结尾添加空格来将字符串填满指定的长度**。char的长度范围是0-255，**char会导致碎片空间，但是好处就是不容易出现页分裂**
- **varchar是一种可变长度的数据类型，它只会存储实际的字符串内容，不会填充空格**。因此，在存储短字符串时，varchar可以节省空间。varchar的长度范围是0-65535（MySQL 5.0.3之后的版本），**vachar不会导致碎片空间，但是可能会出现页分裂。**

`char`/`varchar`**和页分裂的关联**

因为char一开始就会分配固定的内存空间，所以后续的数据插入一定是不会超过这个内存地址空间范围的。

varchar由于可变的数据类型，在底层数据之间的地址是紧密相连的，所以当后面插入的数据超出原本数据范围，那么就没法进行空间覆盖，而空间就需要扩充，那么后续的数据就需要调整地址空间，这样就会导致页分裂，并且页分裂都知道是个连锁反应，这个过程可能会比较耗时。

### Text数据类型可以无限大吗？ 

MySQL 3 种text类型的最大长度如下：

- TEXT：65,535 bytes ~64kb
- MEDIUMTEXT：16,777,215 bytes ~16Mb
- LONGTEXT：4,294,967,295 bytes ~4Gb

### mysql中的一些基本函数，你知道哪些？

一、字符串函数

**CONCAT(str1, str2, ...)**：连接多个字符串，返回一个合并后的字符串。

SELECT CONCAT('Hello', ' ', 'World') AS Greeting;

**LENGTH(str)**：返回字符串的长度（字符数）。

SELECT LENGTH('Hello') AS StringLength;

**SUBSTRING(str, pos, len)**：从指定位置开始，截取指定长度的子字符串。

SELECT SUBSTRING('Hello World', 1, 5) AS SubStr;

**REPLACE(str, from_str, to_str)**：将字符串中的某部分替换为另一个字符串。

SELECT REPLACE('Hello World', 'World', 'MySQL') AS ReplacedStr;

二、数值函数

**ABS(num)**：返回数字的绝对值。

SELECT ABS(-10) AS AbsoluteValue;

**POWER(num, exponent)**：返回指定数字的指定幂次方。

SELECT POWER(2, 3) AS PowerValue;

三、日期和时间函数

**NOW()**：返回当前日期和时间。

SELECT NOW() AS CurrentDateTime;

**CURDATE()**：返回当前日期。

SELECT CURDATE() AS CurrentDate;

四、聚合函数

**COUNT(column)**：计算指定列中的非NULL值的个数。

SELECT COUNT(*) AS RowCount FROM my_table;

**SUM(column)**：计算指定列的总和。

SELECT SUM(price) AS TotalPrice FROM orders;

**AVG(column)**：计算指定列的平均值。

SELECT AVG(price) AS AveragePrice FROM orders;

**MAX(column)**：返回指定列的最大值。

SELECT MAX(price) AS MaxPrice FROM orders;

**MIN(column)**：返回指定列的最小值。

SELECT MIN(price) AS MinPrice FROM orders;

### 数据页分裂问题 

**页分裂作用**

页分裂的目的就是保证后一个数据页中的所有行主键值比前一个数据页中主键值大。

**页分裂例子**

假设你现在已经有两个数据页了。并且你正在往第二个数据页中写数据。

假设你自定义了主键索引，而且你自定义的这个主键索引并不一定是自增的。

那就有可能出现下面这种情况 如下图：

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211114852.png)

假设上图中的id就是你自定义的不会自增的主键

然后随着你将数据写入。就导致后一个数据页中的所有行并不一定比前一个数据页中的行的id大。

这时就会触发页分裂的逻辑。

页分裂的目的就是保证：后一个数据页中的所有行主键值比前一个数据页中主键值大。

经过分裂调整，可以得到下面的这张图。

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211115038.png)

### **SQL查询语句的执行顺序是怎么样的？** 

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1724831265920-93e8a404-7829-4a7e-8b76-bb9f46c2cdea.png)

所有的查询语句都是从FROM开始执行，在执行过程中，每个步骤都会生成一个虚拟表，这个虚拟表将作为下一个执行步骤的输入，最后一个步骤产生的虚拟表即为输出结果。

1. from
2. where
3. group
4. having
5. select
6. order by
7. limit

## 存储引擎

### 讲一讲mysql的引擎吧，你有什么了解？

- InnoDB：InnoDB是从**MySQL5.5之后的默认存储引擎，具有ACID事务支持、行级锁、外键约束**等特性。它适用于高并发的读写操作，支持较好的数据完整性和并发控制。
- MyISAM：MyISAM是MySQL的另一种常见的存储引擎，具有较低的存储空间和内存消耗，适用于大量读操作的场景。然而，MyISAM不支持事务、行级锁和外键约束，因此在并发写入和数据完整性方面有一定的限制。
- Memory：Memory引擎将**数据存储在内存**中，适用于对性能要求较高的读操作，但是在服务器重启或崩溃时数据会丢失。它不支持事务、行级锁和外键约束

### 说一下MySQL的InnoDB与MyISAM的区别？

- **事务**：**InnoDB 支持事务，MyISAM 不支持事务**，这是 MySQL 将默认存储引擎从 MyISAM 变成 InnoDB 的重要原因之一。
  - 关于事务的内容可以看[事物的详细讲述](#事务)

- **索引结构**：InnoDB 是聚簇索引，MyISAM 是非聚簇索引。
  - 关于索引方面可以看[索引的详细讲述](#索引)

- **锁粒度**：**InnoDB 最小的锁粒度是行锁，MyISAM 最小的锁粒度是表锁**。一个更新语句会锁住整张表，导致其他查询和更新都会被阻塞，因此并发访问受限。
  - 关于锁的详细讲述可以看[锁的详细讲述](#锁篇)

- **count 的效率**：InnoDB 不保存表的具体行数，执行 select count(*) from table 时需要全表扫描。而MyISAM 用一个变量保存了整个表的行数，执行上述语句时只需要读出该变量即可，速度很快。
  - 详细可以看[count执行效率](##count(*) 和 count(1) 有什么区别？哪个性能最好？)

- **崩溃恢复**：InnoDB引引擎通过 redolog 日志实现了崩溃恢复，而Myisam是不支持崩溃恢复的。
  - 关于redolog可以看[redo log详细讲述](##redo log)


### MySQL为什么InnoDB是默认引擎？

InnoDB引擎在**事务支持、并发性能、崩溃恢复**等方面具有优势，因此被MySQL选择为默认的存储引擎。

- 事务支持：InnoDB引擎提供了对事务的支持，可以进行ACID（原子性、一致性、隔离性、持久性）属性的操作。**Myisam存储引擎是不支持事务的。**
- 并发性能：InnoDB引擎**采用了行级锁定的机制，可以提供更好的并发性能**，**Myisam存储引擎只支持表锁**，锁的粒度比较大。
- 崩溃恢复：**InnoDB引引擎通过 redolog 日志实现了崩溃恢复**，可以在数据库发生异常情况（如断电）时，通过日志文件进行恢复，保证数据的持久性和一致性。Myisam是不支持崩溃恢复的。

## 索引

### 索引的分类

MySQL可以按照四个角度来分类索引。

- 按「数据结构」分类：**B+tree索引、Hash索引、Full-text索引**。
- 按「物理存储」分类：**聚簇索引（主键索引）、二级索引（辅助索引）**。
- 按「字段特性」分类：**主键索引、唯一索引、普通索引、前缀索引**。
- 按「字段个数」分类：**单列索引、联合索引**。

### 最左匹配法则

但是，如果查询条件是以下这几种，因为不符合最左匹配原则，所以就无法匹配上联合索引，联合索引就会失效:

- where b=2；
- where c=3；
- where b=2 and c=3；

上面这些查询条件之所以会失效，是因为(a, b, c) 联合索引，是先按 a 排序，在 a 相同的情况再按 b 排序，在 b 相同的情况再按 c 排序。所以，**b 和 c 是全局无序，局部相对有序的**，这样在没有遵循最左匹配原则的情况下，是无法利用到索引的。

**范围查询的字段可以用到联合索引，但是在范围查询字段的后面的字段无法用到联合索引**

### where条件的顺序影响使用索引吗？❌

假如有一个联合索引，（a,b），那么如下两个SQL的性能有差别么？

```sql
SELECT * FROM my_table WHERE a = 'value' AND b = 'value2';
SELECT * FROM my_table WHERE b = 'value2' AND a = 'value';
```

其实是没啥影响的，**也就是说WHERE 子句后面多个字段的先后顺序通常不会影响查询的结果**。

主要是因为有**查询优化器**的存在，字段的先后顺序并不重要

#### **总结**

where条件的顺序不影响使用索引，这是因为查询优化器改变了字段的先后顺序

### 什么是索引跳跃扫描/MySQL索引一定遵循最左前缀匹配吗？❌ 

MySQL 8.0.13 版本中，对于range查询（什么是range后面会提到)，引入了**索引跳跃扫描（Index Skip Scan）**优化，支持不符合组合索引最左前缀原则条件下的SQL，依然能够使用组合索引，减少不必要的扫描。

```sql
CREATE TABLE t1 (f1 INT NOT NULL, f2 INT NOT NULL);
CREATE INDEX idx_t on t1(f1,f2);
INSERT INTO t1 VALUES
  (1,1), (1,2), (1,3), (1,4), (1,5),
  (2,1), (2,2), (2,3), (2,4), (2,5);
```

通过上面的SQL，先创建一张t1表，并把f1,f2两个字段设置为联合索引。之后再向其中插入一些记录。

分别在MySQL 5.7.9和MySQL 8.0.30上执行`EXPLAIN SELECT f1, f2 FROM t1 WHERE f2 = 40;`

![image-20241207135653687](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/image-20241207135653687.png)

可以看到，主要有以下几个区别：

> MySQL 5.7中，type = index，rows=160，extra=Using where;Using index
>
> MySQL 8.0中，type = range，rows=16，extra=Using where;Using index for skip scan


这里面的type指的是扫描方式，range表示的是范围扫描，index表示的是索引树扫描，通常情况下，range要比index快得多。

从rows上也能看得出来，使用index的扫描方式共扫描了160行，而使用range的扫描方式只扫描了16行。

接着，重点来了，那就是为啥MySQL 8.0中的扫描方式可以更快呢？主要是因为`Using index for skip scan` 表示他用到了索引跳跃扫描的技术。

也就是说，虽然我们的SQL中，没有遵循最左前缀原则，只使用了f2作为查询条件，但是经过MySQL 8.0的优化以后，还是通过索引跳跃扫描的方式用到了索引了。


#### 优化原理

**那么他是怎么优化的呢？**

在MySQL 8.0.13 及以后的版本中，`SELECT f1, f2 FROM t1 WHERE f2 = 40;`SQL执行过程如下：

- 获取f1字段第一个唯一值，也就是f1=1
- 构造f1=1 and f2 = 40，进行范围查询
- 获取f1字段第二个唯一值，也就是f1=2
- 构造f1=2 and f2 = 40，进行范围查询
- 一直扫描完f1字段所有的唯一值，最后将结果合并返回

也就是说，最终执行的SQL语句是**像**下面这样的：

```
SELECT f1, f2 FROM t1 WHERE f1 =1 and f2 = 40
UNION
SELECT f1, f2 FROM t1 WHERE f1 =2 and f2 = 40;
```

即，MySQL的优化器帮我们把联合索引中的f1字段作为查询条件进行查询了。


#### 限制条件

在知道了索引跳跃扫描的执行过程之后，很多聪明的读者其实就会发现，**这种查询优化比较适合于f1的取值范围比较少，区分度不高的情况（如性别），一旦f1的区分度特别高的话（如出生年月日），这种查询可能会更慢。**

所以，**真正要不要走索引跳跃扫描，还是要经过MySQL的优化器进行成本预估之后做决定的。**

所以，这种优化一般用于那种联合索引中第一个字段区分度不高的情况。但是话又说回来了，我们一般不太会把区分度不高的字段放在联合索引的左边，不过事无绝对，既然MySQL给了一个优化的方案，就说明还是有这样的诉求的。

**但是，我们不能依赖他这个优化，建立索引的时候，还是优先把区分度高的，查询频繁的字段放到联合索引的左边。**

除此之外，在MySQL官网中，还提到索引跳跃扫描还有一些其他的限制条件：

- 表T至少有一个联合索引，但是对于联合索引(A,B,C,D)来说，A和D可以是空的，但B和C必须是非空的。
- 查询必须只能依赖一张表，不能多表join
- 查询中不能使用GROUP BY或DISTINCT语句
- 查询的字段必须是索引中的列


> 原文: <https://www.yuque.com/hollis666/fo22bm/ixpnm8nvbfa9l7gm>

#### **总结**

**作用**

索引跳跃扫描是`msyql8`之后对最左匹配的优化。

**怎么实现的**

具体来说就是对于一个复合索引（a，b）当你查询条件是where b = 2的时候，如果是`mysql5`的话不会走索引extra的执行计划type为index索引树扫描，但是`mysql8`之后type是range，range比index快很多。

底层主要是把一条sql语句拆成多个sql使得where条件遵循最左原则，比如：

1. where a = 1 and b = 2
2. where a =2 and b = 2
3. where a = ? and b  = 2

这里的语句条数取决于a有多少条数据，所以可以知道A的区分度如果高的话会导致sql语句特别多反而不如原来效率高，比较适合A为性别这种字段的情况。

**前提条件**

- 表T至少有一个联合索引，但是对于联合索引(A,B,C,D)来说，A和D可以是空的，但B和C必须是非空的。
- 查询必须只能依赖一张表，不能多表join
- 查询中不能使用GROUP BY或DISTINCT语句
- 查询的字段必须是索引中的列

### 从 innodb 的索引结构分析，为什么索引的 key 长度不能太长?❌

数据页是磁盘上的一个连续区域，通常大小为16KB当然，也可以通过配置进行调整。16KB就意味着Innodb的每次读写都是以 16KB 为单位的，一次从磁盘到内存的读取的最小是16KB，一次从内存到磁盘的持久化也是最小16KB。

1. key太长会导致深度变深
   - B+树是一种平衡多路查找树，其性能部分依赖于树的深度。**key长度增加会导致每个数据页能存储的键值对数量减少（因为页大小固定，key长度更大，能存的数量就更少），这可能导致B+树的深度增加。** 树的深度增加意味着查询、插入或删除操作需要更多的磁盘I/O操作来遍历这些额外的层级，从而降低性能。

2. 磁盘I/O次数要更多
   - **磁盘I/O操作是数据库操作中成本最高的部分之一。**因为每个B+树节点通常对应于磁盘上的一个页，其大小在InnoDB中默认为16KB。**如果索引的key长度很长，每个页面能存放的节点数就会减少，这意味着处理查询时需要读取更多的页面，从而增加了磁盘I/O操作的次数，降低了查询效率。**

3. 字符串长的话，比较速度会更慢
   - 在B+树中进行键值查找时，如果key长度过长，比较操作的成本会增加，尤其是对于字符串这类可变长度的数据类型。这会导致每一次查找操作都消耗更多的CPU资源，进一步影响到查询性能。

所以，索引的 key 长度不建议太长。但是也不要太短，太短可能会导致区分度不够高，比如身份证号，如果你只用前6位当做索引的话，因为重复度很高，那么索引效果就会很差。所以需要在区分度和长度时间做一个平衡。

### 什么是聚簇索引什么是非聚簇索引(二级索引) ?

- 聚簇索引
  - 索引和数据存放在一起,非叶子节点存放索引,叶子节点存放整行数据,只有一个,一般是主键这种

- 非聚簇索引(二级索引)
  - 数据和索引不在一起,非叶子节点存索引,叶子节点存主键id,一般可以有多个,特殊情况需要通过这个id回表去聚簇索引拿到数据


### 如果聚簇索引的数据更新，它的存储要不要变化？

- 如果更新的数据是非索引数据，也就是普通的用户记录，那么存储结构是不会发生变化
- 如果更新的数据是索引数据，那么存储结构是有变化的，因为要维护 b+树的有序性

### MySQL主键是聚簇索引吗？

InnoDB 在创建聚簇索引时，会根据不同的场景选择不同的列作为索引：

- 如果有主键，默认会使用主键作为聚簇索引的索引键；
- 如果没有主键，就选择第一个不包含 NULL 值的唯一列作为聚簇索引的索引键；
- 在上面两个都没有的情况下，InnoDB 将自动生成一个隐式自增 id 列作为聚簇索引的索引键；

一张表只能有一个聚簇索引，那为了实现非主键字段的快速搜索，就引出了二级索引（非聚簇索引/辅助索引），它也是利用了 B+ 树的数据结构，但是二级索引的叶子节点存放的是主键值，不是实际数据。

### 性别字段能加索引么？为啥？

不建议针对性别字段加索引。

实际上与索引创建规则之一区分度有关，性别字段假设有100w数据，50w男、50w女，区别度几乎等于 0

但是也有特殊情况，就比如如果100个数据，95个是男生，5个是女生，然后你查询女生这种加索引也是有意义的，因为可以过滤大部分数据

### Mysql中的索引是怎么实现的 ？ 

MySQL InnoDB 引擎是用了B+树作为了索引的数据结构。

B+Tree 是一种多叉树，叶子节点才存放数据，非叶子节点只存放索引，而且每个节点里的数据是**按主键顺序有序存放**的。因此在叶子节点中，包括了所有的索引值信息，并且每一个叶子节点都有两个指针，分别指向下一个叶子节点和上一个叶子节点，形成一个双向链表。

数据库的索引和数据都是存储在硬盘的，我们可以把读取一个节点当作一次磁盘 I/O 操作。那么上面的整个查询过程一共经历了 3 个节点(三层)，也就是进行了 3 次 I/O 操作。

B+Tree 存储千万级的数据只需要 3-4 层高度就可以满足，这意味着从千万级的表查询目标数据最多需要 3-4 次磁盘 I/O，所以**B+Tree 相比于 B 树和二叉树来说，最大的优势在于查询效率很高，因为即使在数据量很大的情况，查询一个数据的磁盘 I/O 依然维持在 3-4次。**

### B树和B+树的区别是什么呢？

B树:

- 数据和索引存放在一起,所以路数更少,性能就更低

B+树:

- 非叶子节点只存储索引,叶子节点存储数据,这样路数会更多,路数更多层数就更少,查询性能就更好
- 叶子节点数据与数据之间是通过双线链表连接起来的,所以非常适合范围查询和排序扫描。可以沿着叶子节点的链表顺序访问数据，而无需进行多次随机访问，而 B 树要实现范围查询，因此只能通过树的遍历来完成范围查询，这会涉及多个节点的磁盘 I/O 操作，范围查询效率不如 B+ 树。
- B+树在插入、删除和更新操作后会自动重新平衡，确保树的高度保持相对稳定，从而保持良好的搜索性能。

总结:b树索引和数据存放在一起,因为**InnoDB最少IO单位都是一个数据页16k**所以这样导致他每个数据页存储的索引是比较少的相比B+树,B+树由于索引和数据分开来存储,非叶子节点存储索引,叶子节点存储数据,这样每个数据页只存储索引路数一定会更多比B树,而且B+树每个数据直接都是双向链表所以他非常适合范围查询

### MySQL为什么用B+树结构？和其他结构比的优点？

- **B+Tree vs B Tree：**B+Tree 只在叶子节点存储数据，而 B 树 的非叶子节点也要存储数据，所以 B+Tree 的单个节点的数据量更小，在相同的磁盘 I/O 次数下，就能查询更多的节点。另外，B+Tree 叶子节点采用的是双链表连接，适合 MySQL 中常见的**基于范围的顺序查找**，而 B 树无法做到这一点。
- **B+Tree vs 二叉树：**对于有 N 个叶子节点的 B+Tree，其搜索复杂度为O(logdN)，其中 d 表示节点允许的最大子节点个数为 d 个。在实际的应用当中， d 值是大于100的，这样就保证了，即使数据达到千万级别时，B+Tree 的高度依然维持在 3~4 层左右，也就是说一次数据查询操作只需要做 3~4 次的磁盘 I/O 操作就能查询到目标数据。而**二叉树的每个父节点的儿子节点个数只能是 2 个，意味着其搜索复杂度为 O(logN)，这已经比 B+Tree 高出不少**，因此二叉树检索到目标数据所经历的磁盘 I/O 次数要更多。
- **B+Tree vs Hash：**Hash 在做等值查询的时候效率贼快，搜索复杂度为 O(1)。但是 **Hash 表不适合做范围查询，它更适合做等值的查询**，这也是 B+Tree 索引要比 Hash 表索引有着更广泛的适用场景的原因

### 为什么 MySQL 不用 跳表？

B+树的高度在3层时存储的数据可能已达千万级别，但对于跳表而言同样去维护千万的数据量那么所造成的跳表层数过高而导致的磁盘io次数增多，**也就是使用B+树在存储同样的数据下磁盘io次数更少(层数更少)**。

### 创建联合索引时需要注意什么？

建立联合索引时的字段顺序，对索引效率也有很大影响。越靠前的字段被用于索引过滤的概率越高，实际开发工作中**建立联合索引时，要把区分度大的字段排在前面，这样区分度大的字段越有可能被更多的 SQL 使用到**。

### MySQL 单表为什么不要超过 2000W 行? ***

假设

- 非叶子节点内指向其他页的数量为 x
- 叶子节点内能容纳的数据行数为 y
- B+ 数的层数为 z

如下图中所示，**Total =x^(z-1) \*y 也就是说总数会等于 x 的 z-1 次方 与 Y 的乘积**。

![图片](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211231797.png)

> X =？

整个页的大小是16-1=15K, 在索引页中主要记录的是主键与页号，索引一条数据大概也就是 12byte。

所以 x=

16kb = 16*1024b

在除12b得出既 x=15*1024/12≈1280 行

大概1000多条数据

> Y=？

叶子节点和非叶子节点的结构是一样的，同理，能放数据的空间也是15k。

但是叶子节点中存放的是真正的行数据，这个影响的因素就会多很多，比如，字段的类型，字段的数量。每行数据占用空间越大，页中所放的行数量就会越少。

这边我们暂时按一条行数据 1k 来算，那一页就能存下 15 条，Y = 15*1024/1000 ≈15。

算到这边了，是不是心里已经有谱了啊。

根据上述的公式，Total =x^(z-1) *y，已知 x=1280，y=15：

- 假设 B+ 树是两层，那就是 z = 2， Total = （1280 ^1 ）*15 = 19200
- 假设 B+ 树是三层，那就是 z = 3， Total = （1280 ^2） *15 = 24576000 （约 2.45kw）

这不是正好就是文章开头说的最大行数建议值 2000W 嘛！对的，一般 B+ 数的层级最多也就是 3 层。

你试想一下，如果是 4 层，除了查询的时候磁盘 IO 次数会增加，而且这个 Total 值会是多少，大概应该是 3 百多亿吧，也不太合理，所以，3 层应该是比较合理的一个值。

> 到这里难道就完了？

不。

我们刚刚在说 Y 的值时候假设的是 1K ，那比如我实际当行的数据占用空间不是 1K , 而是 5K, 那么单个数据页最多只能放下 3 条数据。

同样，还是按照 z = 3 的值来计算，那 Total = （1280 ^2） *3 = 4915200 （近 500w）

所以，在保持相同的层级（相似查询性能）的情况下，在行数据大小不同的情况下，其实这个最大建议值也是不同的，而且影响查询性能的还有很多其他因素，比如，数据库版本，服务器配置，sql 的编写等等。

MySQL 为了提高性能，会将表的索引装载到内存中，在 InnoDB buffer size 足够的情况下，其能完成全加载进内存，查询不会有问题。

但是，当单表数据库到达某个量级的上限时，导致内存无法存储其索引，使得之后的 SQL 查询会产生磁盘 IO，从而导致性能下降，所以增加硬件配置（比如把内存当磁盘使），可能会带来立竿见影的性能提升哈。

### 索引优缺点 *

#### 优点

索引是用来提高mysql的效率的一种数据结构,索引一般是减少mysql检索时间的,索引自动会对数据排序,就也减少了排序步骤降低cpu压力，并且对添加了索引的数据做查询的话由于b+树的原因会很擅长查询和范围查询

#### 缺点

**链表的维护**

我以主键索引为例举个例子，主键索引的B+树的每一个节点内的记录都是按照主键值由小到大的顺序，采用单向链表的方式进行连接的。如下图所示：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/v2-a72cd85a55feaad0afe25cc7c877e190_b.jpg)

如果我现在要删除主键`id`为1的记录，会破坏3个数据页内的记录排序，需要对这3个数据页内的记录进行重排列，插入和修改操作也是同理。

> 注：这里给大家提一嘴，其实删除操作并不会立即进行数据页内记录的重排列，而是会给被删除的记录打上一个删除的标识，等到合适的时候，再把记录从链表中移除，但是总归需要涉及到排序的维护，势必要消耗性能。

假如这张表有12个字段，我们为这张表的12个字段都设置了索引，我们删除1条记录，需要涉及到12棵B+树的N个数据页内记录的排序维护。

更糟糕的是，你增删改记录的时候，还可能会触发数据页的回收和分裂。还是以上图为例，假如我删除了`id`为13的记录，那么`数据页124`就没有存在的必要了，会被InnoDB存储引擎回收；我插入一条`id`为12的记录，如果`数据页32`的空间不足以存储该记录，InnoDB又需要进行页面分裂。我们不需要知道页面回收和页面分裂的细节，但是能够想象到这个操作会有多复杂。

如果每个字段都创建索引，所有这些索引的维护操作带来的性能损耗，你能想象了吧。

##### 总结

1. 创建索引和维护索引需要**空间成本**，索引就是一棵B+数，每创建一个索引都需要创建一棵B+树，每一棵B+树的节点都是一个数据页，每一个数据页默认会占用16KB的磁盘空间，每一棵B+树又会包含许许多多的数据页。所以，大量创建索引，你的磁盘空间会被迅速消耗。
2. 创建索引和维护索引需要**时间成本**，这个成本随着数据量的增加而加大，空间上的代价你可以使用“钞能力”来解决，但时间上的代价我们可能就束手无策了。
3. 会**降低表的增删改**的效率，mysql的索引都是b+树每次对增删改所有添加索引的字段的b+树结构都会发生改变，这其中可能会有页分裂和页面回收等耗时的操作

### 索引创建原则有哪些(复合索引)？ *

1. 数据量大,单表大概10万数据且查询比较频繁就建议加
2. ==当有些字段经常作为where group order的字段==
3. 区分度高的列,区分度越高效率越高(相同数据太多)
4. 如果是字符串类型,可以建立前缀索引
5. 控制索引数量,索引越多维护成本越高,增删改都要维护索引

### 什么情况下索引会失效 ? ***(更详细一点)

1. 最左前缀法则
2. 范围查询右边的列不能使用索引
3. 在索引列上进行运算索引会失效
4. ==字符串不加单引号==
5. 模糊查询==以%开头==

### 最左前缀法则索引为什么会失效?

这个主要取决于索引底层存储方式,举个例子比如你复合索引(col1,clo2),他底层从存储是这样的

当你where col2 = 10 and col1 = 20,这个时候他索引排序是根据第一个索引先排序第二个索引后排序,就比如col1 10先排一排 col1 20在排一排,随后才轮到col2去比较,当col都为10的时候按col2来排,所以说如果你col2方最左边就会导致索引找不到,你想想如果你要找col20的地方你不知道该怎么找因为你是优先按col1来排序,就算你知道col1=11,col2=20那在这之前还会有col1=10,col2=20所以走不了索引



### 索引合并

索引合并是将MySQL的单个索引合并起来，就比如这里有三种策略

- **Using intersect：使用交集算法，当查询条件使用AND连接时，系统可能会使用多个索引分别检索每个条件，然后找出所有索引结果的交集。**
- **Using union：使用并集算法，当查询条件使用OR连接时，每个条件可能利用不同的索引。系统会分别查找每个索引，然后合并结果。**
- **Using sort_union：使用排序联合算法，在需要对结果进行排序的查询中，如果不同的排序条件各自有索引，系统可以先分别检索每个索引，然后合并并排序这些结果。**

在MySQL中，如果进行了索引合并，执行计划中会明确显示type 为index_merge，key 中会列出用到的索引列表，并且在 extra 中会提示具体用了哪种索引合并策略：

假设有一个用户数据库表`Users`，表中有两个列：`age`和`city`，分别有各自的索引。

```sql
SELECT * FROM Users WHERE age > 30 AND city = 'Hang Zhou';
```

**使用交集（Intersection）索引合并**： 

   - 数据库系统会首先使用`city`索引找到所有在Hang Zhou的用户。
   - 然后使用`age`索引找到所有年龄大于30的用户。
   - 最后，系统将这两个索引的结果做交集运算，从而得到同时满足这两个条件的用户列表。


```sql
SELECT * FROM Users WHERE age > 30 OR city = 'Hang Zhou';
```

**使用联合（Union）索引合并**： 

   - 数据库系统会分别使用`age`索引查找所有年龄大于30的用户，和使用`city`索引查找所有住在纽约的用户。
   - 然后，系统将这两个结果集合并，以得到最终的用户列表。

#### 总结

索引合并就是多个单个索引，根据三种策略进行合并，主要是这三种：

- 交集索引合并，如果查询语句使用了and则可能会分别使用单个索引然后取俩个结果集的交集
- 并集索引合并，如果查询语句使用了or则可能会分别使用单个索引然后取俩个结果集的所有结果
- 排序联合算法（待复习）

### 索引下推

索引下推是mysql在5.6版本之后用来优化索引查询的东西,**索引下推具体作用是减少你回表的次数**,举个例子

还是拿`name`和`phone`的联合索引为例，我们要查询所有`name`为「蝉沐风」，并且手机尾号为6606的记录，查询SQL如下：

```text
SELECT * FROM user_innodb WHERE name = "蝉沐风" AND phone LIKE "%6606";
```

由于联合索引的叶子节点的记录是先按照`name`字段排序，`name`字段相同的情况下再按照`phone`字段排序，因此把`%`加在`phone`字段前面的时候，是==无法利用索引的顺序性来进行快速比较==的，也就是说这条查询语句中只有`name`字段可以使用索引进行快速比较和过滤。正常情况下查询过程是这个样子的：

1. InnoDB使用联合索引查出所有`name`为蝉沐风的二级索引数据，得到3个主键值：3485，78921，423476；
2. 拿到主键索引进行回表，到聚簇索引中拿到这三条完整的用户记录；
3. InnoDB把这3条完整的用户记录返回给MySQL的Server层，在Server层过滤出尾号为6606的用户。

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/v2-cdb6ecfeb556c316e0ca53bd908baf14_b.jpg)

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/v2-a752415a694818e74b54efa075c02b70_b.jpg)

==索引的使用是在存储引擎中进行的，而数据记录的比较是在Server层中进行的。==现在我们把上述搜索考虑地极端一点，假如数据表中10万条记录都符合`name='蝉沐风'`的条件，而只有1条符合`phone LIKE "%6606"`条件，这就意味着，InnoDB需要将99999条无效的记录传输给Server层让其自己筛选，更严重的是，这99999条数据都是通过回表搜索出来的啊！关于回表的代价你已经知道了。

现在引入**索引下推**。索引下推就是==过滤的动作由下层的存储引擎层通过使用索引来完成，而不需要上推到Server层进行处理==。是在MySQL5.6之后完善的功能。

#### 总结

**作用**：索引下推是mysql在5.6版本之后用来优化索引查询的东西,**索引下推具体作用是减少你回表的次数**

**前提**：当你的复合索引的部分索引失效，没法通过索引快速去判断数据，就会出现额外回表的情况，导致MySQL索引的使用是存储引擎进行的，而过滤操作交给了server层(执行器)，涉及回表所以性能不够好

**解决办法**:Mysql引入索引下推将过滤的操作全部放在存储引擎层，这样就能减少无意义的回表

**例子**

```sql
#其中（name，phone）是复合索引
SELECT * FROM user_innodb WHERE name = "蝉沐风" AND phone LIKE "%6606";
```

无下推的情况：

1. InnoDB使用联合索引查出所有`name`为蝉沐风的二级索引数据，得到3个主键值：3485，78921，423476；
2. 拿到主键索引进行回表，到聚簇索引中拿到这三条完整的用户记录；
3. InnoDB把这3条完整的用户记录返回给MySQL的Server层，在Server层过滤出尾号为6606的用户。

有下推的情况：

1. InnoDB使用联合索引查出所有`name`为蝉沐风的二级索引数据，得到3个主键值：3485，78921，423476；
2. 存储引擎继续3个主键值中的phone进行过滤得到：78921
3. 拿到主键索引进行回表，到聚簇索引中拿到这一条条完整的用户记录；
4. InnoDB把这1条完整的用户记录返回给MySQL的Server层

> 可以发现少了俩次回表

### 知道什么是回表查询嘛 ?

在查询时使用了二级索引，如果查询的数据能在二级索引里查询的到，那么就不需要回表，这个过程就是覆盖索引。

**如果查询的数据不在二级索引里，就会先检索二级索引，找到对应的叶子节点，获取到主键值后，然后再检索主键索引，就能查询到数据了，这个过程就是回表**。

#### 怎么减少回表？

- 覆盖索引
- 索引下推

### 知道什么叫覆盖索引嘛 /超大分页? 

**覆盖索引**

这个当然知道,其实我的理解来说,回表就不是覆盖索引,如果你select查询返回的字段只是主键和索引字段的话,就是覆盖索引,那举个例子三条语句:

- select id from xxx where id 这个是直接聚簇查询直接返回整行数据,效率高,是覆盖索引
- select id name from xxx where name='' 这个因为name是索引所以去二级索引查询然后id为主键返回,不需要去回表
- select id name sex from xxx where name='' 那这里就是sex既不是主键也不是索引查询的时候会进行回表操作,他就不是覆盖索引

**超大分页**

问题描述:

超大分页一般都是在数据量比较大时，我们使用了limit分页查询，并且需要对数据进行排序，这个时候效率就很低，我们可以采用覆盖索引和子查询来解决

解决方案:

先分页查询数据的id字段，确定了id之后，用ids再用子查询来过滤，只查询这个id列表中的数据就可以了因为查询id的时候，走的覆盖索引，所以效率可以提升很多

### in走不走索引？

IN通常是走索引的，**当IN后面的数据在数据表中超过30%的匹配时是全表扫描，不走索引**，因此IN走不走索引和后面的数据量有关系。根据实际的情况，需要控制IN查询的范围。原因有以下几点

1. IN 的条件过多，会导致索引失效，走索引扫描

1. IN 的条件过多，返回的数据会很多，可能会导致应用堆内内存溢出。

所以必须要控制好IN的查询个数

### 如果一个列即使单列索引，又是联合索引，单独查它的话先走哪个？

mysql 优化器会分析每个索引的查询成本，然后选择成本最低的方案来执行 sql。

如果单列索引是 a，联合索引是（a ，b），那么针对下面这个查询：

select a, b from table where a = ? and b =?

优化器会选择联合索引，因为查询成本更低，查询也不需要回表，直接索引覆盖了。

### 索引字段是不是建的越多越好？

不是，建的的越多会占用越多的空间，而且在写入频繁的场景下，对于B+树的维护所付出的性能消耗也会越大

### 了解过前缀索引吗？

使用前缀索引是为了减小索引字段大小，可以增加一个索引页中存储的索引值，有效提高索引的查询速度。在一些大字符串的字段作为索引时，使用前缀索引可以帮助我们减小索引项的大小。

## 锁篇

### 锁的分类 

按性质分类：

- 共享锁：也叫读锁。对同一份数据，多个事务读操作可以同时加锁而不互相影响 ，但不能修改数据
- 排他锁：也叫写锁。当前的操作没有完成前，会阻断其他操作的读取和写入

按粒度分类：

- 全局锁:锁定数据库中所有的表(数据备份)

- 表级锁

  - 表锁:锁整表,粒度大

  - 元数据锁:防止DML和DDL冲突,隐式加锁

  - 意向锁:避免表锁时一行一行的查看行锁加锁情况,解决这个问题引入的,隐式的加锁

  -  AUTO-INC 锁,自增锁

- 行级锁

  - 记录锁:对单个记录加锁。**RC和RR都支持**

  - 间隙锁:锁的是记录间的间隙。**RR下才有**

  - 临键锁:锁的是当前记录+记录前的间隙**RR下才有**

> 间隙锁+临键锁->解决幻读问题

按使用方式分类：

- 悲观锁：每次查询数据时都认为别人会修改，很悲观，所以查询时加锁
- 乐观锁：每次查询数据时都认为别人不会修改，很乐观，但是更新时会判断一下在此期间别人有没有去更新这个数据

不同存储引擎支持的锁

| 存储引擎 | 表级锁   | 行级锁   | 页级锁 |
| -------- | -------- | -------- | ------ |
| MyISAM   | 支持     | 不支持   | 不支持 |
| InnoDB   | **支持** | **支持** | 不支持 |
| MEMORY   | 支持     | 不支持   | 不支持 |
| BDB      | 支持     | 不支持   | 支持   |

#### 详细讲述

可以引导面试官问需不需要详细讲某个锁

- 表级锁

  - 表锁:给整个表加锁,加锁之后根据读锁写锁的兼容关系来进行阻塞
    - 表共享读锁
    - 表独占写锁
    - 总结:读锁阻塞其他事务写,写锁阻塞其他线程读和写

  - 元数据锁:系统自己加锁,不需要显示使用,主要目的是避免DDL和DML冲突,DDL:修改表 DML:update 你想想你在update的时候表结构被修改了,那你字段不一致怎么update	

  - 意向锁:避免表锁时一行一行的查看行锁加锁情况
    - 比如你使用update的时候会给一行数据加行锁(独占写锁),如果这个时候你需要加表锁,如果有个记录被加了写锁,就不能加表锁,所以会去一行一行查哪个加了行锁,这样效率太低,意向锁采用的是只要你使用update然后加了个行锁(独占写锁)我就给你加个意向锁(排他),类似于一个标识告知你这个表加了个行锁,后续你想加表锁只需要判断意向锁的性质**(排他、共享**)也就是读写锁的兼容问题就能决定加不加表锁**(read/write)**
    - 总结:能高效的判断出能不能加表锁

- 行级锁

  - 记录锁:给每行数据加行锁,并且能不能添加跟读写锁兼容性有关
    - 行独占写锁
    - 行共享读锁
  - 间隙锁:在行记录之间间隙加锁,比如1234我就是在1-2这个间隙加锁
  - 临键锁:行锁+间隙锁

> 在mysql innodb引擎中RR隔离级别才有间隙锁和临键锁,在RC下只有行锁,这俩种锁唯一目的就是为了解决幻读问题(很大程度)

**如何解决幻读?**

- 快照读通过`MVCC`
- 当前读通过**临键锁+间隙锁**

select ..from where id>= 4 for update这个时候

1. 4的行锁
2. 8的临键锁
3. 8->正无穷的临键锁

总的来说就是锁住[4,+∞]这个范围包括后续>=4的数据插入幻影数据

### MySQL表锁

#### Auto-inc锁

##### 引言

mysql不给主键赋值，也就是null的情况下也会自增。

那么就有几个问题，并发情况下怎么保证自增不会冲突？

##### 5.1版本区别

**在早期的MySQL版本中（5.1之前），AUTO-INC锁是一个表级锁**，它在第一个插入操作开始时被获取，并持续到整个插入结束。这意味着在同一时刻只有一个事务能对该表进行插入操作，这虽然保证了自增值的唯一性和连续性，但限制了并发性能。

**从MySQL 5.1开始，InnoDB引入了一种新的AUTO-INC锁策略，使得AUTO-INC锁在插入操作的过程中给字段一个自增值就直接释放锁，不需要等数据插入完成**，它大大提高了并发插入的性能，因为不同的事务可以更快地连续插入新记录到同一个表中。

InnoDB 存储引擎提供了个 innodb_autoinc_lock_mode 的系统变量，是用来控制选择用 AUTO-INC 锁，还是轻量级的锁。

- 当 innodb_autoinc_lock_mode = 0，就采用 AUTO-INC 锁，语句执行结束后才释放锁；
- 当 innodb_autoinc_lock_mode = 2，就采用轻量级锁，申请自增主键后就释放锁，并不需要等语句执行后才释放。
- 当 innodb_autoinc_lock_mode = 1：
  - 普通 insert 语句，自增锁在申请之后就马上释放；
  - 类似 insert … select 这样的批量插入数据的语句，自增锁还是要等语句结束后才被释放；

##### 主从复制导致的数据不一致

当 innodb_autoinc_lock_mode = 2 是性能最高的方式，但是当搭配 binlog 的日志格式是 statement 一起使用的时候，在「主从复制的场景」中会发生**数据不一致的问题**。

*不过相对来说影响也不大，虽然无法保证自增值的连续性，但至少能确保递增性，因此对索引的维护不会造成额外开销*。

举个例子，考虑下面场景：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/innodb_autoinc_lock_mode=2.png)

session A 往表 t 中插入了 4 行数据，然后创建了一个相同结构的表 t2，然后**两个 session 同时执行向表 t2 中插入数据**。

如果 innodb_autoinc_lock_mode = 2，意味着「申请自增主键后就释放锁，不必等插入语句执行完」。那么就可能出现这样的情况：

- session B 先插入了两个记录，(1,1,1)、(2,2,2)；
- 然后，session A 来申请自增 id 得到 id=3，插入了（3,5,5)；
- 之后，session B 继续执行，插入两条记录 (4,3,3)、 (5,4,4)。

可以看到，**session B 的 insert 语句，生成的 id 不连续**。

当「主库」发生了这种情况，binlog 面对 t2 表的更新只会记录这两个 session 的 insert 语句，如果 binlog_format=statement，记录的语句就是原始语句。记录的顺序要么先记 session A 的 insert 语句，要么先记 session B 的 insert 语句。

但不论是哪一种，这个 binlog 拿去「从库」执行，这时从库是按「顺序」执行语句的，只有当执行完一条 SQL 语句后，才会执行下一条 SQL。因此，在**从库上「不会」发生像主库那样两个 session 「同时」执行向表 t2 中插入数据的场景。所以，在备库上执行了 session B 的 insert 语句，生成的结果里面，id 都是连续的。这时，主从库就发生了数据不一致**。

要解决这问题，binlog 日志格式要设置为 row，这样在 binlog 里面记录的是主库分配的自增值，到备库执行的时候，主库的自增值是什么，从库的自增值就是什么。

所以，**当 innodb_autoinc_lock_mode = 2 时，并且 binlog_format = row，既能提升并发性，又不会出现数据一致性问题**。

#### 意向锁

**知识前提补充**

- 共享锁：也叫读锁。对同一份数据，多个事务读操作可以同时加锁而不互相影响 ，但不能修改数据
- 排他锁：也叫写锁。当前的操作没有完成前，会阻断其他操作的读取和写入

> 读锁只阻塞写不阻塞读,写锁读写都阻塞

对于表锁以及行锁在MySQL中都可以按性质划分为读锁和写锁比如:

- 表读锁
- 表写锁
- 行读锁
- 行写锁

**开篇**

mysql分读写锁,其中不管是加了读锁还是写锁都会对写进行阻塞,也就是加了读写锁就不允许写操作了

既然是这样MySQL有行锁,如果说行锁加了读写锁,你要加表锁必然会有读写锁冲突问题你是加不了的

> 所以MySQL怎么去避免冲突呢?

如果说采用一行一行判断数据加没加读写锁,那么效率是不是太差了

所以就引入意向锁,你可以理解为单纯是一个标识,作用就是当你加表锁的时候直接告诉你有没有读写冲突问题,你就不用一行一行去判断

> 那么MySQL是怎么实现的呢?

在你加行锁的时候他会给你意向锁标识上代表你这整张表加了锁,这样就在加表锁的时候就能通过意向锁判断是当前锁的表是否有加行锁

那么还有个问题,不是所有读写锁都有冲突的

读锁只阻塞写不阻塞读,写锁读写都阻塞,所以只是知道你有加行锁还不够,必然要想要判断你要加的表锁是否会跟你行锁冲突

这里你行锁加了什么锁比如写锁你的意向锁也会加上同样的锁作为标识

**总结**

意向锁就是用来避免表锁和行锁之间的锁冲突问题,以及更高效的解决这个问题

具体实现就是加行锁就给意向锁标识上,并且是同一种锁性质,行锁是读锁意向锁就也是读锁

### MySQL行锁

#### 记录锁

**Record Lock，翻译成记录锁，是加在索引记录上的锁。**例如，`SELECT c1 FROM t WHERE c1 = 10 For UPDATE;`会对c1=10这条记录加锁，为了防止任何其他事务查询、插入、更新或删除c1值为10的行。

**Record Lock是一个典型的行级锁**，但是需要特别注意的是，Record锁的不是这行记录，而是锁索引记录。并且**Record lock锁且只锁索引**！

如果没有索引怎么办？对于这种情况，InnoDB 会创建一个隐藏的聚簇索引，并使用这个索引进行记录锁定。

> **如果我们在一张表中没有定义主键，那么，MySQL会默认选择一个唯一的非空索引作为聚簇索引。如果没有适合的非空唯一索引，则会创建一个隐藏的主键（row_id）作为聚簇索引。**

#### 行锁什么时候加锁?

MySQL只有在当前读的情况下才会加锁(串行化隔离级别除外)

##### 当前读vs快照读

共享锁=读锁=S锁

排他锁=写锁=X锁

这些只是不同叫法

1. 快照读:和MVCC相关,无任何锁的东西
2. 当前读:当前读中执行的sql语句都会加锁
   1. 共享锁:select.....from.....lock in share mode
   2. 排他锁:
      1. select....from.....for update
      2. insert
      3. update
      4. delete


##### 读锁和写锁兼容性问题

读锁:当前线程加锁,其他线程可读不可写

写锁:当前线程加锁,其他线程不可读不可写

##### 例子

```sql
mysql > begin;
mysql > select * from t_test where id = 1 for update;
```

事务会对表中主键 id = 1 的这条记录加上 X 型的记录锁，如果这时候其他事务对这条记录进行删除或者更新操作，那么这些操作都会被阻塞。注意，**其他事务插入一条 id = 1 的新记录并不会被阻塞**，而是会报**主键冲突**的错误，这是因为主键有唯一性的约束。

#### 什么时候释放锁?

当事务执行 commit 后，事务过程中生成的锁都会被释放。

#### 行级锁有哪些种类？

- 记录锁:记录锁，也就是仅仅把一条记录锁上；
- 间隙锁:间隙锁，锁定一个范围，但是不包含记录本身；
- 临键锁:记录锁+间隙锁的组合，锁定一个范围，并且锁定记录本身。

##### 记录锁

锁住的是一条记录。而且记录锁是有 S 锁和 X 锁之分的

##### 间隙锁

只存在于可重复读隔离级别，**目的是为了解决可重复读隔离级别下幻读的现象**。

假设，表中有一个范围 id 为（3，5）间隙锁，那么其他事务就无法插入 id = 4 这条记录了，这样就有效的防止幻读现象的发生。

**间隙锁不能隔着数据锁**,底层是查找到第一个存在的数据就枷锁就比如10 15 20不能直接一把锁(10,20)只能俩把锁(10,15)和(15,20)

**在相同范围里间隙锁与间隙锁之间加锁不冲突,但是insert数据是会有冲突的**

##### 临键锁

假设，表中有一个范围 id 为（3，5] 的 next-key lock，那么其他事务即不能插入 id = 4 记录，也不能修改和删除 id = 5 这条记录。

**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。

比如，一个事务持有了范围为 (1, 10] 的 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，就会被阻塞。

**虽然相同范围的间隙锁是多个事务相互兼容的**，但对于记录锁，我们是要考虑 X 型与 S 型关系，X 型的记录锁与 X 型的记录锁是冲突的。

#### 行级锁的退化以及加锁规则

##### 加锁规则

行级锁加锁规则比较复杂，不同的场景，加锁的形式是不同的。

**加锁的对象是索引，加锁的基本单位是 next-key lock**，**next-key lock 是前开后闭区间，而间隙锁是前开后开区间**。

##### 锁的退化规则

但是，next-key lock 在一些场景下会退化成记录锁或间隙锁。

总结一句，**在能使用记录锁或者间隙锁就能避免幻读现象的场景下， next-key lock 就会退化成记录锁或间隙锁**。

###### 唯一索引等值查询

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会**退化成「记录锁」**。
- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会**退化成「间隙锁」**。

接下里用两个案例来说明。

1**、记录存在的情况**

假设事务 A 执行了这条等值查询语句，查询的记录是「存在」于表中的。

```sql
mysql> select * from user where id = 1 for update;
+----+--------+-----+
| id | name   | age |
+----+--------+-----+
|  1 | 路飞   |  19 |
+----+--------+-----+
```

那么，事务 A 会为 id 为 1 的这条记录就会加上 **X 型的记录锁**。

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725799533734-9d16869c-3427-4d54-8dda-b44faf8601e8.png)

接下来，如果有其他事务，对 id 为 1 的记录进行更新或者删除操作的话，这些操作都会被阻塞，因为更新或者删除操作也会对记录加 X 型的记录锁，而 X 锁和 X 锁之间是互斥关系。

比如，下面这个例子：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725799533718-1b43848c-a2a1-4d3f-aa80-2b8e7cd89fc7.png)

因为事务 A 对 id = 1的记录加了 **X 型的记录锁**，所以事务 B 在修改 id=1 的记录时会被阻塞，事务 C 在删除 id=1 的记录时也会被阻塞。

从这里我们也可以得知，**加锁的对象是针对索引**，因为这里查询语句扫描的 B+ 树是聚簇索引树，即主键索引树，所以是对主键索引加锁。将对应记录的主键索引加 记录锁后，就意味着其他事务无法对该记录进行更新和删除操作了。

为什么唯一索引等值查询并且查询记录存在的场景下，该记录的索引中的 next-key lock 会退化成记录锁？

原因就是在唯一索引等值查询并且查询记录存在的场景下，仅靠记录锁也能避免幻读的问题。

幻读的定义就是，当一个事务前后两次查询的结果集，不相同时，就认为发生幻读。所以，要避免幻读就是避免结果集某一条记录被其他事务删除，或者有其他事务插入了一条新记录，这样前后两次查询的结果集就不会出现不相同的情况。

- 由于主键具有唯一性，所以**其他事务插入 id = 1 的时候，会因为主键冲突，导致无法插入 id = 1 的新记录**。这样事务 A 在多次查询 id = 1 的记录的时候，不会出现前后两次查询的结果集不同，也就避免了幻读的问题。
- 由于对 id = 1 加了记录锁，**其他事务无法删除该记录**，这样事务 A 在多次查询 id = 1 的记录的时候，不会出现前后两次查询的结果集不同，也就避免了幻读的问题。

2、记录不存在的情况



假设事务 A 执行了这条等值查询语句，查询的记录是「不存在」于表中的。

```sql
mysql> select * from user where id = 2 for update;
Empty set (0.03 sec)
```

**此时事务 A 在 id = 5 记录的主键索引上加的是间隙锁，锁住的范围是 (1, 5)。**

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725799975428-af489809-df3b-4dcb-82ae-fb6fa3ed9d09.png)

接下来，如果有其他事务插入 id 值为 2、3、4 这一些记录的话，这些插入语句都会发生阻塞。

注意，如果其他事务插入的 id = 1 或者 id = 5 的记录话，并不会发生阻塞，而是报主键冲突的错误，因为表中已经存在 id = 1 和 id = 5 的记录了。

比如，下面这个例子：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725799975505-66cdffa9-99f0-4eeb-a0b9-bc5342d191d5.png)

因为事务 A 在 id = 5 记录的主键索引上加了范围为 (1, 5) 的 X 型间隙锁，所以事务 B 在插入一条 id 为 3 的记录时会被阻塞住，即无法插入 id = 3 的记录。

为什么唯一索引等值查询并且查询记录「不存在」的场景下，在索引树找到第一条大于该查询记录的记录后，要将该记录的索引中的 next-key lock 会退化成「间隙锁」？

原因就是在唯一索引等值查询并且查询记录不存在的场景下，仅靠间隙锁就能避免幻读的问题。

- 为什么 id = 5 记录上的主键索引的锁不可以是 next-key lock？如果是 next-key lock，就意味着其他事务无法删除 id = 5 这条记录，但是这次的案例是查询 id = 2 的记录，只要保证前后两次查询 id = 2 的结果集相同，就能避免幻读的问题了，所以即使 id =5 被删除，也不会有什么影响，那就没必须加 next-key lock，因此只需要在 id = 5 加间隙锁，避免其他事务插入 id = 2 的新记录就行了。
- 为什么不可以针对不存在的记录加记录锁？锁是加在索引上的，而这个场景下查询的记录是不存在的，自然就没办法锁住这条不存在的记录。

###### 唯一索引范围查询

范围查询和等值查询的加锁规则是不同的。

当唯一索引进行范围查询时，**会对每一个扫描到的索引加 next-key 锁，然后如果遇到下面这些情况，会退化成记录锁或者间隙锁**：

- 情况一：针对「大于等于」的范围查询，因为存在等值查询的条件，那么如果等值查询的记录是存在于表中，那么该记录的索引中的 next-key 锁会**退化成记录锁**。
- 情况二：针对「小于或者小于等于」的范围查询，要看条件值的记录是否存在于表中：

- - 当条件值的记录不在表中，那么不管是「小于」还是「小于等于」条件的范围查询，**扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。
  - 当条件值的记录在表中，如果是「小于」条件的范围查询，**扫描到终止范围查询的记录时，该记录的索引的 next-key 锁会退化成间隙锁**，其他扫描到的记录，都是在这些记录的索引上加 next-key 锁；如果「小于等于」条件的范围查询，扫描到终止范围查询的记录时，该记录的索引 next-key 锁不会退化成间隙锁。其他扫描到的记录，都是在这些记录的索引上加 next-key 锁。

接下来，通过几个实验，才验证我上面说的结论。

**1、针对「大于或者大于等于」的范围查询**

实验一：针对「大于」的范围查询的情况。

假设事务 A 执行了这条范围查询语句：

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where id > 15 for update;
+----+-----------+-----+
| id | name      | age |
+----+-----------+-----+
| 20 | 香克斯    |  39 |
+----+-----------+-----+
1 row in set (0.01 sec)
```

事务 A 加锁变化过程如下：

1. 最开始要找的第一行是 id = 20，由于查询该记录不是一个等值查询（不是大于等于条件查询），所以对该主键索引加的是范围为 (15, 20] 的 next-key 锁；
2. 由于是范围查找，就会继续往后找存在的记录，虽然我们看见表中最后一条记录是 id = 20 的记录，但是实际在 Innodb 存储引擎中，会用一个特殊的记录来标识最后一条记录，该特殊的记录的名字叫 supremum pseudo-record ，所以扫描第二行的时候，也就扫描到了这个特殊记录的时候，会对该主键索引加的是范围为 (20, +∞] 的 next-key 锁。
3. 停止扫描。

可以得知，事务 A 在主键索引上加了两个 X 型 的 next-key 锁：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725800482295-da5f0ab8-75a5-4f55-a732-9a6f891f1592.png)

实验二：针对「大于等于」的范围查询的情况。

假设事务 A 执行了这条范围查询语句：

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where id >= 15 for update;
+----+-----------+-----+
| id | name      | age |
+----+-----------+-----+
| 15 | 乌索普    |  20 |
| 20 | 香克斯    |  39 |
+----+-----------+-----+
2 rows in set (0.00 sec)
```

事务 A 加锁变化过程如下：

1. 最开始要找的第一行是 id = 15，由于查询该记录是一个等值查询（等于 15），所以该主键索引的 next-key 锁会**退化成记录锁**，也就是仅锁住 id = 15 这一行记录。
2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 id = 20，于是对该主键索引加的是范围为 (15, 20] 的 next-key 锁；
3. 接着扫描到第三行的时候，扫描到了特殊记录（ supremum pseudo-record），于是对该主键索引加的是范围为 (20, +∞] 的 next-key 锁。
4. 停止扫描。

可以得知，事务 A 在主键索引上加了三个 X 型 的锁，分别是：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725800481993-4441088a-294f-4a83-b5c3-21b0a69b9f8f.png)



**2、针对「小于或者小于等于」的范围查询**

实验一：针对「小于」的范围查询时，查询条件值的记录「不存在」表中的情况。

假设事务 A 执行了这条范围查询语句，注意查询条件值的记录（id 为 6）并不存在于表中。

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where id < 6 for update;
+----+--------+-----+
| id | name   | age |
+----+--------+-----+
|  1 | 路飞   |  19 |
|  5 | 索隆   |  21 |
+----+--------+-----+
3 rows in set (0.00 sec)
```

事务 A 加锁变化过程如下：

1. 最开始要找的第一行是 id = 1，于是对该主键索引加的是范围为 (-∞, 1] 的 next-key 锁；
2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 id = 5，所以对该主键索引加的是范围为 (1, 5] 的 next-key 锁；
3. 由于扫描到的第二行记录（id = 5），满足 id < 6 条件，而且也没有达到终止扫描的条件，接着会继续扫描。
4. 扫描到的第三行是 id = 10，该记录不满足 id < 6 条件的记录，所以 id = 10 这一行记录的锁会**退化成间隙锁**，于是对该主键索引加的是范围为 (5, 10) 的间隙锁。
5. 由于扫描到的第三行记录（id = 10），不满足 id < 6 条件，达到了终止扫描的条件，于是停止扫描。

从上面的分析中，可以得知事务 A 在主键索引上加了三个 X 型的锁：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725800482429-c840f539-f82e-48eb-9062-58952713a6ae.png)

- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。
- 在 id = 5 这条记录的主键索引上，加了范围为 (1, 5] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 5 的这一条记录，同时也无法插入 id 值为 2、3、4 的这一些新记录。
- 在 id = 10 这条记录的主键索引上，加了范围为 (5, 10) 的间隙锁，意味着其他事务无法插入 id 值为 6、7、8、9 的这一些新记录。

虽然这次范围查询的条件是「小于」，但是查询条件值的记录不存在于表中（ id 为 6 的记录不在表中），所以如果事务 A 的范围查询的条件改成 <= 6 的话，加的锁还是和范围查询条件为 < 6 是一样的。 大家自己也验证下这个结论。

因此，**针对「小于或者小于等于」的唯一索引范围查询，如果条件值的记录不在表中，那么不管是「小于」还是「小于等于」的范围查询，扫描到终止范围查询的记录时，该记录中索引的 next-key 锁会退化成间隙锁，其他扫描的记录，则是在这些记录的索引上加 next-key 锁**。

实验二：针对「小于等于」的范围查询时，查询条件值的记录「存在」表中的情况。

假设事务 A 执行了这条范围查询语句，注意查询条件值的记录（id 为 5）存在于表中。

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where id <= 5 for update;
+----+--------+-----+
| id | name   | age |
+----+--------+-----+
|  1 | 路飞   |  19 |
|  5 | 索隆   |  21 |
+----+--------+-----+
2 rows in set (0.00 sec)
```

事务 A 加锁变化过程如下：

1. 最开始要找的第一行是 id = 1，于是对该记录加的是范围为 (-∞, 1] 的 next-key 锁；
2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 id = 5，于是对该记录加的是范围为 (1, 5] 的 next-key 锁。
3. 由于主键索引具有唯一性，不会存在两个 id = 5 的记录，所以不会再继续扫描，于是停止扫描。

从上面的分析中，可以得到**事务 A 在主键索引上加了 2 个 X 型的锁**：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725800484150-b49197a2-1e9d-414e-90ac-b7a1316c93f4.png)

- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁。意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。
- 在 id = 5 这条记录的主键索引上，加了范围为 (1, 5] 的 next-key 锁。意味着其他事务即无法更新或者删除 id = 5 的这一条记录，同时也无法插入 id 值为 2、3、4 的这一些新记录。

我们也可以通过 `select * from performance_schema.data_locks\G;` 这条语句来看看事务 A 加了什么锁。

实验三：再来看针对「小于」的范围查询时，查询条件值的记录「存在」表中的情况。

如果事务 A 的查询语句是小于的范围查询，且查询条件值的记录（id 为 5）存在于表中。

```sql
select * from user where id < 5 for update;
```

事务 A 加锁变化过程如下：

1. 最开始要找的第一行是 id = 1，于是对该记录加的是范围为 (-∞, 1] 的 next-key 锁；
2. 由于是范围查找，就会继续往后找存在的记录，扫描到的第二行是 id = 5，该记录是第一条不满足 id < 5 条件的记录，于是**该记录的锁会退化为间隙锁，锁范围是 (1,5)**。
3. 由于找到了第一条不满足 id < 5 条件的记录，于是停止扫描。

可以得知，此时**事务 A 在主键索引上加了两种 X 型锁：**

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725800484215-8d1de735-ad7e-4b65-a63f-b13e765fc3d9.png)

- 在 id = 1 这条记录的主键索引上，加了范围为 (-∞, 1] 的 next-key 锁，意味着其他事务即无法更新或者删除 id = 1 的这一条记录，同时也无法插入 id 小于 1 的这一些新记录。
- 在 id = 5 这条记录的主键索引上，加了范围为 (1,5) 的间隙锁，意味着其他事务无法插入 id 值为 2、3、4 的这一些新记录。

###### 非唯一索引等值查询

当我们用非唯一索引进行等值查询的时候，**因为存在两个索引，一个是主键索引，一个是非唯一索引（二级索引），所以在加锁时，同时会对这两个索引都加锁，但是对主键索引加锁的时候，只有满足查询条件的记录才会对它们的主键索引加锁**。

针对非唯一索引等值查询时，查询的记录存不存在，加锁的规则也会不同：

- 当查询的记录「存在」时，由于不是唯一索引，所以肯定存在索引值相同的记录，于是**非唯一索引等值查询的过程是一个扫描的过程，直到扫描到第一个不符合条件的二级索引记录就停止扫描，然后在扫描的过程中，对扫描到的二级索引记录加的是 next-key 锁，而对于第一个不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。同时，在符合查询条件的记录的主键索引上加记录锁**。
- 当查询的记录「不存在」时，**扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁**。

接下里用两个实验来说明。

1、记录不存在的情况

实验一：针对非唯一索引等值查询时，查询的值不存在的情况。

先来说说非唯一索引等值查询时，查询的记录不存在的情况，因为这个比较简单。

假设事务 A 对非唯一索引（age）进行了等值查询，且表中不存在 age = 25 的记录。

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where age = 25 for update;
Empty set (0.00 sec)
```

事务 A 加锁变化过程如下：

- 定位到第一条不符合查询条件的二级索引记录，即扫描到 age = 39，于是**该二级索引的 next-key 锁会退化成间隙锁，范围是 (22, 39)**。
- 停止查询

事务 A 在 age = 39 记录的二级索引上，加了 X 型的间隙锁，范围是 (22, 39)。意味着其他事务无法插入 age 值为 23、24、25、26、....、38 这些新记录。不过对于插入 age = 22 和 age = 39 记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说。

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725803528866-1173c227-9ce9-4fa8-9b1a-28be248c7bc3.png)



当有一个事务持有二级索引的间隙锁 (22, 39) 时，什么情况下，可以让其他事务的插入 age = 22 或者 age = 39 记录的语句成功？又是什么情况下，插入 age = 22 或者 age = 39 记录时的语句会被阻塞？

我们先要清楚，什么情况下插入语句会发生阻塞。

**插入语句在插入一条记录之前，需要先定位到该记录在 B+树 的位置，如果插入的位置的下一条记录的索引上有间隙锁，才会发生阻塞**。

在分析二级索引的间隙锁是否可以成功插入记录时，我们要先要知道二级索引树是如何存放记录的？

二级索引树是按照二级索引值（age列）按顺序存放的，在相同的二级索引值情况下， 再按主键 id 的顺序存放。知道了这个前提，我们才能知道执行插入语句的时候，插入的位置的下一条记录是谁。

基于前面的实验，事务 A 是在 age = 39 记录的二级索引上，加了 X 型的间隙锁，范围是 (22, 39)。

插入 age = 22 记录的成功和失败的情况分别如下：

- 当其他事务插入一条 age = 22，id = 3 的记录的时候，在二级索引树上定位到插入的位置，而**该位置的下一条是 id = 10、age = 22 的记录，该记录的二级索引上没有间隙锁，所以这条插入语句可以执行成功**。
- 当其他事务插入一条 age = 22，id = 12 的记录的时候，在二级索引树上定位到插入的位置，而**该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功**。

插入 age = 39 记录的成功和失败的情况分别如下：

- 当其他事务插入一条 age = 39，id = 3 的记录的时候，在二级索引树上定位到插入的位置，而**该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功**。
- 当其他事务插入一条 age = 39，id = 21 的记录的时候，在二级索引树上定位到插入的位置，而**该位置的下一条记录不存在，也就没有间隙锁了，所以这条插入语句可以插入成功**。

所以，**当有一个事务持有二级索引的间隙锁 (22, 39) 时，插入 age = 22 或者 age = 39 记录的语句是否可以执行成功，关键还要考虑插入记录的主键值，因为「二级索引值（age列）+主键值（id列）」才可以确定插入的位置，确定了插入位置后，就要看插入的位置的下一条记录是否有间隙锁，如果有间隙锁，就会发生阻塞，如果没有间隙锁，则可以插入成功**。

**2、记录存在的情况**

实验二：针对非唯一索引等值查询时，查询的值存在的情况。

假设事务 A 对非唯一索引（age）进行了等值查询，且表中存在 age = 22 的记录。

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where age = 22 for update;
+----+--------+-----+
| id | name   | age |
+----+--------+-----+
| 10 | 山治   |  22 |
+----+--------+-----+
1 row in set (0.00 sec)
```

事务 A 加锁变化过程如下：

- 由于不是唯一索引，所以肯定存在值相同的记录，于是非唯一索引等值查询的过程是一个扫描的过程，最开始要找的第一行是 age = 22，于是对该二级索引记录加上范围为 (21, 22] 的 next-key 锁。同时，因为 age = 22 符合查询条件，于是对 age = 22 的记录的主键索引加上记录锁，即对 id = 10 这一行加记录锁。
- 接着继续扫描，扫描到的第二行是 age = 39，该记录是第一个不符合条件的二级索引记录，所以该二级索引的 next-key 锁会**退化成间隙锁**，范围是 (22, 39)。
- 停止查询。

可以看到，事务 A 对主键索引和二级索引都加了 X 型的锁：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725803528618-f05933bd-6fd5-403e-a3a7-b1be0b6747df.png)

- 主键索引：

- - 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。

- 二级索引（非唯一索引）：

- - 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，不过对于插入 age = 21 和 age = 22 新记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说。
  - 在 age = 39 这条记录的二级索引上，加了范围 (22, 39) 的间隙锁。意味着其他事务无法插入 age 值为 23、24、..... 、38 的这一些新记录。不过对于插入 age = 22 和 age = 39 记录的语句，在一些情况是可以成功插入的，而一些情况则无法成功插入，具体哪些情况，会在后面说。

比如

- 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，针对是否可以插入 age = 21 和 age = 22 的新记录，分析如下：

- - 是否可以插入 age = 21 的新记录，还要看插入的新记录的 id 值，**如果插入 age = 21 新记录的 id 值小于 5，那么就可以插入成功**，因为此时插入的位置的下一条记录是 id = 5，age = 21 的记录，该记录的二级索引上没有间隙锁。**如果插入 age = 21 新记录的 id 值大于 5，那么就无法插入成功**，因为此时插入的位置的下一条记录是 id = 10，age = 22 的记录，该记录的二级索引上有间隙锁。
  - 是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，其他事务插入 age 值为 22 的新记录时，**如果插入的新记录的 id 值小于 10，那么插入语句会发生阻塞；如果插入的新记录的 id 大于 10，还要看该新记录插入的位置的下一条记录是否有间隙锁，如果没有间隙锁则可以插入成功，如果有间隙锁，则无法插入成功**。

- 在 age = 39 这条记录的二级索引上，加了范围 (22, 39) 的间隙锁。意味着其他事务无法插入 age 值为 23、24、..... 、38 的这一些新记录，针对是否可以插入 age = 22 和 age = 39 的新记录，分析如下：

- - 是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，**如果插入 age = 22 新记录的 id 值小于 10，那么插入语句会被阻塞，无法插入**，因为此时插入的位置的下一条记录是 id = 10，age = 22 的记录，该记录的二级索引上有间隙锁（ age = 22 这条记录的二级索引上有 next-key 锁）。**如果插入 age = 22 新记录的 id 值大于 10，也无法插入**，因为此时插入的位置的下一条记录是 id = 20，age = 39 的记录，该记录的二级索引上有间隙锁。
  - 是否可以插入 age = 39 的新记录，还要看插入的新记录的 id 值，从 `LOCK_DATA : 39, 20` 可以得知，其他事务插入 age 值为 39 的新记录时，**如果插入的新记录的 id 值小于 20，那么插入语句会发生阻塞，如果插入的新记录的 id 大于 20，则可以插入成功**。

同时，事务 A 还对主键索引（INDEX_NAME: PRIMARY ）加了 X 型的记录锁：

- 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。

为什么这个实验案例中，需要在二级索引索引上加范围 (22, 39) 的间隙锁？

要找到这个问题的答案，我们要明白 MySQL 在可重复读的隔离级别场景下，为什么要引入间隙锁？其实**是为了避免幻读现象的发生**。

如果这个实验案例中：

```sql
select * from user where age = 22 for update;
```

如果事务 A 不在二级索引索引上加范围 (22, 39) 的间隙锁，只在二级索引索引上加范围为 (21, 22] 的 next-key 锁的话，那么就会有幻读的问题。

前面我也说过，在非唯一索引上加了范围为 (21, 22] 的 next-key 锁，是无法完全锁住 age = 22 新记录的插入，因为对于是否可以插入 age = 22 的新记录，还要看插入的新记录的 id 值，从 `LOCK_DATA : 22, 10` 可以得知，其他事务插入 age 值为 22 的新记录时，如果插入的新记录的 id 值小于 10，那么插入语句会发生阻塞，**如果插入的新记录的 id 值大于 10，则可以插入成功**。

也就是说，只在二级索引索引（非唯一索引）上加范围为 (21, 22] 的 next-key 锁，其他事务是有可能插入 age 值为 22 的新记录的（比如插入一个 age = 22，id = 12 的新记录），那么如果事务 A 再一次查询 age = 22 的记录的时候，前后两次查询 age = 22 的结果集就不一样了，这时就发生了幻读的现象。

**那么当在 age = 39 这条记录的二级索引索引上加了范围为 (22, 39) 的间隙锁后，其他事务是无法插入一个 age = 22，id = 12 的新记录，因为当其他事务插入一条 age = 22，id = 12 的新记录的时候，在二级索引树上定位到插入的位置，而该位置的下一条是 id = 20、age = 39 的记录，正好该记录的二级索引上有间隙锁，所以这条插入语句会被阻塞，无法插入成功，这样就避免幻读现象的发生**。

所以，为了避免幻读现象的发生，就需要在二级索引索引上加范围 (22, 39) 的间隙锁。

###### 非唯一索引范围查询

非唯一索引和主键索引的范围查询的加锁也有所不同，不同之处在于**非唯一索引范围查询，索引的 next-key lock 不会有退化为间隙锁和记录锁的情况**，也就是非唯一索引进行范围查询时，对二级索引记录加锁都是加 next-key 锁。

就带大家简单分析一下，事务 A 的这条范围查询语句：

```sql
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from user where age >= 22  for update;
+----+-----------+-----+
| id | name      | age |
+----+-----------+-----+
| 10 | 山治      |  22 |
| 20 | 香克斯    |  39 |
+----+-----------+-----+
2 rows in set (0.01 sec)
```

事务 A 的加锁变化：

- 最开始要找的第一行是 age = 22，虽然范围查询语句包含等值查询，但是这里不是唯一索引范围查询，所以是不会发生退化锁的现象，因此对该二级索引记录加 next-key 锁，范围是 (21, 22]。同时，对 age = 22 这条记录的主键索引加记录锁，即对 id = 10 这一行记录的主键索引加记录锁。
- 由于是范围查询，接着继续扫描已经存在的二级索引记录。扫面的第二行是 age = 39 的二级索引记录，于是对该二级索引记录加 next-key 锁，范围是 (22, 39]，同时，对 age = 39 这条记录的主键索引加记录锁，即对 id = 20 这一行记录的主键索引加记录锁。
- 虽然我们看见表中最后一条二级索引记录是 age = 39 的记录，但是实际在 Innodb 存储引擎中，会用一个特殊的记录来标识最后一条记录，该特殊的记录的名字叫 supremum pseudo-record ，所以扫描第二行的时候，也就扫描到了这个特殊记录的时候，会对该二级索引记录加的是范围为 (39, +∞] 的 next-key 锁。
- 停止查询

可以看到，事务 A 对主键索引和二级索引都加了 X 型的锁：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725803529016-956e62e0-2d97-45a1-82d3-a4fb4a504298.png)

- 主键索引（id 列）：

- - 在 id = 10 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 10 的这一行记录。
  - 在 id = 20 这条记录的主键索引上，加了记录锁，意味着其他事务无法更新或者删除 id = 20 的这一行记录。

- 二级索引（age 列）：

- - 在 age = 22 这条记录的二级索引上，加了范围为 (21, 22] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 22 的这一些新记录，不过对于是否可以插入 age = 21 和 age = 22 的新记录，还需要看新记录的 id 值，有些情况是可以成功插入的，而一些情况则无法插入，具体哪些情况，我们前面也讲了。
  - 在 age = 39 这条记录的二级索引上，加了范围为 (22, 39] 的 next-key 锁，意味着其他事务无法更新或者删除 age = 39 的这一些记录，也无法插入 age 值为 23、24、25、...、38 的这一些新记录。不过对于是否可以插入 age = 22 和 age = 39 的新记录，还需要看新记录的 id 值，有些情况是可以成功插入的，而一些情况则无法插入，具体哪些情况，我们前面也讲了。
  - 在特殊的记录（supremum pseudo-record）的二级索引上，加了范围为 (39, +∞] 的 next-key 锁，意味着其他事务无法插入 age 值大于 39 的这些新记录。

在 age >= 22 的范围查询中，明明查询 age = 22 的记录存在并且属于等值查询，为什么不会像唯一索引那样，将 age = 22 记录的二级索引上的 next-key 锁退化为记录锁？

因为 age 字段是非唯一索引，不具有唯一性，所以如果只加记录锁（记录锁无法防止插入，只能防止删除或者修改），就会导致其他事务插入一条 age = 22 的记录，这样前后两次查询的结果集就不相同了，出现了幻读现象。

###### 没有加索引的查询

前面的案例，我们的查询语句都有使用索引查询，也就是查询记录的时候，是通过索引扫描的方式查询的，然后对扫描出来的记录进行加锁。

**如果锁定读查询语句，没有使用索引列作为查询条件，或者查询语句没有走索引查询，导致扫描是全表扫描。那么，每一条记录的索引上都会加 next-key 锁，这样就相当于锁住的全表，这时如果其他事务对该表进行增、删、改操作的时候，都会被阻塞**。

不只是锁定读查询语句不加索引才会导致这种情况，update 和 delete 语句如果查询条件不加索引，那么由于扫描的方式是全表扫描，于是就会对每一条记录的索引上都会加 next-key 锁，这样就相当于锁住的全表。

因此，**在线上在执行 update、delete、select ... for update 等具有加锁性质的语句，一定要检查语句是否走了索引，如果是全表扫描的话，会对每一个索引加 next-key 锁，相当于把整个表锁住了**，这是挺严重的问题。

###### 总结

我这里总结下， MySQL 行级锁的加锁规则,能用记录锁或者间隙锁替代临建锁的就会出现锁的退化,当然前提是RR隔离级别才有间隙锁+临建锁。

**唯一索引**

唯一索引等值查询：

- 当查询的记录是「存在」的，在索引树上定位到这一条记录后，将该记录的索引中的 next-key lock 会**退化成「记录锁」**。
- 当查询的记录是「不存在」的，在索引树找到第一条大于该查询记录的记录后，将该记录的索引中的 next-key lock 会**退化成「间隙锁」**。

唯一索引范围查询:

- 当查询的记录是「存在」的,**首先对自己+记录锁,然后所有能找到的数据都加上临建锁,并且∞范围也加上临建锁**
- 当查询的记录是「不存在」的**,对所有能找到的数据加上临建锁,并且∞范围也加上临建锁**
- 当查询的记录条件是＜或<=的时候**,因为临建锁是左开右闭的,这里<就是间隙锁,<=就是临建锁**(间隙锁+记录锁自己)

**非唯一索引**

非唯一索引等值查询：

- 当查询的记录「存在」时,**首先对自己加记录锁,然后由于不是唯一索引所以只有记录锁是没法锁住同一个age=20的情况的,所以左右俩侧都需要加上间隙锁最后就会变为左边临建锁+右边间隙锁,并且会对主键加上记录锁**
  - 这里为什么数据存在并且等值的情况下需要锁住间隙范围呢?,这是因为如果只有记录锁锁住age20他只能锁住一条行数据,而age20由于不是唯一索引就必然存在多条age20的数据,所以会找到左右俩测存在的数据然后加上间隙锁就演变成间隙锁+记录锁+间隙锁=>临建锁+间隙锁

- 当查询的记录「不存在」时，**扫描到第一条不符合条件的二级索引记录，该二级索引的 next-key 锁会退化成间隙锁。因为不存在满足查询条件的记录，所以不会对主键索引加锁。**

==非唯一索引范围查询：==

- 临建锁不会退化,其实想想也很简单,比如说>临建锁肯定要加,>=左边临建锁右边也是临建锁,<左边

特殊情况:

- 这里非唯一所以加了间隙锁比如(22,39)注意,这里是对1俩条行记录之间的间隙枷锁,如果insert一条记录39的数据不一定能插入成功,原因其实是跟加锁的粒度有关系,这里锁住的是age=39并且id=20的一条数据,而不是age=39这一个范围,所以如果插入的是age=39 id=19的数据会失败,因为存在间隙范围内,但是age=39id=21就可以成功

**无索引**

无索引的不管是增删改查都是走的全表扫描,全表扫描会对扫的的所有数据＋锁,**基本等于锁全表**,性能差

### MYSQL死锁

#### 死锁的发生

我建了一张订单表，其中 id 字段为主键索引，order_no 字段普通索引，也就是非唯一索引：

然后，先 `t_order` 表里现在已经有了 6 条记录：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725862736783-7f4eb6bb-ba13-45a4-b4b0-ec462315858e.png)

假设这时有两事务，一个事务要插入订单 1007 ，另外一个事务要插入订单 1008，因为需要对订单做幂等性校验，所以两个事务先要查询该订单是否存在，不存在才插入记录，过程如下：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725862736659-0ad1a19d-6ae6-4e0c-b3e3-0b59f2155ac8.png)

可以看到，两个事务都陷入了等待状态（前提没有打开死锁检测），也就是发生了死锁，因为都在相互等待对方释放锁。

这里在查询记录是否存在的时候，使用了 `select ... for update` 语句，目的为了防止事务执行的过程中，有其他事务插入了记录，而出现幻读的问题。

如果没有使用 `select ... for update` 语句，而使用了单纯的 select 语句，如果是两个订单号一样的请求同时进来，就会出现两个重复的订单，有可能出现幻读，如下图：

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725862736736-192e18ca-2165-4bcc-b43d-e95b809cf39a.png)

#### 为什么会产生死锁？

next-key 锁的加锁规则其实挺复杂的，在一些场景下会退化成记录锁或间隙锁

回到前面死锁的例子。

![img](https://cdn.nlark.com/yuque/0/2024/png/40835658/1725863096141-ca7e0620-48ec-4d2f-a9a8-5bdb4e123969.png)

事务 A 在执行下面这条语句的时候：

```sql
select id from t_order where order_no = 1007 for update;
```

**此时事务 A 在二级索引（INDEX_NAME : index_order）上加的是 X 型的 next-key 锁，锁范围是**`**(1006, +∞]**`。

事务 B 在执行下面这条语句的时候：

```sql
select id from t_order where order_no = 1008 for update;
```

**此时事务 B 在二级索引（INDEX_NAME : index_order）上加的是 X 型的 next-key 锁，锁范围是**`**(1006, +∞]**`。

当事务A和事务B执行以下插入语句时:

```sql
Insert into t_order (order_no, create_date) values (1007, now());
Insert into t_order (order_no, create_date) values (1008, now());
```

此时会获取插入意向锁，**而插入意向锁与间隙锁是冲突的，所以当其它事务持有该间隙的间隙锁时，需要等待其它事务释放间隙锁之后，才能获取到插入意向锁。**

案例中的事务 A 和事务 B 在执行完后 `select ... for update` 语句后都持有范围为`(1006,+∞]`的next-key 锁，而接下来的事务AB的插入操作为了获取到插入意向锁，**都在等待对方事务的间隙锁释放，于是就造成了循环等待，导致死锁。**

为什么间隙锁与间隙锁之间是兼容的？

**间隙锁的意义只在于阻止区间被插入**，因此是可以共存的。**一个事务获取的间隙锁不会阻止另一个事务获取同一个间隙范围的间隙锁**，共享和排他的间隙锁是没有区别的，他们相互不冲突，且功能相同，即两个事务可以同时持有包含共同间隙的间隙锁。

那临建锁与临建锁之间是否兼容？

但是有一点要注意，**next-key lock 是包含间隙锁+记录锁的，如果一个事务获取了 X 型的 next-key lock，那么另外一个事务在获取相同范围的 X 型的 next-key lock 时，是会被阻塞的**。



插入意向锁是什么？

注意！插入意向锁名字虽然有意向锁，**但是它并不是意向锁，它是一种特殊的间隙锁。**

**插入意向锁是一种特殊的间隙锁，但不同于间隙锁的是，该锁只用于并发插入操作**。

如果说间隙锁锁住的是一个区间，那么「插入意向锁」**锁住的就是一个点**。因而从这个角度来说，插入意向锁确实是一种特殊的间隙锁。

插入意向锁与间隙锁的另一个非常重要的差别是：尽管「插入意向锁」也属于间隙锁，但两个事务却不能在同一时间内，一个拥有间隙锁，另一个拥有该间隙区间内的插入意向锁（当然，插入意向锁如果不在间隙锁区间内则是可以的）。

另外，我补充一点，插入意向锁的生成时机：

- 每插入一条新记录，都需要看一下待插入记录的下一条记录上是否已经被加了间隙锁，如果已加间隙锁，此时会生成一个插入意向锁，然后锁的状态设置为等待状态,现象就是 Insert 语句会被阻塞。

#### 如何避免死锁？

在数据库层面，有两种策略通过「打破循环等待条件」来解除死锁状态：

- **设置事务等待锁的超时时间**。当一个事务的等待时间超过该值后，就对这个事务进行回滚，于是锁就释放了，另一个事务就可以继续执行了。在 InnoDB 中，参数 `innodb_lock_wait_timeout` 是用来设置超时时间的，默认值时 50 秒。
- **开启主动死锁检测**。主动死锁检测在发现死锁后，主动回滚死锁链条中的某一个事务，让其他事务得以继续执行。将参数 `innodb_deadlock_detect` 设置为 on，表示开启这个逻辑，默认就开启。

### 锁题目测试

#### MySQL怎么实现悲观锁和乐观锁

在MySQL中，悲观锁是需要依靠数据库提供的锁机制实现的，在InnoDB引擎中，要使用悲观锁，需要先关闭MySQL数据库的自动提交属性，然后通过`select ... for update`来进行加锁。

在数据库中，悲观锁的流程如下：

- 在对记录进行修改前，先尝试为该记录加上排他锁（exclusive locking）。
- 如果加锁失败，说明该记录正在被修改，那么当前查询可能要等待或者抛出异常。具体响应方式由开发者根据实际需要决定。
- 如果成功加锁，那么就可以对记录做修改，事务完成后就会解锁了。
- 其间如果有其他对该记录做修改或加排他锁的操作，都会等待我们解锁或直接抛出异常。

我们举一个简单的例子，如淘宝下单过程中扣减库存的需求说明一下如何使用悲观锁：

```mysql
//0.开始事务
begin; 
//1.查询出商品信息
select quantity from items where id=1 for update;
//2.修改商品quantity为2
update items set quantity=2 where id = 1;
//3.提交事务
commit;
```

以上，在对id = 1的记录修改前，先通过for update的方式进行加锁，然后再进行修改。这就是比较典型的悲观锁策略。

如果以上修改库存的代码发生并发，同一时间只有一个线程可以开启事务并获得id=1的锁，其它的事务必须等本次事务提交之后才能执行。这样我们可以保证当前的数据不会被其它事务修改。

> 上面我们提到，使用select…for update会把数据给锁住，不过我们需要注意一些锁的级别，MySQL InnoDB默认行级锁。行级锁都是基于索引的，如果一条SQL语句用不到索引的话，优化器在选择时候，如果发现锁表可能性能更好的话，有可能会直接锁表。


MySQL中的乐观锁主要通过CAS的机制来实现，一般通过version版本号来实现。

CAS是项乐观锁技术，当多个线程尝试使用CAS同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。

比如前面的扣减库存问题，通过乐观锁可以实现如下：

```mysql
//查询出商品信息，quantity = 3
select quantity from items where id=1
//根据商品信息生成订单
//修改商品quantity为2
update items set quantity=2 where id=1 and quantity = 3;
```

以上，我们在更新之前，先查询一下库存表中当前库存数（quantity），然后在做update的时候，以库存数作为一个修改条件。当我们提交更新的时候，判断数据库表对应记录的当前库存数与第一次取出来的库存数进行比对，如果数据库表当前库存数与第一次取出来的库存数相等，则予以更新，否则认为是过期数据。



##### 如何选择

在乐观锁与悲观锁的选择上面，主要看下两者的区别以及适用场景就可以了。

1、乐观锁并未真正加锁，效率高。**适用于读操作频繁，写操作相对较少的场景。**一旦锁的粒度掌握不好，更新失败的概率就会比较高，容易发生业务失败。 
2、悲观锁依赖数据库锁，效率低。更新失败的概率比较低。**适用于写操作较为频繁，且并发写入的概率较高的场景。**





##### 总结

悲观锁主要是依靠MySQL的排他锁实现的,一般来说是行锁

MySQL可能会优化成表锁,要尽量避免这种情况,所以加锁的字段需要有索引,让MySQL走索引就不用锁整张表了

**MySQL悲观锁逻辑**

先select for update对要修改的数据尝试加锁,如果加锁成功则正常修改数据

如果加锁失败,说明别的事务已经拿到了锁,这个时候根据自己需求可以抛异常或者等待

```sql
//0.开始事务
begin; 
//1.查询出商品信息(加锁,for update会加锁,并且是一个独占写锁)
select quantity from items where id=1 for update;
//2.修改商品quantity为2
update items set quantity=2 where id = 1;
//3.提交事务
commit;
```

**乐观锁加锁逻辑**

乐观锁逻辑都一样,主要是通过一个version版本号字段来维护锁版本

```mysql
//查询出商品信息，quantity = 3
select quantity from items where id=1
//根据商品信息生成订单
//修改商品quantity为2
update items set quantity=2 where id=1 and quantity = 3;
```

1. 先select行数据版本
2. 然后通过这个版本使用sql的and版本=刚刚查到的版本来实现乐观锁

不过,乐观锁的底层也是行锁实现的,因为是update加的行锁,具体是间隙还是临键主要看锁优化

那么这里就有个歧义,悲观乐观都是通过mysql的行锁来实现的,有啥区别吗?

乐观锁最大的好处就是通过CAS的方式做并发校验，这个过程不需要提前加锁，只需要在更新的那一刻加一个短暂的锁而已，而悲观锁的话，需要你先select for update，锁的时长要长得多。

##### 如何选择

在乐观锁与悲观锁的选择上面，主要看下两者的区别以及适用场景就可以了。

1、乐观锁并未真正加锁，效率高。**适用于读操作频繁，写操作相对较少的场景。**一旦锁的粒度掌握不好，更新失败的概率就会比较高，容易发生业务失败。 
2、悲观锁依赖数据库锁，效率低。更新失败的概率比较低。**适用于写操作较为频繁，且并发写入的概率较高的场景。**

#### 字节面试：加了什么锁，导致死锁的？

![img](https://cdn.nlark.com/yuque/0/2024/jpeg/40835658/1725869123092-1d9a4c21-47dd-41d4-8529-55d7cb671eaf.jpeg)

事务A事务B都加了临建锁并且范围都一样,然后互相获取插入意向锁,而插入意向锁跟间隙锁有冲突,所以获取失败阻塞,俩个事务都阻塞并且都在等待对方释放释放锁然后插入

#### 遇到唯一键冲突

如果在插入新记录时，插入了一个与「已有的记录的主键或者唯一二级索引列值相同」的记录（不过可以有多条记录的唯一二级索引列的值同时为NULL，这里不考虑这种情况），此时插入就会失败，然后对于这条记录加上了 **S 型的锁**。

- 如果主键索引重复，插入新记录的事务会给已存在的主键值重复的聚簇索引记录**添加 S 型记录锁**。
- 如果唯一二级索引重复，插入新记录的事务都会给已存在的二级索引列值重复的二级索引记录**添加 S 型 next-key 锁**。

#### MySQL两个线程的update语句同时处理一条数据，会不会有阻塞？

如果是两个事务同时更新了 id = 1，比如 update ... where id = 1，那么是会阻塞的。因为 InnoDB 存储引擎实现了行级锁。

当A事务对 id =1 这行记录进行更新时，会对主键 id 为 1 的记录加X类型的记录锁，这样第二事务对 id = 1 进行更新时，发现已经有记录锁了，就会陷入阻塞状态。

#### 两条update语句处理一张表的不同的主键范围的记录，一个<10，一个>15，会不会遇到阻塞？底层是为什么的？

不会，因为锁住的范围不一样，不会形成冲突。

- 第一条 update sql 的话（ id<10），锁住的范围是（-♾️，10）
- 第二条 update sql 的话（id >15），锁住的范围是（15，+♾️）

#### 如果2个范围不是主键或索引？还会阻塞吗？ 

如果2个范围查询的字段不是索引的话，那就代表 update 没有用到索引，这时候**触发了全表扫描，全部索引都会加行级锁**，这时候第二条 update 执行的时候，就会阻塞了。

因为如果 update 没有用到索引，在扫描过程中会对索引加锁，所以全表扫描的场景下，所有记录都会被加锁

## 事务

### 事务的特性是什么？如何实现的？

- **原子性（Atomicity）**：一个事务中的所有操作，要么全部完成，要么全部不完成，不会结束在中间某个环节，而且事务在执行过程中发生错误，会被回滚到事务开始前的状态，就像这个事务从来没有执行过一样，就好比买一件商品，购买成功时，则给商家付了钱，商品到手；购买失败时，则商品在商家手中，消费者的钱也没花出去。
- **一致性（Consistency）**：是指事务操作前和操作后，数据满足完整性约束，数据库保持一致性状态。比如，用户 A 和用户 B 在银行分别有 800 元和 600 元，总共 1400 元，用户 A 给用户 B 转账 200 元，分为两个步骤，从 A 的账户扣除 200 元和对 B 的账户增加 200 元。一致性就是要求上述步骤操作后，最后的结果是用户 A 还有 600 元，用户 B 有 800 元，总共 1400 元，而不会出现用户 A 扣除了 200 元，但用户 B 未增加的情况（该情况，用户 A 和 B 均为 600 元，总共 1200 元）。
- **隔离性（Isolation）**：数据库允许多个并发事务同时对其数据进行读写和修改的能力，隔离性可以防止多个事务并发执行时由于交叉执行而导致数据的不一致，因为多个事务同时使用相同的数据时，不会相互干扰，每个事务都有一个完整的数据空间，对其他并发事务是隔离的。也就是说，消费者购买商品这个事务，是不影响其他消费者购买的。
- **持久性（Durability）**：事务处理结束后，对数据的修改就是永久的，即便系统故障也不会丢失。

MySQL InnoDB 引擎通过什么技术来保证事务的这四个特性的呢？

- 持久性是通过 redo log （重做日志）来保证的；
- 原子性是通过 undo log（回滚日志） 来保证的；
- 隔离性是通过 MVCC（多版本并发控制） 或间隙锁+临键锁机制来保证的；
- 一致性则是通过持久性+原子性+隔离性来保证；

### 并发事务带来哪些问题？ 

- 脏读:那脏读就是事务A查数据然后事务A在修改数据还没提交,这个时候事物B来查,查到了未提交的数据,那就是脏读
- 不可重复读:在一个事务内多次读取同一个数据，如果出现前后两次读到的数据不一样的情况，就意味着发生了「不可重复读」现象。那这个的话事物A查数据,然后事务B修改数据提交,然后事务A继续查发现这次数据跟上次数据不一致了
- ==幻读==:在一个事务内多次查询某个符合查询条件的「记录数量」，如果出现前后两次查询到的记录数量不一样的情况，就意味着发生了「幻读」现象。

### mysql的是怎么解决并发问题的？

- 锁机制：Mysql提供了多种锁机制来保证数据的一致性，包括行级锁、表级锁、页级锁等。通过锁机制，可以在读写操作时对数据进行加锁，确保同时只有一个操作能够访问或修改数据。
- 事务隔离级别：Mysql提供了多种事务隔离级别，包括读未提交、读已提交、可重复读和串行化。通过设置合适的事务隔离级别，可以在多个事务并发执行时，控制事务之间的隔离程度，以避免数据不一致的问题。
- MVCC（多版本并发控制）：Mysql使用MVCC来管理并发访问，它通过在数据库中保存不同版本的数据来实现不同事务之间的隔离。在读取数据时，Mysql会根据事务的隔离级别来选择合适的数据版本，从而保证数据的一致性。

### 事务的隔离级别有哪些？

- **读未提交（read uncommitted）**，什么都解决不了,指一个事务还没提交时，它做的变更就能被其他事务看到；
- **读提交（read committed）**，解决脏读,**在「每个语句执行前」都会重新生成一个 Read View**
- **可重复读（repeatable read）**，解决不可重复读,**MySQL InnoDB 引擎的默认隔离级别,「启动事务时」生成一个 Read View，然后整个事务期间都在用这个 Read View**；

其实在可重复读的情况下就解决了不可重复读和很大程度上的幻读,这里可以引入MVCC的RC和RR区别

- **串行化（serializable）**,所有都能解决,通过加读写锁的方式来避免并行访问；

### 可重复读隔离级别下，A事务提交的数据，在B事务能看见吗？

可重复读隔离级是由 MVCC（多版本并发控制）实现的，实现的方式是开始事务后（执行 begin 语句后），在执行第一个查询语句后，会创建一个 Read View，**后续的查询语句利用这个 Read View，通过这个 Read View 就可以在 undo log 版本链找到事务开始时的数据，所以事务过程中每次查询的数据都是一样的**，即使中途有其他事务插入了新纪录，是查询不出来这条数据的。

### 可重复读的幻读特例  *

#### 第一个发生幻读现象的场景

还是以这张表作为例子：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410131718330.png)

事务 A 执行查询 id = 5 的记录，此时表中是没有该记录的，所以查询不出来。

```sql
# 事务 A
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> select * from t_stu where id = 5;
Empty set (0.01 sec)
```

然后事务 B 插入一条 id = 5 的记录，并且提交了事务。

```sql
# 事务 B
mysql> begin;
Query OK, 0 rows affected (0.00 sec)

mysql> insert into t_stu values(5, '小美', 18);
Query OK, 1 row affected (0.00 sec)

mysql> commit;
Query OK, 0 rows affected (0.00 sec)
```

此时，**事务 A 更新 id = 5 这条记录，对没错，事务 A 看不到 id = 5 这条记录，但是他去更新了这条记录，这场景确实很违和，然后再次查询 id = 5 的记录，事务 A 就能看到事务 B 插入的纪录了，幻读就是发生在这种违和的场景**。

```sql
# 事务 A
mysql> update t_stu set name = '小林coding' where id = 5;
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0

mysql> select * from t_stu where id = 5;
+----+--------------+------+
| id | name         | age  |
+----+--------------+------+
|  5 | 小林coding   |   18 |
+----+--------------+------+
1 row in set (0.00 sec)
```

整个发生幻读的时序图如下：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410131718511.png)

在可重复读隔离级别下，事务 A 第一次执行普通的 select 语句时生成了一个 ReadView，之后事务 B 向表中新插入了一条 id = 5 的记录并提交。接着，事务 A 对 id = 5 这条记录进行了更新操作，在这个时刻，这条新记录的 trx_id 隐藏列的值就变成了事务 A 的事务 id，之后事务 A 再使用普通 select 语句去查询这条记录时就可以看到这条记录了，于是就发生了幻读。

因为这种特殊现象的存在，所以我们认为 **MySQL Innodb 中的 MVCC 并不能完全避免幻读现象**。

#### 第二个发生幻读现象的场景  *

除了上面这一种场景会发生幻读现象之外，还有下面这个场景也会发生幻读现象。

- T1 时刻：事务 A 先执行「快照读语句」：select * from t_test where id > 100 得到了 3 条记录。
- T2 时刻：事务 B 往插入一个 id= 200 的记录并提交；
- T3 时刻：事务 A 再执行「当前读语句」 select * from t_test where id > 100 for update 就会得到 4 条记录，此时也发生了幻读现象。

**要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句**，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

#### 总结

MySQL InnoDB 引擎的可重复读隔离级别（默认隔离级），根据不同的查询方式，分别提出了避免幻读的方案：

- 针对**快照读**（普通 select 语句），是通过 MVCC 方式解决了幻读。
- 针对**当前读**（select ... for update 等语句），是通过 next-key lock（记录锁+间隙锁）方式解决了幻读。

我举例了两个发生幻读场景的例子。

第一个例子：对于快照读， MVCC 并不能完全避免幻读现象。因为当事务 A 更新了一条事务 B 插入的记录，那么事务 A 前后两次查询的记录条目就不一样了，所以就发生幻读。

第二个例子：对于当前读，如果事务开启后，并没有执行当前读，而是先快照读，然后这期间如果其他事务插入了一条记录，那么事务后续使用当前读进行查询的时候，就会发现两次查询的记录条目就不一样了，所以就发生幻读。

所以，**MySQL 可重复读隔离级别并没有彻底解决幻读，只是很大程度上避免了幻读现象的发生。**

要避免这类特殊场景下发生幻读的现象的话，就是尽量在开启事务之后，马上执行 select ... for update 这类当前读的语句，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录。

### Mysql 设置了可重读隔离级后，怎么保证不发生幻读？

**尽量在开启事务之后，马上执行 select ... for update 这类锁定读的语句**，因为它会对记录加 next-key lock，从而避免其他事务插入一条新记录，就避免了幻读的问题。

### 串行化隔离级别是通过什么实现的？

是通过**行级锁**来实现的，序列化隔离级别下，普通的 select 查询是会对记录加 S 型的 next-key 锁，其他事务就没没办法对这些已经加锁的记录进行增删改操作了，从而避免了脏读、不可重复读和幻读现象

### MVCC

#### 什么是当前什么是快照读?

- **当前读(Current Read)**:每次读取的都是**当前最新的数据**，但是读的时候不允许写，写的时候也不允许读。当前读一般用于修改数据的操作，如update、insert、delete等。**当前读会对读取的数据加上排他锁**，防止其他事务并发修改这条记录。
- **快照读(Snapshot Isolation Read)**每次读取的是**快照数据**，也就是说**当某个数据正在被修改的时候，也可以进行读取该数据，保证读写不冲突。**快照读一般用于查询数据的操作，如select等。**快照读不需要加锁，而是通过MVCC（多版本并发控制）和undolog（回滚日志）来实现**。MVCC会为每个事务创建一个read view（可见性视图），用来判断哪些数据版本对该事务可见。undolog会记录每次修改数据前的旧版本数据，用来支持回滚操作。快照读就是根据read view去找到满足其可见性的记录，在undolog中找到对应的旧版本数据。

在MySQL 中，**只有READ COMMITTED 和 REPEATABLE READ这两种事务隔离级别才会使用快照读。**

- **在 RR 中，快照会在事务中第一次SELECT语句执行时生成**，只有在本事务中对数据进行更改才会更新快照。
- **在 RC 中，每次读取都会重新生成一个快照**，总是读取行的最新版本。

#### MVCC是什么?

mysql的InoDB存储引擎下RC,RR且快照读下会基于MVCC做数据的多版本并发控制,多版本指的是在mysql里一行数据有多个版本,而**并发控制**则是当多个事务同事执行执行某一行数据时mysql控制在多个版本中返回其中一个版本

1. 当前读(间隙锁),复杂select insert update delete
2. 快照读(MVCC) 简单select

#### MVCC原理

1. 隐藏字段:其中trx_id和roll pointer是隐藏字段,分别代表业务唯一id和存储历史版本地址
   1. 注意，以上字段，**只有在聚簇索引的行记录中才会有**，而在普通二级索引中是没有这些值的，至于二级索引的MVCC的支持，有单独的文章介绍

2. undo log版本链:由当前最新记录+该记录的历史undo log日志形成一条版本链,通过rollpointer连接
3. read view:在多个版本中返回一个版本
   1. 主要是**通过版本链数据访问规则/可见性算法来选择返回的版本**
      1. trx_id<min:当前访问的版本是创建read view之前的版本则`可见`
      2. trx_id>max:当前访问的版本是创建read view之后的版本则`不可见`
      3. min<trx_id<max:
         1. 属于m_ids,当前版本的活跃记录还未提交`不可见`
         2. 不属于m_ids中,当前版本的活跃记录已被提交`可见`
   2. 总结一下就是，**一个事务，能看到的是在他开始之前就已经提交的事务的结果，而未提交的结果都是不可见的。**

**说完可以扩展RR和RC隔离级别中read view的区别**

#### MVCC在RC,RR下的不同表现?

**结论**

RC:**每次快照读**都会生成一个新的read view

RR:只有在第一次快照读的时候会生成read view后续的快照读不会生成而是复用这次的read view

**解释**

在同事务内,RC俩次快照读的数据会不一致,因为他每次快照读生成新的read view

在同事务内,RR俩次快照读的数据一致,因为他每次复用第一次的read view

所以这也就解释了为什么RC有不可重复读的问题而RR就解决了这个问题,他就可以重复读

#### MVCC的作用

- 解决读写阻塞
  - 当某行数据加了个行锁比如说独占写锁,然后如果是当前读的话由于加了锁不允许读写,你的事务就会阻塞
  - 但是如果是MVCC他会从多个版本返回一个版本,虽然可能不是最新数据,但是避免了阻塞


- 在RR下快照读可以有效避免幻读问题,但不能完全避免
  - 幻读无非就是事务A在事务B不之情的情况下插入了幻影数据导致俩次查询数据多了
  - 那你想想刚刚讲了RR只有第一次是会生成read view而后续是重复使用这个read view所以说查出来的一直是同一个数据就解决了幻读


#### MVCC完全解决了幻读吗?

情况一:

### 一条update是不是原子性的？为什么？

是原子性，主要通过锁+undolog 日志保证原子性的

- 执行 update 的时候，会加行级别锁，保证了一个事务更新一条记录的时候，不会被其他事务干扰。
- 事务执行过程中，会生成 undolog，如果事务执行失败，就可以通过 undolog 日志进行回滚。

## 性能调优

### OnlineDDL

#### copy算法原理

在MySQL 5.6支持Online DDL之前，有copy算法

1. 新建一张临时表
2. 对原表加共享MDL锁，禁止原表的写，只允许查询操作
3. 逐行拷贝原表数据到临时表，且不进行排序
4. 拷贝完成后升级原表锁为排他MDL锁，禁止原表读写
5. 将原表删除，对临时表重命名为原表，创建索引，完成DDL操作

但是由于copy过程阻塞DML太久所以性能不够好

#### Online DDL特点

1. **DDL操作可与应用的DML操作并发执行**，当然最好是no-rebudiler模式，改进在繁忙生产环境的响应和可用性，因为对于这种业务系统几分钟或几小时不可用是难以忍受的
2. 可以通过调整DDL的lock模式来平衡与DML操作的性能问题
3. Online DDL使用的是In-place方式，相较于table-copy方式能更少的使用I/O资源，在DDL期间也能有较高的吞吐量
4. In-place相较于table-copy还有一个优点是：table-copy读取数据多，频繁的使用buffer pool导致有效缓存数据被调出，影响缓存击中率 降低效率

MySQL中的INPLACE其实还可以分为以下两种模式：

- inplace-no-rebuild ：对二级索引的增删改查、修改变长字段长度（如：varchar）、重命名列名都**不需要重建原表**
- inplace-rebuild：修改主键索引、增加删除列、修改字符集、创建全文索引等都需要**重建原表**，性能跟copy差不多。

#### Online DDL执行阶段

大致可分为三个阶段：初始化、执行和提交

##### Initialization阶段

1. 此阶段会使用MDL读锁，禁止其他并发线程修改表结构
2. 服务器将考虑存储引擎能力、语句中指定的操作以及用户指定的ALGORITHM 和 LOCK选项，确定操作期间允许的并发数

##### Execution阶段

1. 此阶段分为两个步骤 Prepared and Executed
2. 此阶段是否需要MDL写锁取决于Initialization阶段评估的因素，如果需要MDL写锁的话，仅在Prepared过程会短暂的使用MDL写锁
3. 其中最耗时的是Excuted过程

##### Commit Table Definition阶段

1. 此阶段会将MDL读锁升级到MDL写锁，此阶段一般较快，因此独占锁的时间也较短
2. 用新的表定义替换旧的表定义(如果rebuild table)

除了Excuted过程之外的几步如果没有锁冲突，执行时间非常短。Excuted过程占用了DDL绝大部分时间，这期间这个表可以正常读写数据，是因此称为“online ”

#### INPLACE or COPY究竟什么区别

1. ALGORITHM=COPY是MySQL5.5以及之前的方式
2. ALGORITHM=INPLACE是MySQL5.6引入的方式
3. COPY算法，由server层创建一个临时表用于copy数据，然后用新表替换旧表
4. INPLACE算法，“原位替换” 其实主要是指在InnoDB内部完成的DDL操作，在InnoDB内部创建临时文件。整个 DDL 过程都在 InnoDB 内部完成。对于 server 层来说，没有把数据挪动到临时表，是一个“原地”操作，这就是“inplace”名称的来源。
5. 因此对于INPLACE其实分为非重建表和重建表两类方式，非重建表方式直接在原表基础上更新，效率最高；重建表同样需要copy数据(比如新增字段) [详情请参考mysql5.7](https://link.zhihu.com/?target=https%3A//dev.mysql.com/doc/refman/5.7/en/innodb-online-ddl-operations.html%23online-ddl-table-operations)

#### Inplace 与 Online 的关系

1. DDL 过程如果是 Online 的，就一定是 inplace 的；
2. 反过来未必，也就是说 inplace 的 DDL，有可能不是 Online 的。截止到 MySQL 8.0，添加全文索引（FULLTEXT index）和空间索引 (SPATIAL index) 就属于这种情况。

#### Online DDL性能

1. DDL操作的性能很大程度上取决于该操作是否INPLACE执行以及它是否重新构建表。
2. 要评估DDL操作的相对性能，可以使用ALGORITHM=INPLACE和ALGORITHM=COPY比较结果
3. 对于修改表数据的DDL操作，可以通过查看命令结束后显示的“受影响的行”值来确定DDL操作是IN-PLACE，还是执行Table-COPY

#### 总结

onlie是5.6后推出的算法，其实就是inplace

在那之前是copy算法，这个算法核心是复制一个新的表出来然后新表替换旧表，这个过程比较耗时并且会阻塞DML

而inplace算法分俩种模式：

- no-rebuilder，这个模式就是DDL和DML并发的核心
- rebuilder：这个模式会新建表，其实也是copy数据，性能跟copy算法差不多

其中copy算法缺点就是copy过程耗时所以会长时间阻塞DML

而inplace中的no-reblider大部分时间都会非常快，而唯一执行耗时的excuter阶段还无锁，所以不用担心会阻塞DML

### 什么是数据库的主从延迟，如何解决？

数据库的主从延迟是指在主从数据库复制过程中，从服务器（Slave）上的数据与主服务器（Master）上的数据之间存在的时间差或延迟。

一般来说导致主从延迟可能由多种因素引起，以下是一些常见的原因：

1. 网络延迟：主节点和从节点之间的网络延迟导致复制延迟这是比较常见的一种情况，
2. 从节点性能问题：从服务器的性能不足也可能导致复制延迟。如果从服务器的硬件资源（CPU、内存、磁盘）不足以处理接收到的复制事件，延迟可能会增加。
3. 复制线程不够：当从节点只有一个线程，或者线程数不够的时候，数据回放就会慢，就会导致主从节点的数据延迟。


解决主从延迟主要有几个事情可以做：

1. 优化网络：确保主节点和从节点之间的网络连接稳定，尽量同城或者同单元部署，减小网络延迟。
2. 提高从服务器性能：增加从服务器的硬件资源，如CPU、内存和磁盘，以提高其性能，从而更快地处理复制事件。
3. 并行复制：借助MySQL提供的并行复制的能力，提升复制的效率，降低延迟。

#### 并行复制原理

在MySQL的主从复制中，我们介绍过MySQL的主从复制的原理，在复制过程中，主库的binlog会不断地同步到从库，然后从库有一个SQL线程不断地拉取并重放这些SQL语句，那么，一旦日志内容太多的话，一个线程执行就会有延迟，就会导致主从延迟。

为了降低因为这个原因导致的主从延迟，所以MySQL提供了并行复制的方案。在MySQL的多个版本中，先后推出过很多个并行复制的方案：

- MySQL 5.6推出基于**库级别**的并行复制。
- MySQL 5.7推出基于**组提交**的的并行复制。
- MySQL 8.0 推出基于**WRITESET**的并行复制。


##### 库级别并行复制

在MySQL 5.6中，并行是基于Schema的，也就是基于库的，在MySQL 5.6中，可以配置多个库并行进行复制，这意味着每个库都可以有自己的复制线程，可以并行处理来自不同库的写入。这提高了并行复制的性能和效率。

但是，其实大多数业务都是单库的，所以这种方案，在推出之后很多开发者和DBA并不买账，因为实在是太鸡肋了。


##### 组提交的的并行复制

因为5.6的并行复制被很多人诟病，于是在MySQL 5.7中推出了基于组提交的的并行复制，这才是真正意义上的并行复制。这就是注明的 MTS (Enhanced Multi-Threaded Slave) ：[https://dev.mysql.com/blog-archive/multi-threaded-replication-performance-in-mysql-5-7/](https://dev.mysql.com/blog-archive/multi-threaded-replication-performance-in-mysql-5-7/)

[✅介绍下MySQL 5.6中的组提交](https://www.yuque.com/hollis666/fo22bm/bb860tpuha0cuza2?view=doc_embed)

先了解下组提交，然后接着往下看。

在组提交的介绍中我们说过，一个组中多个事务，都处于Prepare阶段之后，才会被优化成组提交。那么就意味着**如果多个事务他们能在同一个组内提交，这个就说明了这个几个事务在锁上是一定是没有冲突的。**

```java
binlog_transaction_dependency_tracking  = WRITESET                 #    COMMIT_ORDER          
transaction_write_set_extraction        = XXHASH64

```

**换句话说，就是这几个事务修改的一定不是同一行记录，所以他们之间才能互不影响，同时进入Prepare阶段，并且进行组提交。**

那么，没有冲突的多条SQL，是不是就可以在主备同步过程中，在备库上并行执行回放呢？

答案是可以的，因为一个组中的多条SQL之间互相不影响，谁先执行，谁后执行，结果都是一样的！

所以，这样Slave就可以用多个SQL线程来并行的执行一个组提交中的多条SQL，从而提升效率，降低主从延迟。


##### 基于WRITESET的并行复制

前面的组提交大大的提升了主从复制的效率，但是他有一个特点，就是他依赖于主库的并行度，假如主库的并发比较高，那么才可以进行组提交，那么才能用到组提交的并行复制优化。

如果主库的SQL执行并没有那么频繁，那么时间间隔可能就会超过组提交的那两个参数阈值，就不会进行组提交。那么复制的时候就不能用并行复制了。

MySQL 8.0为了解决这个问题，引入了基于WriteSet的并行复制，这种情况下即使主库在串行提交的事务，只有互相不冲突，在备库就可以并行回放。

开启WRITESET：

```java
binlog_transaction_dependency_tracking  = WRITESET                 #    COMMIT_ORDER          
transaction_write_set_extraction        = XXHASH64
```

实际上Writeset是一个集合，使用的是C++ STL中的set容器：

```java
std::set<uint64> write_set_unique;
```

集合中的每一个元素都是hash值，这个hash值和transaction_write_set_extraction参数指定的算法有关（可选OFF、MURMUR32、XXHASH64，默认值 XXHASH64），其来源就是行数据的主键和唯一键。

WriteSet 是通过检测两个事务是否更新了相同的记录来判断事务能否并行回放的，因此需要在运行时保存已经提交的事务信息以记录历史事务更新了哪些行，并且在做更新的时候进行冲突检测，拿新更新的记录计算出来的hash值和WriteSet作比较，如果不存在，那么就认为是不冲突的，这样就可以共用同一个last_committed 、

>  last_committed 指的是该事务提交时，上一个事务提交的编号。

就这样，就能保证同一个write_set中的变更都是不冲突的，那么同一个write_set就可以并行的通过多个线程进行回放SQL了。

#### 总结

主从复制延迟的主要三个原因

1. 网络延迟
   1. 主要将服务部署在同城减少网络延迟
2. 硬件性能不够，从节点处理主节点数据太慢也可能会造成延迟
   1. 主要是可以从硬件上面做提升，比如cpu 硬盘
3. 主从复制的线程不够，如果主从复制的复制线程只有一个也会造成延迟
   1. 这个可以通过MySQL的并行复制来处理

##### 并行复制

MySQL的并行复制分版本：

- MySQL5.6，主要是基于库级别的并行复制，可以配置多个库进行并行复制
  - 由于是基于库，大多数业务都是单库，所以推出之后就太鸡肋没人用
- MySQL5.7，基于组提交的并行复制
  - 从这个版本开始就有了很大提升，组提交逻辑是将多个事务放入一个组最后通过提交，同组内的事务一定互不影响，因为底层是有对组提交的flush小阶段加锁的，所以对于这一组事务可以在主从同步的时候开多个sql线程并行同步，他们也不会有影响，因为这一组事务本就互不影响，就算并行执行页不会出现并发问题
- MySQL8.0，基于wrteset队列并行复制
  - 由于组提交会配置俩个参数，在主库数据库并发度不够的去情况下，超过这俩个配置阈值导致没触发组提交，没有组提交就没法并行复制，所以这个版本就推出writeset队列进行并行复制
  - 他的并行复制主要是使用了队列，队列是hash表，用来记录事务修改的行数据。
  - 因此需要在运行时保存已经提交的事务信息以记录历史事务更新了哪些行。
  - 然后每次主从同步的时候都会读取这个writeset，判断队列内是否有修改同一行数据，如果是就有冲突，不是的话就可以并行复制

### 为什么大厂不建议使用多表join？

之所以不建议使用join查询，最主要的原因就是join的效率比较低。

MySQL是使用了嵌套循环（Nested-Loop Join）的方式来实现关联查询的，简单点说就是要通过两层循环，用第一张表做外循环，第二张表做内循环，外循环的每一条记录跟内循环中的记录作比较，符合条件的就输出。

MySQL8.0之后使用hash join优化

#### HashJoin

**所谓Hash Join，其实是因为他底层用到了Hash表。**

Hash Join 是一种针对 equal-join 场景的优化，他的基本思想是将驱动表数据加载到内存，并建立 hash 表，这样只要遍历一遍非驱动表，然后再去通过哈希查找在哈希表中寻找匹配的行 ，就可以完成 join 操作了。

举个栗子。

```
SELECT
  student_name,school_name
FROM
  students LEFT JOIN schools ON students.school_id=schools.id;
```

以上，是一个left join的SQL，**在Hash Join过程中，主要分为两个步骤，分别是构建和探测**。

构建阶段，假如优化器优化后使用students作为驱动表，那么就会把这个驱动表的数据构建到hash表中：

![image.png](../hollis八股文/2024-09-02/Java8/MySQL/img/0BXi4ugyVRh96CZY/1685435976145-4815d9d3-9a52-4e86-93ad-c05b55933a8d-535796.png)

探测阶段，在这个过程中，从school表中取出记录之后，去hash表中查询，找到匹配的数据，在做聚合就行了。

![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501040140773.png)

需要注意的时候，上面的Hash表是在内存中的，但是，内存是有限的（通过join_buffer_size限制），如果内存中存不下驱动表的数据怎么办呢？

#### 扩展知识


##### 基于磁盘的hash join

如果驱动表中的数据量比较大， 没办法一次性的加载到内存中，就需要考虑把这些数据存储在磁盘上。通过将哈希表的一部分存储在磁盘上，分批次地加载和处理数据，从而减少对内存的需求。

在这样的算法中，为了避免一个大的hash表内存中无法完全存储，那么就采用分表的方式来实现，即首先利用 hash 算法将驱动表进行分表，并产生临时分片写到磁盘上。

这样就相当于把一张驱动表，拆分成多个hash表，并且分别存储在磁盘上。

![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501040140470.png)

接下来就是做join了，在这个过程中，会对被驱动表使用同样的 hash 算法进行分区，确定好在哪个分区之后，先确认下这个分区是否已经加载到内存，如果已经加载，则直接在内存中的哈希表中进行查找匹配的行。

![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501040140078.png)

如果哈希值对应的分区尚未加载到内存中，则从磁盘上读取该分区的数据到内存中的哈希表，并进行匹配。

就这样不断的重复进行下去，直到把所有数据都join完，把结果集返回。



#### **总结**

join在MySQL中使用了类似于嵌套循环，俩张表相连的话就类似于俩层循环，比如A join B 条件为A.ID=B.id 其实就相当于俩层循环比较id。

MySQL8.0推出hash join来优化，hash join就相当于把其中一层循环加载到内存中，并构建hash 表，连表的时候只需要遍历一层循环，另一层使用hash检索。

但是这就有一个问题，如果加载的hash表太大内存不够怎么办？

这里MySQL会采用分表加载，也就是将一个表分为多个表然后按批次加载并检索

 

### 什么是InnoDB**的页分裂和页合并**

我们都是知道，B+树是按照索引字段建立的，并且在B+树中是有序的，假如有下面一个索引的树结构，其中的索引字段的值并不连续。
![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021743030.png)

假如，现在我们插入一个新的一条记录，他的索引值是3，那么他就要按照顺序插入到页20中，在索引值为1,2的记录的后面。而如果这个索引页已经满了，那么就需要触发一次页分裂。

> **页分裂是指将该页面中的一部分索引记录移动到一个新的页面中，从而为新记录腾出空间。这样可以保持B+树的平衡和性能。**


以下，就是一次页分裂的过程：

![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021743095.png)

那么，当我们向Innodb中添加数据的时候，如果索引是随机无序的，那么就会导致页分裂。而且分裂这个动作还可能会引起连锁反应，从叶子节点沿着树结构一路分裂到根节点。

有分裂，就会有合并。在InnoDB中，当索引页面中的索引记录删除后，页面可能会变得过于稀疏。这时，为了节省空间和提高性能，可能会触发**页合并**操作。

> **页合并是指将两个相邻的索引页面合并成一个更大的页面，减少B+树的层级，从而提高查询性能。**


![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021743167.png)


#### 扩展知识


##### 页分裂（合并）的危害

首先，页分裂和合并是涉及写操作。频繁进行这些操作会增加数据库的I/O负担和CPU消耗，影响数据库的整体性能。

分裂和合并可能导致B+树索引结构频繁调整，这个过程也会影响插入及删除操作的性能。

频繁的页分裂和合并可能会导致磁盘上存在较多的空间碎片，新分出的一个页一般会有很多空闲空间，使得数据库表占用更多的磁盘空间，而导致浪费。


##### 如何避免页分裂

[✅char和varchar的区别？](https://www.yuque.com/hollis666/fo22bm/xodf4gdc6i9goyt6?view=doc_embed)

[✅MySQL的主键一定是自增的吗？](https://www.yuque.com/hollis666/fo22bm/glycgnryk8953c24?view=doc_embed)

在上面两篇中，我们介绍过，使用varchar或者使用UUID作为主键的话，都会导致页分裂。

所以，尽量选择使用自增的字段作为索引，尤其是主键索引，这样可以很大程度的避免页分裂。

如果要插入大量数据，尽量使用批量插入的方式，而不是逐条插入。这样可以减少页分裂的次数。

频繁删除操作可能导致页面过于稀疏，从而触发页合并。所以，一般建议使用**逻辑删除**而不是物理删除。

> 逻辑删除：即在记录中添加一个标记来表示记录是否被删除(deleted  = 0/1)，而不是真正地从数据库中删除记录。


我们当然还可以根据实际情况，适当调整InnoDB的配置参数，如页大小、填充因子、叶子页合并的阈值等，以优化数据库性能。

#### 总结

**页分裂是什么**

页分裂是当插入一条无序数据，并且数据页空间不够的时候，mysql为了保证当前数据页的数据一定要比上个数据页的数据都大，简单来说就是保证数据页之间的顺序性，毕竟MySQL的b+树是顺序结构，就会进行页分裂。

页分裂这个过程就是保证索引之间的顺序性嘛，所以这个过程是比较耗时的，要尽量避免页分裂。

**怎么会导致页分裂**

- 当你的索引字段不是有序字段的时候，比较容易出现页分裂。
  - ![image.png](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021813939.png)
  - 正常来说自增主键可以确保主键的递增性，使得新插入的数据都会在索引的末尾处，减少了数据页的分裂
  - 而如果是无序的，比如上图我要插入一条数据为3的数据，如果是有序话id不应该为3而是13，但是这里无序就是3会插入到数据页20的1，2数据后面，然后刚好数据页空间不够，mysql为了保证数据页之间的有序性就会导致页分裂
- 当你使用varchar的时候也容易出现页分裂。
  - 简单讲就是varchar是可变的，而数据之间的地址是连续的空间，所以当后续插入的数据比之前大，不能覆盖之前的空间，只能扩充空间，而扩充空间刚好数据页空间不够，需要挪动后续的数据，也就是页分裂了

**怎么预防页分裂**

### MySQL中，如何定位慢查询?

```sql
这个的话其实mysql有个慢日志文件,去修改里面的配置就可以了
#这个是开启慢查询,1就是开启,0为关闭
slow_query_log:1
#那这个呢他就是设置超过这个时间的就记录为慢查询,那我之前是设置为3秒
long_query_time:3
mysql定位慢查询的话其实可以用运维的一个恐惧skywallking但是那个具体我没用过听别人介绍过,那mysql的话是提供了慢日志,这个要去配置文件去开启,那开启的方式呢是有俩个关键地方需要修改的,我记得是....
#为1就是开启记录慢查询,0就是关闭
slow_query_time;1
#这个是设置,就比如说sql时间超过给定的这个数字就记录为慢查询,就我给个3当他sql超过3秒的话才会记录
long_query_time:3
```



### 那这个SQL语句执行很慢, 如何分析呢？

SQL语句执行慢的话可以使用explain
那这里有几个需要关注的字段分别是:

- key和key_len:这个的话他是会显示是否命中索引,或者你添加的索引是否有被使用
- type:那type的话我只知道出现index,all就说明sql需要优化,这个代表出现了全盘扫描和索引树扫描
- extra:这个的话他是会对你的sql提出优化建议,可以根据这个来判断查询是否有回表

### sql的优化的经验,你平时对sql语句做了哪些优化呢？

- 索引失效
  - 主要通过explian执行计划
  - 重点关注三个字段**key+type+extra**
  - 如果有用到索引，那么是走了覆盖索引呢？还是索引下推呢？还是扫描了整颗索引树呢？或者是用到了索引跳跃扫描等等。
- 多表join
  - 多表join类似嵌套循环过滤数据，MySQL8.0之后又hash join优化了
- 查询字段太多
  - 一般来说select字段少于100个的话问题不大，如果超过这个数量可以考虑垂直分表，把一些字段拆出去，但是分表要使用join，所以还得综合考虑一下
- 表中数据量太大
  - 一般来说表数据量超过1000w就算走了索引效率也不够好
  - 可以将数据归档，就比如保留最近半年数据，半年前的数据做归档
  - 利用好第三方数据库es之类的迁移数据
  - 使用分库、分表、分区
- 索引区分度不高
  - 区分度不高一般来说加索引过滤的数据不多，所以效果不够好，但是如果是99个男1个女，查女，这样还是过滤了很多数据的
- 数据库连接数不够
  - 待补充
- 数据库的表结构不合理
  - 有的时候比如某个字段中存了很长的内容，或者没有做合理的冗余需要多表关联查询等等。
  - 解决思路就是重构，或者分表。
- 数据库IO或者CPU比较高
  - 待补充

- 数据库参数不合理
  - 这个也是经常会遇到的，针对我们具体的业务场景，做一些适当的参数调整，有时候也能大大的提升SQL的效率。比如调整内存大小、缓存大小、线程池大小等。
- 事务比较长
  - 这个和慢SQL同理，都是占用了数据库链接，导致其他请求要等待。
- 锁竞争导致的等待
  - 当有大并发争抢共享资源的时候，就会导致锁等待，这个过程就会拉长耗时，导致SQL变慢。
  - 这个也可以参考上面的CPU被打满的问题。



### 如何优化一个大规模的数据库系统？

这是一个典型的考察知识储备以及架构能力的问题，一般可以考虑从以下几个方面回答：

1. **硬件优化**：升级硬件是提高数据库性能的一种有效方式，包括增加内存、磁盘、CPU和网络带宽等。
   - **存储优化**：使用高性能的SSD（固态硬盘）硬盘代替传统的HDD（机械硬盘）硬盘，减少I/O延迟。
   - **内存扩展**：增加服务器内存，以便更多的数据可以缓存在内存中，提高读取速度。4G不够就上8G，再不够就上16G。
   - **网络优化**：使用高速网络连接，减少网络延迟。或者把应用服务器和数据库服务器部署在同一个区域或者同一个机房中，减少网络延迟。

2. **数据库设计**：一个好的数据库设计可以提高数据库的性能和可扩展性。
   - **表结构优化**：简化表结构，避免过度的多表join。将频繁联合查询的数据进行合理冗余。
   - **数据归档**：定期归档旧数据，减少表的大小，提高查询性能。

3. **查询优化**：查询是数据库性能的瓶颈之一。使用合适的查询语句、索引、聚合、优化器统计信息等技术，可以有效地优化查询性能。
   - **SQL优化**：优化查询语句，避免复杂的子查询，避免多表JOIN，避免深度分页。
   - **查询计划分析**：利用数据库的查询分析器，分析并优化执行计划。

4. **索引优化**：索引是优化数据库性能的重要手段之一，可以使用合适的索引来提高查询效率。
   - 创建合适的索引：对经常查询和排序的列创建索引。
   - 避免过多索引：过多索引会降低写入性能。
   - 使用覆盖索引：尽量使用索引覆盖所有查询字段，避免回表操作。

5. **缓存机制**：将热点数据缓存在内存中，可以大大加快访问速度。可以使用Redis等缓存技术来实现。
   - **查询缓存**：使用Redis等内存数据库缓存频繁查询结果。
   - **对象缓存**：对经常访问的并且变化不频繁的对象（如用户信息）进行缓存。可以放到本地缓存中。

6. **负载均衡**：合理的负载均衡可以分摊数据库的负载，提高整个系统的性能。可以使用分布式数据库、读写分离等技术来实现。
   - **读写分离**：使用主从复制架构，主数据库处理写操作，从数据库处理读操作。
   - **数据库集群**：使用数据库集群分散负载。比如建立一主一备，把一些扫表任务和备库连接即可。

7. **分区和分片**：将数据分成多个区域或分片，可以降低单个节点的压力，提高整个系统的可扩展性和性能。
   - **分库分表**：通过分库来提升数据量连接数，进一步提升吞吐量。通过分表降低单表数据量，提升查询性能。

8. **数据备份和恢复**：备份和恢复是数据安全的基础，也是数据库可用性的重要保障。可以使用备份和恢复技术、数据同步技术等来保障数据的安全性和可用性。

9. **性能监控和调优**：定期对数据库系统进行性能监控和调优，可以及时发现并解决性能问题。可以使用性能监控工具、数据库性能分析工具等来实现。

综上所述，优化一个大规模的数据库系统需要从多个方面进行考虑和实践，需要不断地进行调整和改进，才能达到更好的性能和可用性。

### 创建表的时候，你们是如何优化的呢？

这个的话要根据字段了,就比如数值的话,int,bigint,tinyint具体选择,那字符串的话就是char和vachar,char效率更高

### 给你张表，发现查询速度很慢，你有那些解决方案

- **分析查询语句**：使用EXPLAIN命令分析SQL执行计划，找出慢查询的原因，比如是否使用了全表扫描，是否存在索引未被利用的情况等，并根据相应情况对索引进行适当修改。
- **创建或优化索引**：根据查询条件创建合适的索引，特别是经常用于WHERE子句的字段、Orderby 排序的字段、Join 连表查询的字典、 group by的字段，并且如果查询中经常涉及多个字段，考虑创建联合索引，使用联合索引要符合最左匹配原则，不然会索引失效
- **避免索引失效：**比如不要用左模糊匹配、函数计算、表达式计算等等。
- **查询优化**：避免使用SELECT *，只查询真正需要的列；使用覆盖索引，即索引包含所有查询的字段；联表查询最好要以小表驱动大表，并且被驱动表的字段要有索引，当然最好通过冗余字段的设计，避免联表查询。
- **分页优化：**针对 limit n,y 深分页的查询优化，可以把Limit查询转换成某个位置的查询：select * from tb_sku where id>20000 limit 10，该方案适用于主键自增的表，
- **优化数据库表**：如果单表的数据超过了千万级别，考虑是否需要将大表拆分为小表，减轻单个表的查询压力。也可以将字段多的表分解成多个表，有些字段使用频率高，有些低，数据量大时，会由于使用频率低的存在而变慢，可以考虑分开。
- **使用缓存技术**：引入缓存层，如Redis，存储热点数据和频繁查询的结果，但是要考虑缓存一致性的问题，对于读请求会选择旁路缓存策略，对于写请求会选择先更新 db，再删除缓存的策略。

## MySql日志

- **undo log（回滚日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**原子性**，主要**用于事务回滚和 MVCC**。
- **redo log（重做日志）**：是 Innodb 存储引擎层生成的日志，实现了事务中的**持久性**，主要**用于掉电等故障恢复**,**将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。；
- **binlog （归档日志）**：是 **Server** 层生成的日志，主要**用于数据备份和主从复制**；

### undo log

#### 回滚

- undo log怎么做到回滚的?

  - 在执行一条“增删改”语句的时候MySQL 会**隐式开启事务**,那么，考虑一个问题。一个事务在执行过程中，在还没有提交事务之前，如果 MySQL 发生了崩溃，要怎么回滚到事务之前的数据呢？

- 解决方案:

  - 在事务没提交之前，MySQL 会先记录更新前的数据到 undo log 日志文件里面，当事务回滚时，可以利用 undo log 来进行回滚。

    - 在**插入**一条记录时，要把这条记录的**主键值记下来**，这样之后回滚时只需要把这个主键值对应的记录**删掉**就好了；

    - 在**删除**一条记录时，要把这条记录中的**内容都记下来**，这样之后回滚时再把由这些内容组成的记录**插入**到表中就好了；

    - 在**更新**一条记录时，要把被更新的列的**旧值记下来**，这样之后回滚时再把这些列**更新为旧值**就好了。


#### 并发控制

undo log构成

- 一条记录的每一次更新操作产生的 undo log 格式都有一个 roll_pointer 指针和一个 trx_id 事务id：
  - 通过 trx_id 可以知道该记录是被哪个事务修改的；
  - 通过 roll_pointer 指针可以将这些 undo log 串成一个链表，这个链表就被称为版本链；


### redo log

- **实现事务的持久性**，能够保证 MySQL 在任何时间段突然崩溃，重启后之前已提交的记录都不会丢失；
- **将写操作从「随机写」变成了「顺序写」**，提升 MySQL 写入磁盘的性能。

**为什么需要 Buffer Pool？**

MySQL 的数据都是存在磁盘中的，那么我们要更新一条记录的时候，得先要从磁盘读取该记录，然后在内存中修改这条记录。那修改完这条记录是选择直接写回到磁盘，还是选择缓存起来呢？

当然是**缓存**起来好，这样下次有查询语句命中了这条记录，直接读取缓存中的记录，就不需要从磁盘获取数据了。

为此，Innodb 存储引擎设计了一个**缓冲池（Buffer Pool）**，来提高数据库的读写性能。

- 当**读**取数据时，如果数据存在于 Buffer Pool 中，客户端就会直接**读取 Buffer Pool** 中的数据，否则再去**磁盘中读取**。
- 当**修改**数据时，如果数据存在于 Buffer Pool 中，那直接**修改 Buffer Pool 中**数据所在的页，然后将其页设置为**脏页**（该页的内存数据和磁盘上的数据已经不一致），为了减少磁盘I/O，不会立即将脏页写入磁盘，后续**由后台线程**选择一个合适的时机将脏页写入到磁盘

为了防止**断电**导致数据丢失的问题，当有一条记录需要更新的时候，InnoDB 引擎就会先更新内存（同时标记为脏页），然后将本次对这个页的修改以 redo log 的形式记录下来

### bin log

这个是基于server层面的,它不需要依赖其他引擎,其他俩个就是依赖innodb引擎,他主要作用是**数据恢复和主从复制**

**数据恢复区别于故障恢复**

数据恢复是当数据被删除需要恢复,而故障恢复时当服务器宕机需要恢复,听起来有点像实际上区别很大,具体要讲**redo log和bin log的区别**

**主从复制**

其实就是主节点去要像从节点同步数据,这个时候主节点会发binlog给从节点去同步,就是这么实现的

### redo log和undo log区别

- redo log 记录了此次事务「**完成后**」的数据状态，记录的是更新**之后**的值；
- undo log 记录了此次事务「**开始前**」的数据状态，记录的是更新**之前**的值；

事务提交之前发生了崩溃，重启后会通过 undo log 回滚事务，事务提交之后发生了崩溃，重启后会通过 redo log 恢复事务

### redo log 和bin log区别

因为 redo log 文件是循环写，是会**边写边擦除日志**的，只记录未被刷入磁盘的数据的物理日志，已经刷入磁盘的数据都会从 redo log 文件里**擦除**。

binlog 文件**保存的是全量的日志**，也就是保存了所有数据变更的情况，理论上只要记录在 binlog 上的数据，都可以恢复，所以如果不小心整个数据库的数据被删除了，得用 binlog 文件恢复数据。

### 二阶段提交

#### 为什么需要二阶段提交？

事务提交后，redo log 和 binlog 都要持久化到磁盘，但是这两个是独立的逻辑，可能出现半成功的状态，这样就造成两份日志之间的逻辑不一致。



举个例子，假设 id = 1 这行数据的字段 name 的值原本是 `jay`，然后执行 `UPDATE t_user SET name = xiaolin WHERE id = 1;` 如果在持久化 redo log 和 binlog 两个日志的过程中，出现了半成功状态，那么就有两种情况：(双刷问题)

- **如果在将 redo log 刷入到磁盘之后， MySQL 突然宕机了，而 binlog 还没有来得及写入**。MySQL 重启后，通过 redo log 能将 Buffer Pool 中 id = 1 这行数据的 name 字段恢复到新值 xiaolin，但是 binlog 里面没有记录这条更新语句，在主从架构中，binlog 会被复制到从库，由于 binlog 丢失了这条更新语句，从库的这一行 name 字段是旧值 jay，与主库的值不一致性；
- **如果在将 binlog 刷入到磁盘之后， MySQL 突然宕机了，而 redo log 还没有来得及写入**。由于 redo log 还没写，崩溃恢复以后这个事务无效，所以 id = 1 这行数据的 name 字段还是旧值 jay，而 binlog 里面记录了这条更新语句，在主从架构中，binlog 会被复制到从库，从库执行了这条更新语句，那么这一行 name 字段是新值 xiaolin，与主库的值不一致性；

可以看到，在持久化 redo log 和 binlog 这两份日志的时候，如果出现半成功的状态，就会造成主从环境的数据不一致性。**这是因为 redo log 影响主库的数据，binlog 影响从库的数据**，所以 redo log 和 binlog 必须保持一致才能保证主从数据一致。

**MySQL 为了避免出现两份日志之间的逻辑不一致的问题，使用了「两阶段提交」来解决**，两阶段提交其实是分布式事务一致性协议，它可以保证多个逻辑操作要不全部成功，要不全部失败，不会出现半成功的状态。

**两阶段提交把单个事务的提交拆分成了 2 个阶段，分别是「准备（Prepare）阶段」和「提交（Commit）阶段」**，每个阶段都由协调者（Coordinator）和参与者（Participant）共同完成。注意，不要把提交（Commit）阶段和 commit 语句混淆了，commit 语句执行的时候，会包含提交（Commit）阶段。

举个拳击比赛的例子，两位拳击手（参与者）开始比赛之前，裁判（协调者）会在中间确认两位拳击手的状态，类似于问你准备好了吗？

- **准备阶段**：裁判（协调者）会依次询问两位拳击手（参与者）是否准备好了，然后拳击手听到后做出应答，如果觉得自己准备好了，就会跟裁判说准备好了；如果没有自己还没有准备好（比如拳套还没有带好），就会跟裁判说还没准备好。
- **提交阶段**：如果两位拳击手（参与者）都回答准备好了，裁判（协调者）宣布比赛正式开始，两位拳击手就可以直接开打；如果任何一位拳击手（参与者）回答没有准备好，裁判（协调者）会宣布比赛暂停，对应事务中的回滚操作。

##### 总结

**作用**

用来解决redolog和binlog不一致的问题。

主要是解决双刷的问题。

1. 假如redolog刷盘成功，binlog刷盘的时候宕机导致失败，导致主从同步的时候由于redo做了持久化，主库恢复了数据，主从同步依靠binlog，binlog刷盘失败，所以同步数据丢失
2. binlog刷盘成功，但是这个时候redolog还没刷盘成功就宕机了，这个会导致主从同步的时候binlog刷盘成功所以数据同步成功，但是redolog失败所以主库数据不一致
3. 总结：redolog保证主库数据，binlog保证从库数据一致

**二阶段流程**

这个流程主要是跟分布式事务类似，分为俩个阶段

- 准备阶段：参与者向协调者汇报是否准备好了
- 提交阶段：协调者收到参与者的汇报之后，根据参与者的结果来选择回滚或者提交

#### 两阶段提交的过程是怎样的？

在 MySQL 的 InnoDB 存储引擎中，开启 binlog 的情况下，MySQL 会同时维护 binlog 日志与 InnoDB 的 redo log，为了保证这两个日志的一致性，MySQL 使用了**内部 XA 事务**（是的，也有外部 XA 事务，跟本文不太相关，我就不介绍了），内部 XA 事务由 binlog 作为协调者，存储引擎是参与者。

当客户端执行 commit 语句或者在自动提交的情况下，MySQL 内部开启一个 XA 事务，**分两阶段来完成 XA 事务的提交**，如下图：

![两阶段提交](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910129.png)

从图中可看出，事务的提交过程有两个阶段，就是**将 redo log 的写入拆成了两个步骤：prepare 和 commit，中间再穿插写入binlog**，具体如下：（双1）

- **prepare 阶段**：将 XID（内部 XA 事务的 ID） 写入到 redo log，同时将 redo log 对应的事务状态设置为 prepare，然后将 redo log 持久化到磁盘（innodb_flush_log_at_trx_commit = 1 的作用）；
- **commit 阶段**：把 XID 写入到 binlog，然后将 binlog 持久化到磁盘（sync_binlog = 1 的作用），接着调用引擎的提交事务接口，将 redo log 状态设置为 commit，此时该状态并不需要持久化到磁盘，只需要 write 到文件系统的 page cache 中就够了，因为只要 binlog 写磁盘成功，就算 redo log 的状态还是 prepare 也没有关系，一样会被认为事务已经执行成功；

##### 总结

这个流程主要是跟分布式事务类似，MySQL采用的是内部XA，分为俩个阶段

- 准备阶段

  - 将XID（事务id）写入redolog（写XID）
  - 设置redolog状态为`准备阶段`（设置状态）
  - 将redolog持久化到磁盘（刷盘）

- 提交阶段：协调者收到参与者的汇报之后，根据参与者的结果来选择回滚或者提交

  - 将XID（事务id）写入binlog（写XID）
  - 设置redolog状态为`提交阶段`（设置状态）
  - 将binlog持久化到磁盘（刷盘）

  > 但是这里保持数据一致主要是参考binlog有没有redolog的xid，而不是redolog的提交状态

#### 异常重启会出现什么现象？

我们来看看在两阶段提交的不同时刻，MySQL 异常重启会出现什么现象？下图中有时刻 A 和时刻 B 都有可能发生崩溃：

![时刻 A 与时刻 B](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910262.png)

不管是时刻 A（redo log 已经写入磁盘， binlog 还没写入磁盘），还是时刻 B （redo log 和 binlog 都已经写入磁盘，还没写入 commit 标识）崩溃，**此时的 redo log 都处于 prepare 状态**。

在 MySQL 重启后会按顺序扫描 redo log 文件，碰到处于 prepare 状态的 redo log，就拿着 redo log 中的 XID 去 binlog 查看是否存在此 XID：

- **如果 binlog 中没有当前内部 XA 事务的 XID，说明 redolog 完成刷盘，但是 binlog 还没有刷盘，则回滚事务**。对应时刻 A 崩溃恢复的情况。
- **如果 binlog 中有当前内部 XA 事务的 XID，说明 redolog 和 binlog 都已经完成了刷盘，则提交事务**。对应时刻 B 崩溃恢复的情况。

可以看到，**对于处于 prepare 阶段的 redo log，即可以提交事务，也可以回滚事务，这取决于是否能在 binlog 中查找到与 redo log 相同的 XID**，如果有就提交事务，如果没有就回滚事务。这样就可以保证 redo log 和 binlog 这两份日志的一致性了。

所以说，**两阶段提交是以 binlog 写成功为事务提交成功的标识**，因为 binlog 写成功了，就意味着能在 binlog 中查找到与 redo log 相同的 XID。

> 处于 prepare 阶段的 redo log 配合 binlog的xid，重启就提交事务，MySQL 为什么要这么设计?

binlog 已经写入了，之后就会被从库（或者用这个 binlog 恢复出来的库）使用。

所以，在主库上也要提交这个事务。采用这个策略，主库和备库的数据就保证了一致性。

> 事务没提交的时候，redo log 会被持久化到磁盘吗？

会的。

事务执行中间过程的 redo log 也是直接写在 redo log buffer 中的，这些缓存在 redo log buffer 里的 redo log 也会被「后台线程」每隔一秒一起持久化到磁盘。

也就是说，**事务没提交的时候，redo log 也是可能被持久化到磁盘的**。

有的同学可能会问，如果 mysql 崩溃了，还没提交事务的 redo log 已经被持久化磁盘了，mysql 重启后，数据不就不一致了？

放心，这种情况 mysql 重启会进行回滚操作，因为事务没提交的时候，binlog 是还没持久化到磁盘的。

所以， redo log 可以在事务没提交之前持久化到磁盘，但是 binlog 必须在事务提交之后，才可以持久化到磁盘。



##### 总结

重启异常主要会有俩个问题

1. 当重启之后MySQL会去读取redolog的xid，当读到xid为prepare阶段的时候就会去binlog找到对应的xid，看看binlog是否有这个xid，如果有说明binlog已经刷盘了，则直接提交就行了。
2. 如果binlog没有这个xid就回滚呗

#### 两阶段提交有什么问题？

两阶段提交虽然保证了两个日志文件的数据一致性，但是性能很差，主要有两个方面的影响：

- **磁盘 I/O 次数高**：对于“双1”配置，每个事务提交都会进行两次 fsync（刷盘），一次是 redo log 刷盘，另一次是 binlog 刷盘。
- **锁竞争激烈**：两阶段提交虽然能够保证「单事务」两个日志的内容一致，但在「多事务」的情况下，却不能保证两者的提交顺序一致，因此，在两阶段提交的流程基础上，还需要加一个锁来保证提交的原子性，从而保证多事务的情况下，两个日志的提交顺序一致。

> 为什么两阶段提交的磁盘 I/O 次数会很高？

binlog 和 redo log 在内存中都对应的缓存空间，binlog 会缓存在 binlog cache，redo log 会缓存在 redo log buffer，它们持久化到磁盘的时机分别由下面这两个参数控制。一般我们为了避免日志丢失的风险，会将这两个参数设置为 1：

- 当 sync_binlog = 1 的时候，表示每次提交事务都会将 binlog cache 里的 binlog 直接持久到磁盘；
- 当 innodb_flush_log_at_trx_commit = 1 时，表示每次事务提交时，都将缓存在 redo log buffer 里的 redo log 直接持久化到磁盘；

可以看到，如果 sync_binlog 和 当 innodb_flush_log_at_trx_commit 都设置为 1，那么在每个事务提交过程中， 都会**至少调用 2 次刷盘操作**，一次是 redo log 刷盘，一次是 binlog 落盘，所以这会成为性能瓶颈。

> 为什么锁竞争激烈？

在早期的 MySQL 版本中，通过使用 prepare_commit_mutex 锁来保证事务提交的顺序，在一个事务获取到锁时才能进入 prepare 阶段，一直到 commit 阶段结束才能释放锁，下个事务才可以继续进行 prepare 操作。

通过加锁虽然完美地解决了顺序一致性的问题，但在并发量较大的时候，就会导致对锁的争用，性能不佳。



##### **总结**

俩个问题：

1. I/O性能差，这个主要是因为俩个日志都有对应的缓存binlog cache 和redolog buffer，一般为了避免数据丢失都是设置双1，也就是每提交一次都刷盘，这样就会导致每次提交事务binlog和redo会共刷俩次盘
2. 锁竞争性能差，早期MySQL为了解决多个事务提交的顺序性，采用锁机制，只有当前面的事务commit之后才能拿到锁进行准备阶段，这种在并发大的时候性能不够好

> MySQL后面依靠组提交来进行优化

#### 组提交

**MySQL 引入了 binlog 组提交（group commit）机制，当有多个事务提交的时候，会将多个 binlog 刷盘操作合并成一个，从而减少磁盘 I/O 的次数**，如果说 10 个事务依次排队刷盘的时间成本是 10，那么将这 10 个事务一次性一起刷盘的时间成本则近似于 1。

引入了组提交机制后，prepare 阶段不变，只针对 commit 阶段，将 commit 阶段拆分为三个过程：

- **flush 阶段**：多个事务按进入的顺序将 binlog 从 cache 写入文件（不刷盘）；
- **sync 阶段**：对 binlog 文件做 fsync 操作（多个事务的 binlog 合并一次刷盘）；
- **commit 阶段**：各个事务按顺序做 InnoDB commit 操作；

上面的**每个阶段都有一个队列**，每个阶段有锁进行保护，因此保证了事务写入的顺序，第一个进入队列的事务会成为 leader，leader领导所在队列的所有事务，全权负责整队的操作，完成后通知队内其他事务操作结束。

![每个阶段都有一个队列](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910269.png)

对每个阶段引入了队列后，锁就只针对每个队列进行保护，不再锁住提交事务的整个过程，可以看的出来，**锁粒度减小了，这样就使得多个阶段可以并发执行，从而提升效率**。

> 有 binlog 组提交，那有 redo log 组提交吗？

这个要看 MySQL 版本，MySQL 5.6 没有 redo log 组提交，MySQL 5.7 有 redo log 组提交。

在 MySQL 5.6 的组提交逻辑中，每个事务各自执行 prepare 阶段，也就是各自将 redo log 刷盘，这样就没办法对 redo log 进行组提交。

所以在 MySQL 5.7 版本中，做了个改进，在 prepare 阶段不再让事务各自执行 redo log 刷盘操作，而是推迟到组提交的 flush 阶段，也就是说 prepare 阶段融合在了 flush 阶段。

这个优化是将 redo log 的刷盘延迟到了 flush 阶段之中，sync 阶段之前。通过延迟写 redo log 的方式，为 redolog 做了一次组写入，这样 binlog 和 redo log 都进行了优化。

接下来介绍每个阶段的过程，注意下面的过程针对的是“双 1” 配置（sync_binlog 和 innodb_flush_log_at_trx_commit 都配置为 1）。

> flush 阶段

第一个事务会成为 flush 阶段的 Leader，此时后面到来的事务都是 Follower ：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910000.png)

接着，获取队列中的事务组，由绿色事务组的 Leader 对 redo log 做一次 write + fsync，即一次将同组事务的 redolog 刷盘：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910847.png)

完成了 prepare 阶段后，将绿色这一组事务执行过程中产生的 binlog 写入 binlog 文件（调用 write，不会调用 fsync，所以不会刷盘，binlog 缓存在操作系统的文件系统中）。

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910915.png)

从上面这个过程，可以知道 flush 阶段队列的作用是**用于支撑 redo log 的组提交**。

如果在这一步完成后数据库崩溃，由于 binlog 中没有该组事务的记录，所以 MySQL 会在重启后回滚该组事务。

> sync 阶段

绿色这一组事务的 binlog 写入到 binlog 文件后，并不会马上执行刷盘的操作，而是**会等待一段时间**，这个等待的时长由 `Binlog_group_commit_sync_delay` 参数控制，**目的是为了组合更多事务的 binlog，然后再一起刷盘**，如下过程：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910242.png)

不过，在等待的过程中，如果事务的数量提前达到了 `Binlog_group_commit_sync_no_delay_count` 参数设置的值，就不用继续等待了，就马上将 binlog 刷盘，如下图：

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910387.png)

从上面的过程，可以知道 sync 阶段队列的作用是**用于支持 binlog 的组提交**。

如果想提升 binlog 组提交的效果，可以通过设置下面这两个参数来实现：

- `binlog_group_commit_sync_delay= N`，表示在等待 N 微妙后，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘，也就是将「 binlog 文件」持久化到磁盘。
- `binlog_group_commit_sync_no_delay_count = N`，表示如果队列中的事务数达到 N 个，就忽视binlog_group_commit_sync_delay 的设置，直接调用 fsync，将处于文件系统中 page cache 中的 binlog 刷盘。

如果在这一步完成后数据库崩溃，由于 binlog 中已经有了事务记录，MySQL会在重启后通过 redo log 刷盘的数据继续进行事务的提交。

> commit 阶段

最后进入 commit 阶段，调用引擎的提交事务接口，将 redo log 状态设置为 commit。

![img](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202501021910878.png)

commit 阶段队列的作用是承接 sync 阶段的事务，完成最后的引擎提交，使得 sync 可以尽早的处理下一组事务，最大化组提交的效率。



##### 总结

**组提交作用**

MySQL的二阶段提交在单事务的情况下并没有什么性能问题，但是在多事务的情况下如果配置了俩个日志的双1和为了保证事务的顺序性要加锁，这俩个问题都会导致二阶段提交的性能差。

所以组提交就是用来优化二阶段提交的磁盘I/O和加锁性能差的俩个问题

**组提交具体怎么解决的**？

不管什么版本MySQL有对binlog的组提交优化。

具体来说就是将二阶段的commit阶段细分为三个阶段

1. flush阶段，这个阶段主要是将多个事物的所有binlog顺序写入bin log cache（只是写入，不刷盘）
2. sync阶段，多个事务的binlog合并一次刷盘
3. commit阶段，各个事务按顺序提交

**为什么细化三步就能优化呢**？

- 首先，将多个事务在flsuh阶段写入一个文件最后在sync阶段合并刷盘，这样刷盘的效率堪比一次io，如果分开就是10次io
- 各个阶段为了保证顺序，每个阶段都有锁，这样锁的粒度由整个commit的粒度细化为三个flush、sync、commit细小阶段的锁，粒度更小

**只有binlog由组提交优化吗？**

在MySQL5.6之前redolog是各个事务每个prepare阶段都分开提交

在MySQL5.6之后redolog也有组提交，他将prepare阶段放在flush阶段进行组提交了

**flush、sync、commit具体流程**

flush

1. redo log，write+sync，也就是redolog组提交的优化
2. binlog，write，binlog只写入binlog cache而不刷盘

> 所以flush阶段主要是给redolog做组提交。
>
> 如果这里宕机了，redolog刷盘了，但是binlog没有对应的xid则会回滚

sync

1. binlog调用sync，也就是真正将操作系统缓存文件刷盘

> 所以sync阶段主要是给binlog做组提交（刷盘）。
>
> 这里刷盘时间不是立即刷盘，而是看你的配置（MySQL 磁盘 I/O 很高，有什么优化的方法？有说）。
>
> 如果这里宕机了，binlog刷盘了，所以当重启之后读取redolog的prepare阶段就会去看binlog有没有对应的xid，这里肯定用所以就提交了。

commit

1.这个阶段主要是进行commit状态标识，让别的事务能提前拿到锁

#### MySQL 磁盘 I/O 很高，有什么优化的方法？

现在我们知道事务在提交的时候，需要将 binlog 和 redo log 持久化到磁盘，那么如果出现 MySQL 磁盘 I/O 很高的现象，我们可以通过控制以下参数，来 “延迟” binlog 和 redo log 刷盘的时机，从而降低磁盘 I/O 的频率：

- 设置组提交的两个参数： binlog_group_commit_sync_delay 和 binlog_group_commit_sync_no_delay_count 参数，延迟 binlog 刷盘的时机，从而减少 binlog 的刷盘次数。这个方法是基于“额外的故意等待”来实现的，因此可能会增加语句的响应时间，但即使 MySQL 进程中途挂了，也没有丢失数据的风险，因为 binlog 早被写入到 page cache 了，只要系统没有宕机，缓存在 page cache 里的 binlog 就会被持久化到磁盘。
- 将 sync_binlog 设置为大于 1 的值（比较常见是 100~1000），表示每次提交事务都 write，但累积 N 个事务后才 fsync，相当于延迟了 binlog 刷盘的时机。但是这样做的风险是，主机掉电时会丢 N 个事务的 binlog 日志。
- 将 innodb_flush_log_at_trx_commit 设置为 2。表示每次事务提交时，都只是缓存在 redo log buffer 里的 redo log 写到 redo log 文件，注意写入到「 redo log 文件」并不意味着写入到了磁盘，因为操作系统的文件系统中有个 Page Cache，专门用来缓存文件数据的，所以写入「 redo log文件」意味着写入到了操作系统的文件缓存，然后交由操作系统控制持久化到磁盘的时机。但是这样做的风险是，主机掉电的时候会丢数据。

### Buffer Pool

#### Buffer Pool的作用

- 每次查询访问数据库性能差,引入buffer pool解决这个问题

#### 如何提高缓存命中率？

- 要实现这个，最容易想到的就是 LRU（Least recently used）算法。
- 该算法的思路是，链表头部的节点是最近使用的，而链表末尾的节点是最久没被使用的。那么，当空间不够了，就淘汰最久没被使用的节点，从而腾出空间。

简单的 LRU 算法的实现思路是这样的：

* 当访问的页在 Buffer Pool 里，就直接把该页对应的 LRU 链表节点移动到链表的头部。
* 当访问的页不在 Buffer Pool 里，除了要把页放入到 LRU 链表的头部，还要淘汰 LRU 链表末尾的节点。

比如下图，假设 LRU 链表长度为 5，LRU 链表从左到右有 1，2，3，4，5 的页。

如果访问了 3 号的页，因为 3 号页在 Buffer Pool 里，所以把 3 号页移动到头部即可。

而如果接下来，访问了 8 号页，因为 8 号页不在 Buffer Pool 里，所以需要先淘汰末尾的 5 号页，然后再将 8 号页加入到头部。


比如下图，假设 LRU 链表长度为 5，LRU 链表从左到右有 1，2，3，4，5 的页。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007900.png)

如果访问了 3 号的页，因为 3 号页在 Buffer Pool 里，所以把 3 号页移动到头部即可。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007769.png)

而如果接下来，访问了 8 号页，因为 8 号页不在 Buffer Pool 里，所以需要先淘汰末尾的 5 号页，然后再将 8 号页加入到头部。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007826.png)

比如下图，假设 LRU 链表长度为 5，LRU 链表从左到右有 1，2，3，4，5 的页。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007900.png)

如果访问了 3 号的页，因为 3 号页在 Buffer Pool 里，所以把 3 号页移动到头部即可。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007769.png)

而如果接下来，访问了 8 号页，因为 8 号页不在 Buffer Pool 里，所以需要先淘汰末尾的 5 号页，然后再将 8 号页加入到头部。

![](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/202410211007826.png)



## mysql集群

- 分表是解决数据量大，查询慢的问题。
- 分库是解决并发高，连接数不够的问题。

主从

读写分离,通过binlog日志进行主从同步,通过分片中间件shardingSphere来路由,如果是增删改路由到主节点,如果是查根据路由策略选择一个从节点执行

主从复制:

第一：主库在事务提交时，会把数据变更记录在二进制日志文件 Binlog 中。

第二：从库读取主库的二进制日志文件 Binlog ，写入到从库的中继日志 Relay Log 。

第三：从库重做中继日志中的事件，将改变反映它自己的数据

分片(分库分表,十亿)



一开始你的数据可能是中小规模,随着发展你的数据量越来越大,你单库已经扛不住了,可以使用分片把单个数据打散成多个数据库,比如我把数据归为近一年以及超过一年的数据,那我会根据shardingsphere来路由,如果是查询先查一年数据少,查询快,不能保证高可用

# 阿里数据库能抗秒杀原理

其实，在阿里电商的秒杀等（据我了解，淘宝、天猫、猫超、大麦等都是这么干的）场景中，主要还是基于MySQL数据库在做扣减的，主要是因为这样做最可靠了（避免了redis扣减方案中的数据不一致、少卖等问题）。

但是我们都知道，数据库是抗不了热点行的并发更新的，于是阿里内部就对MySQL做了patch。

这个技术叫做Inventory Hint，其实就是一个补丁。（官方介绍：[https://help.aliyun.com/zh/rds/apsaradb-rds-for-mysql/inventory-hint](https://help.aliyun.com/zh/rds/apsaradb-rds-for-mysql/inventory-hint?spm=a2c4g.11186623.0.0.4b224ac9RdTjAG) ）

## 使用方法

他的用法很简单，只需要在正常的update语句中增加上特殊的hint语句就行了，如：

```
UPDATE /*+ COMMIT_ON_SUCCESS ROLLBACK_ON_FAIL TARGET_AFFECT_ROW(1)*/ T
SET c = c - 1
WHERE id = 1;
```

这里面的`COMMIT_ON_SUCCESS`、`ROLLBACK_ON_FAIL`和`TARGET_AFFECT_ROW`都是一些Hint语法：

- COMMIT_ON_SUCCESS：当前语句执行成功就提交事务上下文。
- ROLLBACK_ON_FAIL：当前语句执行失败就回滚事务上下文。
- TARGET_AFFECT_ROW(NUMBER)：如果当前语句影响行数是指定的就成功，否则语句失败。

> hint：MySQL 中的 "Hint" 是一种特殊的语法，允许开发者向数据库引擎提供如何执行特定查询的额外信息或建议。这些提示不改变查询的结果，但可以影响查询的执行路径，比如如何选择索引、是否使用缓存等。使用 Hint 的目的是为了优化查询性能。


很显然，前面我们提到的这几个hint是阿里自己支持的。所以只有内部的数据库， 或者阿里云的RDS才支持。


## 原理介绍

当我们是使用`COMMIT_ON_SUCCESS`等hint标记了一条SQL之后，就相当于告诉MySQL内核，这行可能是热点更新。

于是，MySQL的内核层就会自动识别带此类标记的更新操作，在一定的时间间隔内，将收集到的更新操作按照主键或者唯一键进行分组，这样更新相同行的操作就会被分到同一组中。

![image-20241207140840004](https://pic-1329573580.cos.ap-nanjing.myqcloud.com/image-20241207140840004.png)

> 为了进一步提升性能，在实现上，使用两个执行单元。当第一个执行单元收集完毕准备提交时，第二个执行单元立即开始收集更新操作；当第二个执行单元收集完毕准备提交时，第一个执行单元已经提交完毕并开始收集新一批的更新操作，两个单元不断切换，并行执行。


根据热点行做了分组之后，就可以作进一步优化了，这个过程主要有3个关键的优化点：

**1、减少行级锁的申请等待**

在同一组中，需要更新的都是同一条记录，那么根据SQL的提交顺序，就可以排队了。

然后我们只需要在第一条更新SQL（Leader)执行的时候，尝试去获取目标行的锁，如果获取成功，则开始操作。

然后这一组中后续的更新操作（Follower）也会尝试获取锁，但是会先判断是不是已经被第一条更新操作获取到了，如果是的话，那么就不需要等待，直接获取锁。

这样就可以大大降低行级锁的申请的阻塞等待时长。

**2、减少B+树的索引遍历操作**

MySQL是以B+索引的方式管理数据的，每次执行查询时，都需要遍历索引才能定位到目标数据行，数据表越大，索引层级越多，遍历时间就越长。

如果针对热点行更新操作做了分组之后，我们只需要在每组的第一条SQL执行过程中，通过遍历索引定位数据行，之后就可以把这些数据行缓存到Row Cache中，并且在Row Cache进行修改。

在同组的后续操作时，也不再需要进行数据索引了，直接从Row Cache获取数据并修改就行了。

这样就大大降低了B+树的索引遍历操作的耗时。

**3、减少事务提交次数**

如果是没有用这种方式，我们的多条update语句会是多条事务，那么每一个事务都要单独做一次提交。

有了分组、排队、组提交之后，就只需要一组中的并发操作都执行完，然后做一次组提交即可，大大降低提交次数。

## 总结

> 为什么阿里使用MySQL不适用redis?

如果使用redis的话要考虑数据的持久化，涉及持久化就一定有一致性的要求，这个会比较麻烦，所以避免这些一致性的问题舍弃了redis。

> 那就有另一个问题，MySQL涉及锁、事务、查询性能，阿里是怎么解决的呢？

这里主要是依靠msyql的hint语法，而阿里有内部的这个语法比如：

- COMMIT_ON_SUCCESS：当前语句执行成功就提交事务上下文。
- ROLLBACK_ON_FAIL：当前语句执行失败就回滚事务上下文。
- TARGET_AFFECT_ROW(NUMBER)：如果当前语句影响行数是指定的就成功，否则语句失败。

使用这些hint语法之后就相当于告诉MySQL内核层，这条数据是热点数据，在后续的时间内的所有写操作他会按照主键或者唯一键进行分组，这样就能让同一条行数据放进一个组

> 为什么要放一个组？有什么好处吗？

放同一个组主要是由三个好处：

1. 减少锁的申请等待
   1. 具体来说就是当你把一条行数据的所有写操作放到一个组里，然后按照顺序队列去拿锁，比如ABC三条update操作，首先对A加锁，然后由于是顺序队列，所以不会出现竞争的情况，BC就也不需要锁（因为A没执行完还没轮到他，类似于并发变串行了）BC就也不需要阻塞等待了，然后当A执行完就轮到B，B会判断组内第一条sql是否拿到过锁，拿到过B就直接加锁，这里也是因为顺序的原因所以也不需要担心阻塞问题
2. 减少查询的IO次数
3. 减少事务提交次数
   1. 分组之前每条update都毁显示开始事务，分组之后先当于多条命令放入一个事务里了
   2. 那么这里会有个问题？如果其中一个执行失败了那不就全部回滚了，这里其实我也没有想的很明白，我逆向思维的话mysql执行失败只有可能是因为并发的时候加了锁导致一条sql执行失败，所以这种情况一般不会出现，具体是咋样有待考证

